#pragma once
// MIT License
// Copyright (c) 2025 Jan-Willem Krans (janwillem32 <at> hotmail <dot> com)
// Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
// The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

// Radixsortbidi8
// This library implements an efficient stable sort on arrays using an 8-bit indexed, bidirectional, least significant bit first radix sort method.
// This is currently a single-file library, with some additional folders and files only used for its test suite.
// Sorting functionality is available for unsigned integer, signed integer, floating-point and enumeration types.
// All these sorting functions can sort forwards and reverse, order forwards and reverse, and optionally filter by absolute value.
// Several filters are available, such as two types of absolute, and an inverse pattern for signed integer and floating point types.
// See "Modes of operation for the template functions" for more details on that.
// Implemented function optimisations include the ability to skip sorting steps, using multithreaded parallel (bidirectional) indexing and copying while sorting, and usage of platform-specific intrinsic functions.
// Radix sort in general can be used to sort all array sizes, but is more efficient when applied to somewhat larger arrays compared to other efficient (and stable) comparison-based methods, like introsort.
// See "Performance tests" for more details about array sizes, types and achievable improvements in speed when sorting large arrays with this library.

// Examples of using the 4 templates with simple arrays as input (automatically deduced template parameters are omitted here):
// The rsbd8::radixsort() and rsbd8::radixsortcopy() template wrapper functions (typically) merely allocate memory prior to using the actual sorting functions.
// No intermediate buffer array is required when any variant of rsbd8::radixsortcopynoalloc() is used for sorting 8-bit types.
//
// bool succeeded{rsbd8::radixsort(count, inputarr, pagesizeoptional)};
// bool succeeded{rsbd8::radixsortcopy(count, inputarr, outputarr, pagesizeoptional)};
// rsbd8::radixsortnoalloc(count, inputarr, bufferarr, false);
// rsbd8::radixsortcopynoalloc(count, inputarr, outputarr, bufferarr);

// Examples of using the 4 templates with input from first-level indirection (automatically deduced template parameters are omitted here):
// This library includes common and much less common use scenarios to deal with input from first- and second-level indirection.
// The address offset template parameters (compile-time constants) displace the pointers as acting on a flat "std::byte const *" array, so not as some sort of array indices, to handle some cases with oddly formed structures.
// For the more advanced use cases, an extra argument (run-time, variadic function parameter) can be added to index the indirection when dealing with a (member) pointer/array.
// These index parameters are typically used in a more straightforward manner and use regular indexing.
// The variant with a getter function allows any number of extra arguments to pass on to the getter function.
// Using a getter function that can throw (meaning that it lacks "noexcept") will disable multithreading by this library for the entire sorting operation and will slow down sorting considerably.
// Multithreading can be disabled completely by setting the macro RSBD8_DISABLE_MULTITHREADING.
//
// bool succeeded{rsbd8::radixsort<&myclass::getterfunc>(count, inputarr, pagesizeoptional)};
// bool succeeded{rsbd8::radixsort<&myclass::member>(count, inputarr, pagesizeoptional)};
// bool succeeded{rsbd8::radixsort<uint64_t, addressoffset>(count, inputarr, pagesizeoptional)};
//
// bool succeeded{rsbd8::radixsortcopy<&myclass::getterfunc>(count, inputarr, outputarr, pagesizeoptional, getterparameters...)};
// bool succeeded{rsbd8::radixsortcopy<&myclass::member>(count, inputarr, outputarr, pagesizeoptional)};
// bool succeeded{rsbd8::radixsortcopy<bool, addressoffset>(count, inputarr, outputarr, pagesizeoptional)};
//
// rsbd8::radixsortnoalloc<&myclass::getterfunc>(count, inputarr, bufferarr, false, getterparameters...);
// rsbd8::radixsortnoalloc<&myclass::member>(count, inputarr, bufferarr, false);
// rsbd8::radixsortnoalloc<int16_t, addressoffset>(count, inputarr, bufferarr, false);
//
// rsbd8::radixsortcopynoalloc<&myclass::getterfunc>(count, inputarr, outputarr, bufferarr, getterparameters...);
// rsbd8::radixsortcopynoalloc<&myclass::member>(count, inputarr, outputarr, bufferarr);
// rsbd8::radixsortcopynoalloc<float, addressoffset>(count, inputarr, outputarr, bufferarr);
//
// There are only 16 template functions that almost directly implement sorting with indirection here:
// = rsbd8::radixsortcopynoalloc()
// = rsbd8::radixsortnoalloc()
// These both have 8-bit, 16/24/32/40/48/56/64-bit, (x87) 80-bit (plus padding) and 128-bit type template functions.
// These all have a version that handles straightforward arrays, and a version that handles arrays with indirection.
// All other items are compile-time, multithreading, or inlined wrapper helper tools for these.
// The user of this library can expand on the base functionality by utilising the tools:
// = rsbd8::getoffsetof
// = rsbd8::allocatearray()
// = rsbd8::deallocatearray()
// = rsbd8::buffermemorywrapper

// Examples of using the 4 templates with input from second-level indirection (automatically deduced template parameters are omitted here):
// As the use case for these almost always involve multi-pass techniques, the user is advised to allocate the (reusable) buffers accordingly and avoid the use of radixsortcopy() and radixsort().
// The rsbd8::allocatearray() and rsbd8::deallocatearray() inline function templates are provided for handling an intermediate buffer.
// Again, no intermediate buffer is required when rsbd8::radixsortcopynoalloc() is used for sorting a single-part type.
// These will internally first retrieve a pointer to a "T" type array "T *myarray".
// After that it's dereferenced at the origin (first set of examples) or indexed (second set of examples) as "myarray[indirectionindex]" to retrieve the value used for sorting.
// Again, all "addressoffset" variants as template inputs displace like on a flat "std::byte const *", so not as some sort of array indices.
// The "addressoffset1" item here displaces the pointer in the input array to get the secondary pointer.
// The "addressoffset2" item here displaces the secondary pointer to get the (unfiltered) sorting value.
//
// Examples of using the 2 templates with no indexed second-level indirection:
//
// rsbd8::radixsortcopynoalloc<&myclass::getterfunc, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2>(count, inputarr, outputarr, bufferarr, getterparameters...);
// rsbd8::radixsortcopynoalloc<&myclass::member, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2>(count, inputarr, outputarr, bufferarr);
// rsbd8::radixsortcopynoalloc<long *, addressoffset1, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2>(count, inputarr, outputarr, bufferarr);
//
// rsbd8::radixsortnoalloc<&myclass::getterfunc, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2>(count, inputarr, bufferarr, false, getterparameters...);
// rsbd8::radixsortnoalloc<&myclass::member, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2>(count, inputarr, bufferarr, false);
// rsbd8::radixsortnoalloc<wchar_t *, addressoffset1, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2>(count, inputarr, bufferarr, false);
//
// Examples of using the 2 templates with indexed second-level indirection:
//
// rsbd8::radixsortcopynoalloc<&myclass::getterfunc, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2, true>(count, inputarr, outputarr, bufferarr, indirectionindex, getterparameters...);
// rsbd8::radixsortcopynoalloc<&myclass::member, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2, true>(count, inputarr, outputarr, bufferarr, indirectionindex);
// rsbd8::radixsortcopynoalloc<double *, addressoffset1, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2, true>(count, inputarr, outputarr, bufferarr, indirectionindex);
//
// rsbd8::radixsortnoalloc<&myclass::getterfunc, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2, true>(count, inputarr, bufferarr, false, indirectionindex, getterparameters...);
// rsbd8::radixsortnoalloc<&myclass::member, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2, true>(count, inputarr, bufferarr, false, indirectionindex);
// rsbd8::radixsortnoalloc<unsigned long *, addressoffset1, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2, true>(count, inputarr, bufferarr, false, indirectionindex);
//
// Examples of using the 2 templates with indexed first- and second-level indirection:
//
// rsbd8::radixsortcopynoalloc<&myclass::member, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2, true>(count, inputarr, outputarr, bufferarr, indirectionindex1, indirectionindex2);
// rsbd8::radixsortcopynoalloc<long double *, addressoffset1, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2, true>(count, inputarr, outputarr, bufferarr, indirectionindex1, indirectionindex2);
//
// rsbd8::radixsortnoalloc<&myclass::member, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2, true>(count, inputarr, bufferarr, false, indirectionindex1, indirectionindex2);
// rsbd8::radixsortnoalloc<short *, addressoffset1, rsbd8::sortingdirection::ascfwdorder, rsbd8::sortingmode::native, addressoffset2, true>(count, inputarr, bufferarr, false, indirectionindex1, indirectionindex2);

// The 4 main sorting template functions that are implemented here
// = radixsortnoalloc():
// --- counted (first parameter "count", the end of arrays are no inputs to these functions unlike some sorting functions)
// --- sorts an array (second parameter "input")
// --- uses an array as a buffer of the same size and type (third parameter "buffer")
// --- with a toggle to output to either the input array or the buffer array (optional fourth parameter "movetobuffer")
// --- the array that is not selected for output contains garbage afterwards (typically the leftovers from an intermediate sorting stage)
// --- both arrays need to be writable, but when using indirection the members can be const-qualified
// = radixsortcopynoalloc():
// --- counted (first parameter "count")
// --- similar to radixsortnoalloc(), but will not write to the input array, which can be const-qualified (second parameter "input")
// --- uses a dedicated output array of the same size (third parameter "output")
// --- uses a memory buffer of the same size, which contains garbage afterwards (fourth parameter)
// = radixsort():
// --- wrapper template for radixsortnoalloc()
// --- only allocates memory for the buffer parameter
// --- (Windows-only) large page size for VirtualAlloc() can be used if enabled with the lock memory privilege for the application enabled (optional third parameter)
// --- (POSIX implementing systems-only) flags for enabling pages with huge TLB functionality for mmap() can be used (optional third parameter)
// = radixsortcopy():
// --- wrapper template for radixsortcopynoalloc()
// --- only allocates memory for the buffer parameter
// --- (Windows-only) large page size for VirtualAlloc() can be used if enabled with the lock memory privilege for the application enabled (optional third parameter)
// --- (POSIX implementing systems-only) flags for enabling pages with huge TLB functionality for mmap() can be used (optional third parameter)

// Modes of operation for the template functions
namespace rsbd8{
// All sorting functions here are templates with a compile-time constant sorting mode and direction.
enum struct sortingmode : unsigned char{// 5 bits as bitfields, bit 6 is used to select automatic modes (64 and greater)
// The three generic modes that can be activated are:
	native = 64,
// = automatic unsigned integer, signed integer or floating-point, depending on input type (default)
	nativeabs = 65,
// = automatic unsigned integer, absolute signed integer or absolute floating-point, depending on input type
// - (no distinct effect when used on an unsigned integer input type)
	nativetieredabs = 66,
// = automatic unsigned integer, absolute signed integer or absolute floating-point, depending on input type, but negative inputs will sort just below their positive counterparts
// - (no distinct effect when used on an unsigned integer input type)
//
// The five regular modes that can be activated are:
	forceunsigned = 0,
// = regular unsigned integer (default for unsigned input types)
	specialsigned = forceunsigned,
// - and also inside-out signed integer
// - (sorts ascending from 0, maximum value, minimum value, to -1)
	forcesigned = 1 << 1,
// = regular signed integer (default for signed input types)
	forceabssigned = 1 | 1 << 1,
// = absolute signed integer:
	forcefloatingp = 1 << 1 | 1 << 2,
// = regular floating-point (default for floating-point input types)
	forceabsfloatingp = 1 | 1 << 1 | 1 << 2,
// = absolute floating-point
	specialunsigned = forceabsfloatingp,
// - and also unsigned integer without using the top bit
//
// The three special modes that can be activated are:
	specialfloatingp = 1 << 2,
// = inside-out floating-point
// - (sorts ascending from +0., +infinity, +NaN, -NaN, -infinity, to -0.)
	forcetieredabsfloatingp = 1 | 1 << 2,
// = absolute floating-point, but negative inputs will sort just below their positive counterparts
// - (sorts ascending from -0., +0., -infinity, +infinity, to various -NaN or +NaN values at the end)
	forcetieredabssigned = 1
// = absolute signed integer, but negative inputs will sort just below their positive counterparts
// - (sorts ascending from 0, -1, 1, -2, 2, and so on, will work correctly for minimum values)
};
// The two reversing modes are:
enum struct sortingdirection : unsigned char{// 2 bits as bitfields
// = isdescsort (default false): reverse the sorting direction
// = isrevorder (default false): reverse the array direction when sorting items with the same value (only used when dealing with indirection)
// Enabling isdescsort costs next to nothing in terms of performance, isrevorder does initially take minor extra processing when handling multi-part types.
	ascfwdorder = 0,
// = isdescsort = false, isrevorder = false: stable sort, low to high (default)
	dscrevorder = 1 | 1 << 1,
// = isdescsort = true, isrevorder = true: stable sort, high to low, the complete opposite direction of the default functionality
	dscfwdorder = 1,
// = isdescsort = true, isrevorder = false: stable sort, high to low, but keeps items with the same value in the same order as in the source
	ascrevorder = 1 << 1
// = isdescsort = false, isrevorder = false: stable sort, low to high, but reverses items of the same value compared to the order in the source
// This last combination is very uncommon, but could be useful in some rare cases.
};
// To give an example of isdescsort = true, isrevorder = false, as it's a bit tricky to imagine without a reference:
// myclass collA[]{{1, "first"}, {1, "second"}, {-5, "third"}, {2, "fourth"}};// list construct
// myclass *pcollA[]{collA, collA + 1, collA + 2, collA + 3};// list pointers
// rsbd8::radixsortnoalloc<&myclass::keyorder, rsbd8::dscfwdorder>(4, pcollA, psomeunusedbuffer);
// Members of "pcollA" will then get sorted according to their value "keyorder", in reverse order, while keeping the same array order.
// Pointers will in this case point to: {2, "fourth"}, {1, "first"}, {1, "second"}, {-5, "third"}.
// That is different from fully reversing the order when using this line instead of the above:
// rsbd8::radixsortnoalloc<&myclass::keyorder, rsbd8::dscrevorder>(4, pcollA, psomeunusedbuffer);
// Pointers will in this case point to: {2, "fourth"}, {1, "second"}, {1, "first"}, {-5, "third"}.
// Notice the same reverse stable sorting here, but opposite placement when encountering the same value multiple times.
}// namespace rsbd8

// Miscellaneous notes
// Sorting unsigned values is the fastest, very closely followed up by signed values, followed up by floating-point values in this library.
// Unsigned 128-bit and larger integers can be sorted by sequential sorting from the bottom to the top parts as unsigned (64-bit) elements when using indirection.
// Signed (but otherwise unfiltered) 128-bit and larger integers are sorted the same, with only the topmost (64-bit) element sorted as signed because of the sign bit (assuming unfiltered input).
// Likewise, regular absolute-filtered floating-point 128-bit and larger types can be sorted like that as a top part with one or more unsigned bottom parts.
// Re-use the same intermediate buffer combined with radixsortnoalloc() or radixsortcopynoalloc() when sorting 128-bit and larger integers like this.
// Inputs of type bool are reinterpreted as the unsigned integer type of the same size, but handling them is extremely efficient anyway.
// Anything but 0 or 1 in bool source data will impact sorting, but this only happens if the user deliberately overrides the compiler behaviour for bool data.
// The sign of type char is ambiguous, as by the original C standard, so cast inputs to char8_t * or unsigned char *, or force unsigned processing modes if unambiguously a binary sort of char characters is desired.
// Floating-point NaN values are sorted before negative infinity for the typical machine-generated "undefined" QNaN (0xFFF8'0000'0000'0000 on an IEEE double).
// Floating-point NaN positive values (implies not machine-generated) are sorted after positive infinity (0x7FF0'0000'0000'0001 and onward on an IEEE double).
// Floating-point SNaN (signalling) values do not trigger signals inside these functions. (Several functions in namespace std can do that.)

// Naming and tooling conventions used in this library
// Textual:
// All names are lowercase, with no separators, and any sort of length, based on convenience or frequency of local usage.
// Any sort of names on a global or namespace scope are longer, to provide some context.
// All user-facing functions have a general description in a comment in the line above them. This is generally what any IDE will display when giving a tooltip.
// The few macro definitions are all uppercase, separated by underscores, starting with "RSBD8_".
// There are most certainly no limitations on the lengths of functions, lines, comment blocks or other items. Everything is just kept sensible, in proper order, never spaghettified or obscured much, feature-rich (even if a little complicated sometimes), and very much optimised.
// This library undefines its macro definitions at the end of the file and does not need to expose any macro definitions externally.
// This library has three imported code sections that are clearly marked at the beginning and end. These are edited sparingly, unless they get replaced again with a newer version.
// Namespaces:
// rsbd8:: is radixsortbidi8, the enveloping namespace for this library
// rsbd8::helper:: is the underlying namespace for helper functions and constants. These are usually not directly invoked by the user of this library. No efforts are made to actually hide items from the user though.
// Templates:
// <typename T, typename U, typename V, typename W>
// These are the basic 4 placeholders for types.
// T is used for the main input type, or the primary type being referred to.
// U is the deferred type, for example the unsigned variant of T, or an unsigned general utility type that is larger than T.
// V is always used as the class type of input and output arrays when dealing with indirection.
// W is the wildcard type, often an automatically deduced item in templated helper functions, but also often just a companion type to U.
// Some other template parts have defaults set up for them, especially for the longer lists of template parameters. This provides the user with often having less verbosity in their code.
// Template variable arguments as a C++ feature are used extensively by this library. Function-based variable arguments passing as inherited from original C isn't needed.
// All sorts of levels and configurations of template metaprogramming (C++17 and onward) are used in this library for optimisation, enhancing debugging or just making the code more concise.
// Functional:
// This is an optimal performance library. Of course, minor performance setbacks may arise from just compiler interpretation or functional issues. Keep the code close to the bare metal, in the right order to easily compile to instructions and use tons of optimisation features to overcome such issues.
// Even if many parts here just don't comply with most of the programming world's "clean" code rules, performance is key first, reducing the total count of functions but balanced with maintainability is second, and being descriptive of functionality in a combination of code and comments is third.
// Defensive programming here is mostly left to template metaprogramming and compile-time assertions, as compile-time items don't hurt performance.
// Parameter and environment checking is left to the absolute minimum outside of the debug mode. Of course, memory allocation failures are handled, and likewise the cases of potentially throwing functions.

// Notes on ongoing research and development
//
// TODO, add support for types larger than 128 bits
// = TODO, currently all functions here are guarded with an upper limit of 8-, 64-, 80, 96- or 128-bit (using std::enable_if sections). Future functionality will either require lifting the limits on current functions, or adding another set of functions for the larger data types. Given that radix sort variants excel at processing large data types compared to comparison-based sorting methods, do give this some priority in development.
//
// TODO, document computer system architecture-dependent code parts and add more options
// = TODO, the current mode for pipelining (ILP - instruction level processing) is set to not need more than 15 integer registers, of which are 8 to 11 "hot" for parallel processing. This isn't a universally ideal option. To name two prominent systems, 32-bit x86 only has 7 available general-purpose registers, while ARM AArch64 (ARM64) has 31.
// = TODO, investigate SIMD, in all of its shapes and sizes. Some experimentation has been done with x64+AVX-512 in an early version, but compared to other optimisations and strategies it never yielded much for these test functions.
// = TODO, the current version of this library does not provide much optimisation for processing any 64-bit (or larger) types on 32-bit systems at all. This can be documented, and later on optimised.
// = TODO, similarly, 16-bit systems still exist. (Even though these often do include a capable 32-bit capable data path line, see the history of x86 in this regard for example.) If this library can be optimised for use in a reasonably current 16-bit microcontroller, document and later on optimise for it.
// = TODO, add more platform-dependent, optimised code sequences here similar to the current collections in the rsbd8::helper namespace.
// = TODO, test and debug this library on more machines, platforms and such. Functionality and performance should both be guaranteed.
//
// TODO, this is a C++17 minimum library, but more modern features are welcome
// = TODO, bumping up the library to C++20 minimum isn't advised (yet), but could be in the near future.
// = TODO, C++23 features currently don't add much over some of the improvements seen in C++20, but for example indexed varargs from C++26 could certainly provide some simplification in a few functions here. Adding more modern C++ features (even if as optional items for now) is welcome.
//
// TODO, add support for non-array inputs
// = TODO, as the basic std::sort and std::stable_sort variants already support this functionality, it could be an advantage to add support for the other C++ iterable data sets.
// = TODO, performance testing and use case investigation is required for this subject, as radix sort types only really work well on somewhat larger arrays, and probably other larger iterable data sets, too.

// Extended filtering information for each of the 8 main modes
//
// Modes of operation for the template functions
// = regular unsigned integer "forceunsigned" (default for unsigned input types)
// - and also inside-out signed integer "specialsigned"
// - (sorts ascending from 0, maximum value, minimum value, to -1):
// --- isabsvalue = false, issignmode = false, isfltpmode = false
// --- lowest amount of filtering cost
// --- straightforward process, no filter at all
// = regular signed integer "forcesigned" (default for signed input types):
// --- isabsvalue = false, issignmode = true, isfltpmode = false
// --- lower amount of filtering cost
// --- no filter in the processing phases
// --- virtually flips the most significant bit when calculating offsets
// = absolute signed integer "forceabssigned":
// --- isabsvalue = true, issignmode = true, isfltpmode = false
// --- medium amount of filtering cost
// --- creates a sign bit mask, adds it to the input and uses it with xor on the input as a filter in the processing phases
// = regular floating-point "forcefloatingp" (default for floating-point input types):
// --- isabsvalue = false, issignmode = true, isfltpmode = true
// --- higher amount of filtering cost
// --- creates a sign bit mask and uses it with xor on the exponent and mantissa bits as a filter in the processing phases
// --- virtually flips the most significant bit when calculating offsets
// = absolute floating-point "forceabsfloatingp"
// - and also unsigned integer without using the top bit "specialunsigned":
// --- isabsvalue = true, issignmode = true, isfltpmode = true
// --- low amount of filtering cost
// --- masks out the sign bit in the processing phases
//
// The three special modes that can be activated are:
// = inside-out floating-point "specialfloatingp"
// - (sorts ascending from +0., +infinity, +NaN, -NaN, -infinity, to -0.):
// --- isabsvalue = false, issignmode = false, isfltpmode = true
// --- higher amount of filtering cost
// --- creates a sign bit mask and uses it with xor on the exponent and mantissa bits as a filter in the processing phases
// = absolute floating-point, but negative inputs will sort just below their positive counterparts "forcetieredabsfloatingp"
// - (sorts ascending from -0., +0., -infinity, +infinity, to various -NaN or +NaN values at the end):
// --- isabsvalue = true, issignmode = false, isfltpmode = true
// --- medium amount of filtering cost
// --- bit rotates left by one to move the sign bit to the least significant bit in the processing phases
// --- virtually flips the least significant bit when calculating offsets
// = absolute signed integer, but negative inputs will sort just below their positive counterparts "forcetieredabssigned"
// - (sorts ascending from 0, -1, 1, -2, 2, and so on, will work correctly for minimum values):
// --- isabsvalue = true, issignmode = false, isfltpmode = false
// --- medium amount of filtering cost
// --- creates a sign bit mask, shifts the input left by one and uses xor on the input with the sign bit mask as a filter in the processing phases
//
// Enabling reverse ordering on these modes will add slightly more to the initial filtering cost.
// For example, the highest amount of filtering cost would be on the floating-point full reverse mode.
// Take "highest amount" with a grain of salt though, as way more complicated filtering stages can be designed than what is used for this combination of filters.
//
// absolute floating-point, but negative inputs will sort just below their positive counterparts operates differently
// Bit rotate left by one is the first filtering step to make this possible.
// Sort as unsigned, just with the least significant bit flipped to complete the filter.
// As an example of this, the 8-bit sorting pattern in ascending mode:
// 0b1000'0000 -0.
// 0b0000'0000 +0.
// 0b1000'0001 -exp2(1 - mantissabitcount - exponentbias)
// 0b0000'0001 +exp2(1 - mantissabitcount - exponentbias)
// ...
// 0b1111'1111 -QNaN (maximum amount of ones)
// 0b0111'1111 +QNaN (maximum amount of ones)

// Performance tests
// This library has a performance test suite used for development.
// These performance test results are for sorting a block of 1 GiB, with fully random bits in integer and floating-point arrays (with no indirection or filtering).
// std::stable_sort() vs rsbd8::radixsort(), measured in 100 ns units:
// float :_ 79341528806 vs 5068443726, a factor of 15.65 in speedup
// double:_ 51521014035 vs 5284085860, a factor of 9.750 in speedup
// uint64:_ 50518618540 vs 4882182872, a factor of 10.35 in speedup
// int64 :_ 50963293135 vs 5159867120, a factor of 9.877 in speedup
// uint32: 101573004007 vs 3701128500, a factor of 27.44 in speedup
// int32 : 103658239227 vs 3801618261, a factor of 27.27 in speedup
// uint16: 155841701982 vs 3233975799, a factor of 48.19 in speedup
// int16 : 149563180451 vs 3191150398, a factor of 46.87 in speedup
// uint8 : 213574532756 vs 3039381578, a factor of 70.27 in speedup
// int8 _: 211616058503 vs 2946007896, a factor of 71.83 in speedup
// A radix sort with indirection, with its relatively fewer memory accesses compared to a comparison-based sort, will definitely often be one of the most optimal choices.
// However, sorting with indirection is slower than sorting without indirection, as expected.
// simple tests of the first-level indirection rsbd8::radixsort() vs the direct variant above, measured in 100 ns units:
// uint64: 20172428670 vs 4882182872, a factor of 4.132 in slowdown, purely because of indirection
// double: 20779336651 vs 5284085860, a factor of 3.932 in slowdown, purely because of indirection
//
// The next tests were done on smaller blocks.
// There will be a minimum amount of array entries where rsbd8::radixsort() starts to get the upper hand in speed over std::stable_sort().
// These test results were obtained by performance testing on multiple sizes of blocks between .5 to 8 KB, with again fully random bits in unsigned integer and floating-point arrays (with no indirection):
// float : 875 array entries
// double: 600 array entries
// uint64: 700 array entries
// uint32: 400 array entries
// uint16: 375 array entries
// uint8 : 300 array entries
// Interpreting this means that radix sort variants will be faster for somewhat larger arrays when sorting data under the given conditions.
// In this case that's a sequence of just plain numbers in an array.
// When dealing with sorting while using indirection or filtering, test results will vary.
//
// System configuration data:
// Performance testing was done on 2025-11-17 on development PC 1:
// = Intel Core i9 11900K, specification string: 11th Gen Intel Core i9-11900K @ 3.50GHz
// = Corsair CMK16GX4M2B3200C16 * 2, 32 GiB, 1600 MHz (XMP-3200 scheme), DDR4
// = ASRock Z590 PG Velocita, UEFI version L1.92
// = Windows 11 Home, 10.0.26100.6584
// - The CPU was locked to run at 3.5 GHz on all cores in the UEFI, without boosts or throttling during testing.
// - Some background programs were disabled, and the main testing was done by just running the test executable and letting DebugView x64 do the readout
// - DebugView is also useful at keeping a record on any outputs generated by other simultaneous processes to analyse some possible disturbances. If any such disturbances were detected, the test run was discarded and re-done.

// Table of contents (searchable)
//
// = MIT License
// = Radixsortbidi8
// = Examples of using the 4 templates with simple arrays as input
// = Examples of using the 4 templates with input from indirection
// = Examples of using the 4 templates with input from modified indirection
// = Bonus example of the longest regular item, with complete decoration of the template
// = The 4 main sorting template functions that are implemented here
// = Modes of operation for the template functions
// = Miscellaneous notes
// = Naming and tooling conventions used in this library
// = Notes on ongoing research and development
// = Extended filtering information for each of the 8 main modes
// = Performance tests
// = Table of contents
//
// = Per-compiler function attributes
// = Include statements and the last checks for compatibility
//
// = Helper constants and functions
// = Helper functions to implement the 8 main modes
// = Helper functions to implement the offset transforms
// = Function implementation templates for multi-part types
// = Function implementation templates for single-part types
//
// = Definition of the GetOffsetOf template
// = Generic large array allocation and deallocation functions
// = Wrapper template functions for the main sorting functions in this library
//
// = Library finalisation

// Per-compiler function attributes
// RSBD8_FUNC_INLINE is suitable to attempt force inlining of any function.
// RSBD8_FUNC_NORMAL is specifically for template functions in this header-only library, and doesn't include even a regular "inline" statement to prevent linking issues for non-template functions.
// These are the only two macros defined in this file, and #undef statements are used for them at the end.
#if defined(DEBUG) || defined(_DEBUG)// This part is debug-only. These are non-standard conforming macros, but note that the "NDEBUG" rule for detecting non-debug builds should only ever apply to runtime assert() statments in C++.
#ifdef _MSC_VER
#define RSBD8_FUNC_INLINE __declspec(noalias safebuffers) inline
#define RSBD8_FUNC_NORMAL __declspec(noalias safebuffers)
#else
#define RSBD8_FUNC_INLINE inline
#define RSBD8_FUNC_NORMAL
#endif
#else// release-only
#ifdef __clang__
#define RSBD8_FUNC_INLINE [[gnu::always_inline]] [[gnu::gnu_inline]] inline
#define RSBD8_FUNC_NORMAL
#elif defined(__GNUC__)
#define RSBD8_FUNC_INLINE [[gnu::always_inline]] inline
#define RSBD8_FUNC_NORMAL
#elif defined(__xlC__) || defined(__ghs__) || defined(__KEIL__) || defined(__CA__) || defined(__C166__) || defined(__C51__) || defined(__CX51__)
#define RSBD8_FUNC_INLINE inline __attribute__((always_inline))
#define RSBD8_FUNC_NORMAL
#elif defined(_MSC_VER)
#pragma warning(error: 4714)
#define RSBD8_FUNC_INLINE __declspec(noalias safebuffers) __forceinline
#define RSBD8_FUNC_NORMAL __declspec(noalias safebuffers)
#else
#define RSBD8_FUNC_INLINE inline
#define RSBD8_FUNC_NORMAL
#endif
#endif

// Include statements and the last checks for compatibility
// Compiler features minimum requirements are evaluated during compile-time if part of C++14 and newer.
// A more difficult test to implement here would be for example to detect mixed endianness between floating-point double and other data for on older ARM platforms.
// The C++20 "std::endian" parts in the "bit" header currently unfortunately don't indicate more than little, big and undefined mixed endianness.
namespace rsbd8::helper{// Avoid putting any include files into this library's namespace.
	RSBD8_FUNC_INLINE void spinpause();// simple forward declaration for the spinlocks used in multithreaded processing
}
//
// C++17 features detection
#if 201703L > __cplusplus
// Microsoft C/C++-compatible compilers don't set the __cplusplus predefined macro conforming to the standard by default for some very outdated legacy code reasons.
// /Zc:__cplusplus can correct it, but it's not part of the regular "Standards conformance" /permissive- compiler options.
// Use its internal macro here as a temporary fix.
#if !defined(_MSVC_LANG) || 201703L > _MSVC_LANG
#error Compiler does not conform to C++17 to compile this library.
#endif
#endif
// limited to C++17
#include <cassert>
#include <climits>
#include <cfloat>
#include <future>
#include <atomic>
#include <functional>
#include <new>
#if !defined(_WIN32) && defined(_POSIX_C_SOURCE)// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
#include <sys/types.h>
#include <sys/mman.h>
#endif
#if CHAR_BIT & 8 - 1
#error This platform has an addressable unit that isn't divisible by 8. For these kinds of platforms it's better to re-write this library and not use an 8-bit indexed radix sort method.
#endif
#ifndef UINTPTR_MAX
#error This platform has no uintptr_t type, which should be near impossible. This library can be edited to get around that, however it might be more advantageous to edit the compiler.
#endif
#if 202002L <= __cplusplus || defined(_MSVC_LANG) && 202002L <= _MSVC_LANG
// limited to C++20
#include <bit>// (C++20)
// Library feature-test macros (C++20)
//
// Structured binding declaration (C++17)
#ifndef __cpp_structured_bindings
#error Compiler does not meet requirements for __cpp_structured_bindings for this library.
#endif
// std::byte (C++17)
#ifndef __cpp_lib_byte
#error Compiler does not meet requirements for __cpp_lib_byte for this library.
#endif
// std::is_nothrow_invocable_v (C++17)
// std::invoke_result_t (C++17)
#ifndef __cpp_lib_is_invocable
#error Compiler does not meet requirements for __cpp_lib_is_invocable for this library.
#endif
// std::is_same_v (C++17)
// std::is_integral_v (C++17)
// std::is_unsigned_v (C++17)
// std::is_signed_v (C++17)
// std::is_floating_point_v (C++17)
#ifndef __cpp_lib_type_trait_variable_templates
#error Compiler does not meet requirements for __cpp_lib_type_trait_variable_templates for this library.
#endif
// std::conditional_t (C++14)
// std::enable_if_t (C++14)
// std::make_unsigned_t (C++14)
// std::make_signed_t (C++14)
#ifndef __cpp_lib_transformation_trait_aliases
#error Compiler does not meet requirements for __cpp_lib_transformation_trait_aliases for this library.
#endif
//
// Compiler features under consideration for usage in a newer version of the library:
// std::endian (C++20)
// __cpp_lib_endian
//
// Compiler features that are not required, but will be used conditionally:
// std::bit_width (C++20)
// __cpp_lib_int_pow2
// std::countr_zero (C++20)
// __cpp_lib_bitops
// std::bit_cast (C++20)
// __cpp_lib_bit_cast
// [[likely]] (C++20)
// __has_cpp_attribute(likely)
// [[unlikely]] (C++20)
// __has_cpp_attribute(unlikely)
// [[nodiscard]] (C++17)
// __has_cpp_attribute(nodiscard)
// [[maybe_unused]] (C++17)
// __has_cpp_attribute(maybe_unused)
#endif
// Get compiler- and platform-specific intrinsic functions headers:
// (many items here are imported code, and this part also generates the spinpause() function)
#if defined(_MSC_VER)
// Microsoft C/C++-compatible compiler
#include <intrin.h>
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
RSBD8_FUNC_INLINE void rsbd8::helper::spinpause(){YieldProcessor();}
#elif (defined(_M_IX86) && !defined(_M_HYBRID_X86_ARM64)) || (defined(_M_X64) && !defined(_M_ARM64EC))
RSBD8_FUNC_INLINE void rsbd8::helper::spinpause(){_mm_pause();}
#elif defined(_M_ARM64) || defined(_M_ARM64EC) || defined(_M_HYBRID_X86_ARM64)
RSBD8_FUNC_INLINE void rsbd8::helper::spinpause(){__dmb(_ARM64_BARRIER_ISHST); __yield();}
#elif defined(_M_ARM)
RSBD8_FUNC_INLINE void rsbd8::helper::spinpause(){__dmb(_ARM_BARRIER_ISHST); __yield();}
#else
#error Unsupported system architecture for this compiler. Edit this library to add support for it.
#endif

#elif defined(__armel__) || defined(__ARMEL__)
// avoid using anything for this old ARM target
RSBD8_FUNC_INLINE void rsbd8::helper::spinpause(){}

#elif (defined(__GNUC__) || defined(__clang__)) && (defined(__x86_64__) || defined(__i386__))
// GCC/Clang-compatible compiler, targeting x86/x86-64
#include <x86intrin.h>
RSBD8_FUNC_INLINE void rsbd8::helper::spinpause(){_mm_pause();}

#elif  (defined(__GNUC__) || defined(__clang__)) && (defined(__ARM_NEON__) || defined(__aarch64__))
// GCC/Clang-compatible compiler, targeting ARM with NEON
#include <arm_neon.h>
#if defined (MISSING_ARM_VLD1)
#include <ATen/cpu/vec256/missing_vld1_neon.h>
#elif defined (MISSING_ARM_VST1)
#include <ATen/cpu/vec256/missing_vst1_neon.h>
#endif
RSBD8_FUNC_INLINE void rsbd8::helper::spinpause(){
	__asm__ __volatile__ ("isb sy" ::: "memory");
	__asm__ __volatile__ ("yield" ::: "memory");
}

#elif (defined(__GNUC__) || defined(__clang__)) && defined(__IWMMXT__)
// GCC/Clang-compatible compiler, targeting ARM with WMMX
#include <mmintrin.h>
RSBD8_FUNC_INLINE void rsbd8::helper::spinpause(){
	__asm__ __volatile__ ("isb sy" ::: "memory");
	__asm__ __volatile__ ("yield" ::: "memory");
}

#elif (defined(__GNUC__) || defined(__clang__)) && (defined(__arm__) || (defined(__ARM_ARCH) && __ARM_ARCH >= 8) || defined(__ARM_ARCH_8A__))
// GCC/Clang-compatible compiler, targeting other ARM
RSBD8_FUNC_INLINE void rsbd8::helper::spinpause(){
	__asm__ __volatile__ ("isb sy" ::: "memory");
	__asm__ __volatile__ ("yield" ::: "memory");
}

#elif (defined(__GNUC__) || defined(__clang__) || defined(__xlC__)) && (defined(__VEC__) || defined(__ALTIVEC__))
// GCC/Clang/XLC-compatible compiler, targeting PowerPC with VMX/VSX
#include <altivec.h>
RSBD8_FUNC_INLINE void rsbd8::helper::spinpause(){
	__asm__ __volatile__ ("or 27,27,27" ::: "memory");
}

#elif (defined(__GNUC__) || defined(__clang__)) && defined(__SPE__)
// GCC/Clang-compatible compiler, targeting PowerPC with SPE
#include <spe.h>
RSBD8_FUNC_INLINE void rsbd8::helper::spinpause(){
	__asm__ __volatile__ ("or 27,27,27" ::: "memory");
}

#elif (defined(__GNUC__) || defined(__clang__) || defined(__xlC__)) && (defined(__powerpc__) || defined(__ppc__) || defined(__PPC__))
// GCC/Clang/XLC-compatible compiler, targeting other PowerPC
RSBD8_FUNC_INLINE void rsbd8::helper::spinpause(){
	__asm__ __volatile__ ("or 27,27,27" ::: "memory");
}

#elif (defined(__GNUC__) || defined(__clang__)) && defined(__ia64__)  // IA64
// GCC/Clang-compatible compiler, targeting IA-64
RSBD8_FUNC_INLINE void rsbd8::helper::spinpause(){
	__asm__ __volatile__ ("hint @pause" ::: "memory");
}

#elif (defined(__GNUC__) || defined(__clang__)) && defined(__riscv) && __riscv_xlen == 64
// GCC/Clang-compatible compiler, targeting RISC-V 64
RSBD8_FUNC_INLINE void rsbd8::helper::spinpause(){
#ifdef __riscv_zihintpause
	__asm__ __volatile__ ("pause" ::: "memory");
#else
	__asm__ __volatile__ (".4byte 0x100000F" ::: "memory");// also just the encoding of "pause"
#endif
}

#else
// fallback, with warning
#pragma message("Compiler and system architecture not detected. Edit this library to add support for it.")
RSBD8_FUNC_INLINE void rsbd8::helper::spinpause(){}

#endif
namespace rsbd8{// Avoid putting any include files into this library's namespace.

// Helper constants and functions
namespace helper{// This libary defines a number of helper items, so categorise them as such.

// Atomic light barrier for spinpause()-based loops (see above for that function)
// This assumes that the initial state of atomiclightbarrier is zero.
template<size_t threadnumber, size_t threadcount, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	threadnumber < threadcount &&// 0-based threadnumber, make sure to give every thread a unique, sequential number
	1 < threadcount &&// at least 2 threads
	std::is_integral_v<T> &&
	std::numeric_limits<std::make_unsigned_t<T>>::max()	 >= threadcount - 1,
	void> simplebarrier(std::atomic<T> &atomiclightbarrier){
	static T constexpr val{(threadnumber == threadcount - 1)? ~static_cast<T>(threadnumber) + 1 : static_cast<T>(1)};
	T old{atomiclightbarrier.fetch_add(val)};// the only modification here
	if(~val + 1 != old) do{// two's complement negation comparison
		spinpause();
	}while(atomiclightbarrier.load(std::memory_order_relaxed));
};

// Integer binary logarithm of the pointer size constant
unsigned char constexpr log2ptrs{
#ifdef __cpp_lib_int_pow2// (C++20)
	static_cast<unsigned char>(std::bit_width(sizeof(void *) - 1))
#else
	32 == sizeof(void *)? 5 :
	16 == sizeof(void *)? 4 :
	8 == sizeof(void *)? 3 :
	4 == sizeof(void *)? 2 :
	2 == sizeof(void *)? 1 :
	1 == sizeof(void *)? 0 :
	static_cast<unsigned char>(~0)
#endif
};

// Utility structs to generate tests for the often padded 80-bit long double types
// Platforms with a native 80-bit long double type are all little endian, hence that is the only implementation here.
struct alignas(16) longdoubletest128{uint_least64_t mantissa; uint_least16_t signexponent; uint_least16_t padding[3];};
struct longdoubletest96{uint_least32_t mantissa[2]; uint_least16_t signexponent; uint_least16_t padding;};
struct longdoubletest80{uint_least16_t mantissa[4]; uint_least16_t signexponent;};

// Endianess detection
// A dirty method that heavily relies on proper inlining and compiler optimisation of that, but it at least can detect the floating-point mixed endianness cases if used properly
template<typename T>
constexpr RSBD8_FUNC_INLINE std::enable_if_t<
	128 >= CHAR_BIT * sizeof(T) &&
	std::is_integral_v<typename std::conditional_t<std::is_enum_v<T>,
		std::underlying_type<T>,
		std::enable_if<true, T>>::type>,
	T> generatehighbit(){
	return{static_cast<T>(1) << (CHAR_BIT * sizeof(T) - 1)};
}
template<typename T>
constexpr RSBD8_FUNC_INLINE std::enable_if_t<
	128 >= CHAR_BIT * sizeof(T) &&
	std::is_floating_point_v<typename std::conditional_t<std::is_enum_v<T>,
		std::underlying_type<T>,
		std::enable_if<true, T>>::type>,
	T> generatehighbit(){
	// This will definately not work on some floating-point types from machines from the digial stone age.
	// However, this is a C++17 and onwards compatible library, and those devices have none of that.
	return{static_cast<T>(-0.)};
}
template<typename T>
constexpr RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T>,
	T> generatehighbit(){
	T out{};
	out.signexponent = 0x8000u;
	return{out};
}

// Utility templates to create an immediate member object pointer for the type and offset indirection wrapper functions
#pragma pack(push, 1)
template<typename T, ptrdiff_t indirection1> struct memberobjectgenerator;
template<typename T>
struct memberobjectgenerator<T, 0>{
	T object;// no padding, as the object starts at the origin
};
template<typename T, ptrdiff_t indirection1>
struct memberobjectgenerator{
	std::byte padding[static_cast<size_t>(indirection1)];// this will work, but array counts are just required to be positive
	T object;// some amount of padding is used
};
#pragma pack(pop)

// Utility templates to call the getter function while optionally splitting off the second-level indirection index parameter, or dereference the member object pointer
// These will all reinterpret references as pointers.
template<auto indirection1, bool isindexed2, typename V, typename U, typename W>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_object_pointer_v<decltype(indirection1)> &&
	isindexed2 &&
	!std::is_lvalue_reference_v<decltype(std::declval<V *>()->*indirection1)>,
	std::remove_reference_t<decltype(std::declval<V *>()->*indirection1)>> splitget(V *p, U index1, W index2)noexcept{
	using T = std::remove_reference_t<decltype(std::declval<V *>()->*indirection1)>;
	// do not pass a nullptr here
	assert(p);
	return{reinterpret_cast<V *>(reinterpret_cast<T *>(p) + index1)->*indirection1};
}
template<auto indirection1, bool isindexed2, typename V, typename U, typename W>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_object_pointer_v<decltype(indirection1)> &&
	isindexed2 &&
	std::is_lvalue_reference_v<decltype(std::declval<V *>()->*indirection1)>,
	std::remove_reference_t<decltype(std::declval<V *>()->*indirection1)> *> splitget(V *p, U index1, W index2)noexcept{
	using T = std::remove_reference_t<decltype(std::declval<V *>()->*indirection1)>;
	// do not pass a nullptr here
	assert(p);
	return{reinterpret_cast<T *>(&(reinterpret_cast<V *>(reinterpret_cast<T *>(p) + index1)->*indirection1))};// always reinterpret references as pointers
}
template<auto indirection1, bool isindexed2, typename V, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_object_pointer_v<decltype(indirection1)> &&
	!std::is_lvalue_reference_v<decltype(std::declval<V *>()->*indirection1)>,
	std::remove_reference_t<decltype(std::declval<V *>()->*indirection1)>> splitget(V *p, U index)noexcept{
	using T = std::remove_reference_t<decltype(std::declval<V *>()->*indirection1)>;
	// do not pass a nullptr here
	assert(p);
	if constexpr(isindexed2){
		static_assert(std::is_pointer_v<T>, "invalid variable argument count for usage without second-level indirection");
		return{p->*indirection1};
	}else{
		return{reinterpret_cast<V *>(reinterpret_cast<T *>(p) + index)->*indirection1};
	}
}
template<auto indirection1, bool isindexed2, typename V, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_object_pointer_v<decltype(indirection1)> &&
	std::is_lvalue_reference_v<decltype(std::declval<V *>()->*indirection1)>,
	std::remove_reference_t<decltype(std::declval<V *>()->*indirection1)> *> splitget(V *p, U index)noexcept{
	using T = std::remove_reference_t<decltype(std::declval<V *>()->*indirection1)>;
	// do not pass a nullptr here
	assert(p);
	if constexpr(isindexed2){
		return{reinterpret_cast<T *>(&(p->*indirection1))};// always reinterpret references as pointers
	}else{
		return{reinterpret_cast<T *>(&(reinterpret_cast<V *>(reinterpret_cast<T *>(p) + index)->*indirection1))};// always reinterpret references as pointers
	}
}
template<auto indirection1, bool isindexed2, typename V>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_object_pointer_v<decltype(indirection1)> &&
	!isindexed2 &&
	!std::is_lvalue_reference_v<decltype(std::declval<V *>()->*indirection1)>,
	std::remove_reference_t<decltype(std::declval<V *>()->*indirection1)>> splitget(V *p)noexcept{
	// do not pass a nullptr here
	assert(p);
	return{p->*indirection1};
}
template<auto indirection1, bool isindexed2, typename V>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_object_pointer_v<decltype(indirection1)> &&
	!isindexed2 &&
	std::is_lvalue_reference_v<decltype(std::declval<V *>()->*indirection1)>,
	std::remove_reference_t<decltype(std::declval<V *>()->*indirection1)> *> splitget(V *p)noexcept{
	using T = std::remove_reference_t<decltype(std::declval<V *>()->*indirection1)>;
	// do not pass a nullptr here
	assert(p);
	return{reinterpret_cast<T *>(&(p->*indirection1))};// always reinterpret references as pointers
}
template<auto indirection1, bool isindexed2, typename V, typename W, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_function_pointer_v<decltype(indirection1)> &&
	isindexed2 &&
	!std::is_lvalue_reference_v<std::invoke_result_t<decltype(indirection1), V *, vararguments...>>,
	std::invoke_result_t<decltype(indirection1), V *, vararguments...>> splitget(V *p, W index2, vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(indirection1), V *, vararguments...>){
	static_cast<void>(index2);
	// do not pass a nullptr here
	assert(p);
	return{(p->*indirection1)(varparameters...)};
}
template<auto indirection1, bool isindexed2, typename V, typename W, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_function_pointer_v<decltype(indirection1)> &&
	isindexed2 &&
	std::is_lvalue_reference_v<std::invoke_result_t<decltype(indirection1), V *, vararguments...>>,
	std::remove_reference_t<std::invoke_result_t<decltype(indirection1), V *, vararguments...>> *> splitget(V *p, W index2, vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(indirection1), V *, vararguments...>){
	static_cast<void>(index2);
	using T = std::remove_reference_t<std::invoke_result_t<decltype(indirection1), V *, vararguments...>>;
	// do not pass a nullptr here
	assert(p);
	return{reinterpret_cast<T *>(&(p->*indirection1)(varparameters...))};// always reinterpret references as pointers
}
template<auto indirection1, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_function_pointer_v<decltype(indirection1)> &&
	!isindexed2 &&
	!std::is_lvalue_reference_v<std::invoke_result_t<decltype(indirection1), V *, vararguments...>>,
	std::invoke_result_t<decltype(indirection1), V *, vararguments...>> splitget(V *p, vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(indirection1), V *, vararguments...>){
	// do not pass a nullptr here
	assert(p);
	return{(p->*indirection1)(varparameters...)};
}
template<auto indirection1, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_function_pointer_v<decltype(indirection1)> &&
	!isindexed2 &&
	std::is_lvalue_reference_v<std::invoke_result_t<decltype(indirection1), V *, vararguments...>>,
	std::remove_reference_t<std::invoke_result_t<decltype(indirection1), V *, vararguments...>>> splitget(V *p, vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(indirection1), V *, vararguments...>){
	using T = std::remove_reference_t<std::invoke_result_t<decltype(indirection1), V *, vararguments...>>;
	// do not pass a nullptr here
	assert(p);
	return{reinterpret_cast<T *>(&(p->*indirection1)(varparameters...))};// always reinterpret references as pointers
}

// Utility templates to split off the first parameter
template<typename W, typename... vararguments>
RSBD8_FUNC_INLINE W splitparameter(W first, vararguments...)noexcept{
	return{first};
}
RSBD8_FUNC_INLINE void splitparameter()noexcept{
	// This function is a dummy, but it does allow the version without any extra arguments to exist.
}

// Utility template to retrieve the first-level source for full outputs
template<auto indirection1, bool isindexed2, typename T, typename V, typename... vararguments>
RSBD8_FUNC_INLINE auto indirectinput1(V *p, vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	using U = std::invoke_result_t<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>;// splitget will convert references to pointers
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,// the default for all platforms
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,// only to support the 80-bit long double type with padding (always little endian)
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;// only to support the 80-bit long double type (always little endian)
	// do not pass a nullptr here
	assert(p);
	if constexpr(std::is_member_object_pointer_v<decltype(indirection1)>){
		if constexpr(!std::is_pointer_v<U>){// indirection directly to member, ignore isindexed2
			static_assert(sizeof(T) == sizeof(U), "misinterpreted indirection input type");
			if constexpr(1 == sizeof...(varparameters)){// indirection to member with an index
				if constexpr(64 < CHAR_BIT * sizeof(T)){
					std::byte const *pfinal{reinterpret_cast<std::byte const *>(p) + sizeof(T) * splitparameter(varparameters...)};
					return std::pair<uint_least64_t, W>{
						reinterpret_cast<V const *>(pfinal)->*reinterpret_cast<uint_least64_t V:: *>(indirection1),
						reinterpret_cast<V const *>(p + sizeof(uint_least64_t))->*reinterpret_cast<W V:: *>(indirection1)
					};
				}else{
					return reinterpret_cast<V const *>(reinterpret_cast<T const *>(p) + splitparameter(varparameters...))->*reinterpret_cast<T V:: *>(indirection1);
				}
			}else if constexpr(0 == sizeof...(varparameters)){// indirection to member without an index
				if constexpr(64 < CHAR_BIT * sizeof(T)){
					return std::pair<uint_least64_t, W>{
						p->*reinterpret_cast<uint_least64_t V:: *>(indirection1),
						reinterpret_cast<V const *>(reinterpret_cast<std::byte const *>(p) + sizeof(uint_least64_t))->*reinterpret_cast<W V:: *>(indirection1)
					};
				}else{
					return p->*reinterpret_cast<T V:: *>(indirection1);
				}
			}else static_assert(false, "impossible first-level indirection indexing parameter count");
		}else{
			static_assert(!std::is_pointer_v<std::remove_pointer_t<U>> && !std::is_reference_v<std::remove_pointer_t<U>>, "third level indirection is not supported");
			static_assert(sizeof(T) == sizeof(std::remove_pointer_t<U>), "misinterpreted indirection input type");
			return reinterpret_cast<std::byte const *>(splitget<indirection1, isindexed2, V>(p, varparameters...));
		}
	}else if constexpr(std::is_member_function_pointer_v<decltype(indirection1)>){
		if constexpr(!std::is_pointer_v<U>){// indirection directly to item, ignore isindexed2
			static_assert(sizeof(T) == sizeof(U), "misinterpreted indirection input type");
			U val{(p->*indirection1)(varparameters...)};
			if constexpr(64 < CHAR_BIT * sizeof(T)){
				return std::pair<uint_least64_t, W>{
					*reinterpret_cast<uint_least64_t *>(&val),
					reinterpret_cast<W *>(&val)[sizeof(uint_least64_t) / sizeof(W)]
				};
			}else{
#ifdef __cpp_lib_bit_cast
				return std::bit_cast<T>(val);
#else
				return *reinterpret_cast<T *>(&val);
#endif
			}
		}else{// indirection to second level pointer
			static_assert(!std::is_pointer_v<std::remove_pointer_t<U>> && !std::is_reference_v<std::remove_pointer_t<U>>, "third level indirection is not supported");
			static_assert(sizeof(T) == sizeof(std::remove_pointer_t<U>), "misinterpreted indirection input type");
			return reinterpret_cast<std::byte const *>(splitget<indirection1, isindexed2, V>(p, varparameters...));
		}
	}else static_assert(false, "unsupported indirection input type");
}

// Utility templates to either retrieve the second-level source or pass though results for full outputs
template<auto indirection1, ptrdiff_t indirection2, bool isindexed2, typename T, typename... vararguments>
RSBD8_FUNC_INLINE auto indirectinput2(std::byte const *pintermediate, vararguments... varparameters)noexcept{
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,// the default for all platforms
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,// only to support the 80-bit long double type with padding (always little endian)
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;// only to support the 80-bit long double type (always little endian)
	// do not pass a nullptr here
	assert(pintermediate);
	if constexpr(std::is_member_object_pointer_v<decltype(indirection1)>){
		if constexpr(0 == sizeof...(varparameters)){// indirection to member with no indices, ignore isindexed2
			if constexpr(64 < CHAR_BIT * sizeof(T)){
				return std::pair<uint_least64_t, W>{
					*reinterpret_cast<uint_least64_t const *>(pintermediate + indirection2),
					*reinterpret_cast<W const *>(pintermediate + indirection2 + sizeof(uint_least64_t))
				};
			}else{
				return *reinterpret_cast<T const *>(pintermediate + indirection2);
			}
		}else if constexpr(1 == sizeof...(varparameters)){// indirection to member with an index
			if constexpr(isindexed2){// second level extra index
				if constexpr(64 < CHAR_BIT * sizeof(T)){
					std::byte const *pfinal{pintermediate + indirection2 + sizeof(T) * splitparameter(varparameters...)};
					return std::pair<uint_least64_t, W>{
						*reinterpret_cast<uint_least64_t const *>(pfinal),
						*reinterpret_cast<W const *>(pfinal + sizeof(uint_least64_t))
					};
				}else{
					return reinterpret_cast<T const *>(pintermediate + indirection2)[splitparameter(varparameters...)];
				}
			}else{// first level extra index
				if constexpr(64 < CHAR_BIT * sizeof(T)){
					std::byte const *pfinal{pintermediate + indirection2};
					return std::pair<uint_least64_t, W>{
						*reinterpret_cast<uint_least64_t const *>(pfinal),
						*reinterpret_cast<W const *>(pfinal + sizeof(uint_least64_t))
					};
				}else{
					return *reinterpret_cast<T const *>(pintermediate + indirection2);
				}
			}
		}else if constexpr(2 == sizeof...(varparameters)){// indirection to member with two indices, ignore isindexed2
			std::pair indices{varparameters...};
			if constexpr(64 < CHAR_BIT * sizeof(T)){
				std::byte const *pfinal{pintermediate + indirection2 + sizeof(T) * indices.second};
				return std::pair<uint_least64_t, W>{
					*reinterpret_cast<uint_least64_t const *>(pfinal),
					*reinterpret_cast<W const *>(pfinal + sizeof(uint_least64_t))
				};
			}else{
				return reinterpret_cast<T const *>(pintermediate + indirection2)[indices.second];
			}
		}else static_assert(false, "impossible second-level indirection indexing parameter count");
	}else if constexpr(std::is_member_function_pointer_v<decltype(indirection1)>){
		if constexpr(isindexed2){// second level extra index
			if constexpr(64 < CHAR_BIT * sizeof(T)){
				std::byte const *pfinal{pintermediate + indirection2 + sizeof(T) * splitparameter(varparameters...)};
				return std::pair<uint_least64_t, W>{
					*reinterpret_cast<uint_least64_t const *>(pfinal),
					*reinterpret_cast<W const *>(pfinal + sizeof(uint_least64_t))
				};
			}else{
				return reinterpret_cast<T const *>(pintermediate + indirection2)[splitparameter(varparameters...)];
			}
		}else{// second level without an index
			if constexpr(64 < CHAR_BIT * sizeof(T)){
				std::byte const *pfinal{pintermediate + indirection2};
				return std::pair<uint_least64_t, W>{
					*reinterpret_cast<uint_least64_t const *>(pfinal),
					*reinterpret_cast<W const *>(pfinal + sizeof(uint_least64_t))
				};
			}else{
				return *reinterpret_cast<T const *>(pintermediate + indirection2);
			}
		}
	}else static_assert(false, "unsupported indirection input type");
}
template<auto indirection1, ptrdiff_t indirection2, bool isindexed2, typename T, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<!std::is_pointer_v<T>,
	T> indirectinput2(T passthrough, vararguments...)noexcept{
	return{passthrough};
}

// Utility templates to get either the member object type or the member function return type
template<auto indirection1, bool isindexed2, typename V, typename dummy = void, typename... vararguments> struct memberpointerdeducebody;
// partial specialisation, by std::is_member_function_pointer_v
template<auto indirection1, bool isindexed2, typename V, typename... vararguments>
struct memberpointerdeducebody<indirection1, isindexed2, V, std::enable_if_t<std::is_member_function_pointer_v<decltype(indirection1)>>, vararguments...>{
	using type = std::invoke_result_t<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>;
};
// partial specialisation, by std::is_member_object_pointer_v
template<auto indirection1, bool isindexed2, typename V, typename... vararguments>
struct memberpointerdeducebody<indirection1, isindexed2, V, std::enable_if_t<std::is_member_object_pointer_v<decltype(indirection1)>>, vararguments...>{
	using type = std::remove_reference_t<decltype(std::declval<V>().*indirection1)>;
};
template<auto indirection1, bool isindexed2, typename V, typename... vararguments>
using memberpointerdeduce = typename memberpointerdeducebody<indirection1, isindexed2, V, void, vararguments...>::type;

// Utility template to either pass through a type or allow std::underlying_type to do its work
template<typename T>
using stripenum = typename std::conditional_t<std::is_enum_v<T>, std::underlying_type<T>, std::enable_if<true, T>>::type;

// Utility template to pick an unsigned type of the lowest rank with the same size or allow std::make_unsigned to do its work
// This does not require using stripenum first to work.
template<typename T>
using tounifunsigned = std::conditional_t<std::is_class_v<T> || std::is_union_v<T>, T,// pass these through
	std::conditional_t<1 == sizeof(T), unsigned char,
	std::conditional_t<sizeof(short) == sizeof(T), unsigned short,
	std::conditional_t<sizeof(signed) == sizeof(T), unsigned,
	std::conditional_t<sizeof(long) == sizeof(T), unsigned long,
	std::conditional_t<sizeof(long long) == sizeof(T), unsigned long long,
	typename std::conditional_t<std::is_integral_v<T> || std::is_enum_v<T>, std::make_unsigned<T>, std::enable_if<true, void>>::type>>>>>>;

// Utility template to use add-with-carry operations if possible for the boolean operator less than
RSBD8_FUNC_INLINE void addcarryofless(unsigned &accumulator, size_t minuend, size_t subtrahend)noexcept{
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_subcl) && __has_builtin(__builtin_addc)
	unsigned long carry;
	__builtin_subcl(minuend, subtrahend, 0, &carry);
	unsigned checkcarry;
	accumulator = __builtin_addc(accumulator, 0, static_cast<unsigned>(carry), &checkcarry);
	static_cast<void>(checkcarry);
	assert(!checkcarry);// the chosen accumulator should be big enough to never wrap-around
#elif defined(_M_X64)
	unsigned char checkcarry{_addcarry_u32(_subborrow_u64(0, minuend, subtrahend, nullptr), accumulator, 0, &accumulator)};// cmp r, r followed by adc r, 0
	static_cast<void>(checkcarry);
	assert(!checkcarry);// the chosen accumulator should be big enough to never wrap-around
#elif defined(_M_IX86)
	unsigned char checkcarry{_addcarry_u32(_subborrow_u32(0, minuend, subtrahend, nullptr), accumulator, 0, &accumulator)};// cmp r, r followed by adc r, 0
	static_cast<void>(checkcarry);
	assert(!checkcarry);// the chosen accumulator should be big enough to never wrap-around
#else
	accumulator += minuend < subtrahend;
#endif
}

// Utility template to use add-with-carry operations if possible for the boolean operator less than or equal
RSBD8_FUNC_INLINE void addcarryoflessorequal(unsigned &accumulator, size_t minuend, size_t subtrahend)noexcept{
	// The specialised versions actually calculate greater-than-or-equal, but with everything reversed.
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_subcl) && __has_builtin(__builtin_subc)
	unsigned long carry;
	__builtin_subcl(subtrahend, minuend, 0, &carry);
	unsigned checkcarry;
	accumulator = __builtin_subc(accumulator, ~0u, static_cast<unsigned>(carry), &checkcarry);
	static_cast<void>(checkcarry);
	assert(checkcarry);// the chosen accumulator should be big enough to never wrap-around
#elif defined(_M_X64)
	unsigned char checkcarry{_subborrow_u32(_subborrow_u64(0, subtrahend, minuend, nullptr), accumulator, ~0u, &accumulator)};// cmp r, r followed by sbb r, -1
	static_cast<void>(checkcarry);
	assert(checkcarry);// the chosen accumulator should be big enough to never wrap-around
#elif defined(_M_IX86)
	unsigned char checkcarry{_subborrow_u32(_subborrow_u32(0, subtrahend, minuend, nullptr), accumulator, ~0u, &accumulator)};// cmp r, r followed by sbb r, -1
	static_cast<void>(checkcarry);
	assert(checkcarry);// the chosen accumulator should be big enough to never wrap-around
#else
	accumulator += minuend <= subtrahend;
#endif
}

// Utility template of bit scan forward
template<typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_integral_v<typename std::conditional_t<std::is_enum_v<T>,
		std::underlying_type<T>,
		std::enable_if<true, T>>::type>,
	unsigned> bitscanforwardportable(T input)noexcept{
	assert(input);// design decision: do not allow 0 as input as neither x86/x64 bsf nor using the de Bruijn sequence supports it
#if defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))
	if constexpr(32 >= CHAR_BIT * sizeof(T)) return{static_cast<unsigned>(__builtin_ctz(input))};
	else return{static_cast<unsigned>(__builtin_ctzll(input))};
#elif defined(_M_X64)
	// will run bsf (bit scan forward) on older architectures, which is fine
	if constexpr(32 >= CHAR_BIT * sizeof(T)) return{_tzcnt_u32(input)};
	else return{static_cast<unsigned>(_tzcnt_u64(input))};
#elif defined(_M_IX86)
	// will run bsf (bit scan forward) on older architectures, which is fine
	if constexpr(32 >= CHAR_BIT * sizeof(T)) return{_tzcnt_u32(input)};
	else{// The 32-bit x86 platform hardly has any 64-bit integer support, so just split the halves.
		// This is still easy to handle in assembly (2 general options, and some instructions can be interleaved in between):
		// tzcnt r, r; add r, 32; tzcnt r, r; cmovc r, r
		// tzcnt r, r; add r, 32; bsf r, r; cmovz r, r
		DWORD hi{32 + _tzcnt_u32(static_cast<DWORD>(input >> 32))}, lo;// will run bsf (bit scan forward) on older architectures, which is fine
		return{_BitScanForward(&lo, static_cast<DWORD>(input & 0xFFFFFFFFu))? lo : hi};
	}
#elif defined(_M_ARM) || defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64) || defined(_M_ARM64EC)
	if constexpr(32 >= CHAR_BIT * sizeof(T)) return{_CountTrailingZeros(input)};
	else return{_CountTrailingZeros64(input)};
#elif defined(__cpp_lib_bitops)
	return{static_cast<unsigned>(std::countr_zero(input))};
#else// Count the consecutive zero bits (trailing) on the right with multiply and lookup.
	if constexpr(32 >= CHAR_BIT * sizeof(T)){
		static unsigned char constexpr MultiplyDeBruijnBitPosition[32]{
			0,
			1,
			28, 2,
			29, 14, 24, 3,
			30, 22, 20, 15, 25, 17, 4, 8,
			31, 27, 13, 23, 21, 19, 16, 7,
			26, 12, 18, 6,
			11, 5,
			10,
			9};
		return{MultiplyDeBruijnBitPosition[static_cast<std::make_unsigned_t<T>>(input & -static_cast<std::make_signed_t<T>>(input)) * 0x077CB531u >> 27]};
	}else{
		static unsigned char constexpr MultiplyDeBruijnBitPosition[64]{
			0,
			1,
			17, 2,
			18, 50, 3, 57,
			47, 19, 22, 51, 29, 4, 33, 58,
			15, 48, 20, 27, 25, 23, 52, 41, 54, 30, 38, 5, 43, 34, 59, 8,
			63, 16, 49, 56, 46, 21, 28, 32, 14, 26, 24, 40, 53, 37, 42, 7,
			62, 55, 45, 31, 13, 39, 36, 6,
			61, 44, 12, 35,
			60, 11,
			10,
			9};
		return{MultiplyDeBruijnBitPosition[static_cast<std::make_unsigned_t<T>>(input & -static_cast<std::make_signed_t<T>>(input)) * 0x37E84A99DAE458Fu >> 58]};
	}
#endif
}

// Utility template of rotate left by a compile-time constant amount
template<unsigned char amount, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_integral_v<typename std::conditional_t<std::is_enum_v<T>,
		std::underlying_type<T>,
		std::enable_if<true, T>>::type>,
	T> rotateleftportable(T input)noexcept{
#if defined(_M_IX86) || defined(_M_X64) || defined(_M_ARM) || defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64) || defined(_M_ARM64EC)
	if constexpr(64 == CHAR_BIT * sizeof(T)) return{static_cast<T>(_rotl64(static_cast<unsigned long long>(input), amount))};
	else if constexpr(32 == CHAR_BIT * sizeof(T)) return{static_cast<T>(_rotl(static_cast<unsigned>(input), amount))};
	else if constexpr(16 == CHAR_BIT * sizeof(T)) return{static_cast<T>(_rotl16(static_cast<unsigned short>(input), amount))};
	else if constexpr(8 == CHAR_BIT * sizeof(T)) return{static_cast<T>(_rotl8(static_cast<unsigned char>(input), amount))};
	else static_assert(false, "Implementing larger types will require additional work and optimisation for this library.");
#elif defined(__cpp_lib_bitops)
	return{std::rotl(input, amount)};
#else// revert to shifting and combining
	// Given that T might be smaller than type "int", prevent the undesired integral promotion when following C/C++ rules here.
	// By far most compilers can optimise this to a single instruction.
	T copy{input};
	copy <<= amount;
	input >>= CHAR_BIT * sizeof(T) - amount;
	input |= copy;
	return{input};
#endif
}

// Utility template of rotate right by a compile-time constant amount
template<unsigned char amount, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_integral_v<typename std::conditional_t<std::is_enum_v<T>,
		std::underlying_type<T>,
		std::enable_if<true, T>>::type>,
	T> rotaterightportable(T input)noexcept{
#if defined(_M_IX86) || defined(_M_X64) || defined(_M_ARM) || defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64) || defined(_M_ARM64EC)
	if constexpr(64 == CHAR_BIT * sizeof(T)) return{static_cast<T>(_rotr64(static_cast<unsigned long long>(input), amount))};
	else if constexpr(32 == CHAR_BIT * sizeof(T)) return{static_cast<T>(_rotr(static_cast<unsigned>(input), amount))};
	else if constexpr(16 == CHAR_BIT * sizeof(T)) return{static_cast<T>(_rotr16(static_cast<unsigned short>(input), amount))};
	else if constexpr(8 == CHAR_BIT * sizeof(T)) return{static_cast<T>(_rotr8(static_cast<unsigned char>(input), amount))};
	else static_assert(false, "Implementing larger types will require additional work and optimisation for this library.");
#elif defined(__cpp_lib_bitops)
	return{std::rotr(input, amount)};
#else// revert to shifting and combining
	// Given that T might be smaller than type "int", prevent the undesired integral promotion when following C/C++ rules here.
	// By far most compilers can optimise this to a single instruction.
	T copy{input};
	copy <<= CHAR_BIT * sizeof(T) - amount;
	input >>= amount;
	input |= copy;
	return{input};
#endif
}

// Helper functions to implement the 8 main modes
// The filtertop8() and filtershift8() template functions are customised for the sorting phase, and have no need for variants with pointers.
// These also only output size_t (or multiple) for direct use as indices.
// The filterinput() template functions modify their inputs and each has a variant that write their inputs to memory either once or twice.
// There are 5 of these, handling 1, 2, 3, 4 or 8 inputs.
// Each of these have two varians that take one or two pointers per input to store each input before modification.
// = modes with no filtering here:
// --- regular unsigned integer and also inside-out signed integer
// --- regular signed integer
// = modes with one-pass filtering here:
// --- absolute floating-point and also unsigned integer without using the top bit
// --- absolute floating-point, but negative inputs will sort just below their positive counterparts
// = modes with two-pass filtering here:
// --- regular floating point
// --- inside-out floating-point
// --- absolute signed integer
// --- absolute signed integer, but negative inputs will sort just below their positive counterparts

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	size_t> filtertop8(U cur)noexcept{
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curp{static_cast<std::make_signed_t<T>>(cur)};
		if constexpr(isfltpmode || !issignmode){
			T curo{static_cast<T>(cur)};
			curo += curo;
			cur = curo;
		}
		curp >>= CHAR_BIT * sizeof(T) - 1;
		U curq{static_cast<T>(curp)};
		if constexpr(isfltpmode){
			cur >>= 1;
			if constexpr(issignmode) cur += curq;
		}else if constexpr(issignmode){
			T curo{static_cast<T>(cur)};
			curo += static_cast<T>(curq);
			cur = curo;
		}
		cur ^= curq;
		if constexpr(8 < CHAR_BIT * sizeof(T)) cur >>= CHAR_BIT * sizeof(T) - 8;
		return{static_cast<size_t>(cur)};
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		if constexpr(8 < CHAR_BIT * sizeof(T)){
			cur >>= CHAR_BIT * sizeof(T) - 8 - !issignmode;
			return{static_cast<size_t>(cur & 0xFFu >> static_cast<unsigned char>(issignmode))};
		}else if constexpr(issignmode){
			return{static_cast<size_t>(cur & 0x7Fu)};
		}else{
			cur = rotateleftportable<1>(static_cast<T>(cur));
			return{static_cast<size_t>(cur)};
		}
	}else if constexpr(8 < CHAR_BIT * sizeof(T)){
		cur >>= CHAR_BIT * sizeof(T) - 8;
		return{static_cast<size_t>(cur)};
	}else return{static_cast<size_t>(cur)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	std::pair<size_t, size_t>> filtertop8(U cura, U curb)noexcept{
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		if constexpr(isfltpmode || !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		if constexpr(isfltpmode || !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		if constexpr(isfltpmode){
			cura >>= 1;
			curb >>= 1;
		}
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			cura = curoa;
			curb = curob;
		}
		cura ^= curqa;
		curb ^= curqb;
		if constexpr(8 < CHAR_BIT * sizeof(T)){
			cura >>= CHAR_BIT * sizeof(T) - 8;
			curb >>= CHAR_BIT * sizeof(T) - 8;
		}
		return{static_cast<size_t>(cura), static_cast<size_t>(curb)};
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		if constexpr(8 < CHAR_BIT * sizeof(T)){
			cura >>= CHAR_BIT * sizeof(T) - 8 - !issignmode;
			curb >>= CHAR_BIT * sizeof(T) - 8 - !issignmode;
			return{static_cast<size_t>(cura & 0xFFu >> static_cast<unsigned char>(issignmode)), static_cast<size_t>(curb & 0xFFu >> static_cast<unsigned char>(issignmode))};
		}else if constexpr(issignmode){
			return{static_cast<size_t>(cura & 0x7Fu), static_cast<size_t>(curb & 0x7Fu)};
		}else{
			cura = rotateleftportable<1>(static_cast<T>(cura));
			curb = rotateleftportable<1>(static_cast<T>(curb));
			return{static_cast<size_t>(cura), static_cast<size_t>(curb)};
		}
	}else if constexpr(8 < CHAR_BIT * sizeof(T)){
		cura >>= CHAR_BIT * sizeof(T) - 8;
		curb >>= CHAR_BIT * sizeof(T) - 8;
		return{static_cast<size_t>(cura), static_cast<size_t>(curb)};
	}else return{static_cast<size_t>(cura), static_cast<size_t>(curb)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	std::tuple<size_t, size_t, size_t, size_t>> filtertop8(U cura, U curb, U curc, U curd)noexcept{
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		if constexpr(isfltpmode || !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		if constexpr(isfltpmode || !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		std::make_signed_t<T> curpc{static_cast<std::make_signed_t<T>>(curc)};
		if constexpr(isfltpmode || !issignmode){
			T curoc{static_cast<T>(curc)};
			curoc += curoc;
			curc = curoc;
		}
		curpc >>= CHAR_BIT * sizeof(T) - 1;
		U curqc{static_cast<T>(curpc)};
		std::make_signed_t<T> curpd{static_cast<std::make_signed_t<T>>(curd)};
		if constexpr(isfltpmode || !issignmode){
			T curod{static_cast<T>(curd)};
			curod += curod;
			curd = curod;
		}
		curpd >>= CHAR_BIT * sizeof(T) - 1;
		U curqd{static_cast<T>(curpd)};
		if constexpr(isfltpmode){
			cura >>= 1;
			curb >>= 1;
			curc >>= 1;
			curd >>= 1;
		}
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			T curoc{static_cast<T>(curc)};
			T curod{static_cast<T>(curd)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			curoc += static_cast<T>(curqc);
			curod += static_cast<T>(curqd);
			cura = curoa;
			curb = curob;
			curc = curoc;
			curd = curod;
		}
		cura ^= curqa;
		curb ^= curqb;
		curc ^= curqc;
		curd ^= curqd;
		if constexpr(8 < CHAR_BIT * sizeof(T)){
			cura >>= CHAR_BIT * sizeof(T) - 8;
			curb >>= CHAR_BIT * sizeof(T) - 8;
			curc >>= CHAR_BIT * sizeof(T) - 8;
			curd >>= CHAR_BIT * sizeof(T) - 8;
		}
		return{static_cast<size_t>(cura), static_cast<size_t>(curb), static_cast<size_t>(curc), static_cast<size_t>(curd)};
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		if constexpr(8 < CHAR_BIT * sizeof(T)){
			cura >>= CHAR_BIT * sizeof(T) - 8 - !issignmode;
			curb >>= CHAR_BIT * sizeof(T) - 8 - !issignmode;
			curc >>= CHAR_BIT * sizeof(T) - 8 - !issignmode;
			curd >>= CHAR_BIT * sizeof(T) - 8 - !issignmode;
			return{static_cast<size_t>(cura & 0xFFu >> static_cast<unsigned char>(issignmode)), static_cast<size_t>(curb & 0xFFu >> static_cast<unsigned char>(issignmode)), static_cast<size_t>(curc & 0xFFu >> static_cast<unsigned char>(issignmode)), static_cast<size_t>(curd & 0xFFu >> static_cast<unsigned char>(issignmode))};
		}else if constexpr(issignmode){
			return{static_cast<size_t>(cura & 0x7Fu), static_cast<size_t>(curb & 0x7Fu), static_cast<size_t>(curc & 0x7Fu), static_cast<size_t>(curd & 0x7Fu)};
		}else{
			cura = rotateleftportable<1>(static_cast<T>(cura));
			curb = rotateleftportable<1>(static_cast<T>(curb));
			curc = rotateleftportable<1>(static_cast<T>(curc));
			curd = rotateleftportable<1>(static_cast<T>(curd));
			return{static_cast<size_t>(cura), static_cast<size_t>(curb), static_cast<size_t>(curc), static_cast<size_t>(curd)};
		}
	}else if constexpr(8 < CHAR_BIT * sizeof(T)){
		cura >>= CHAR_BIT * sizeof(T) - 8;
		curb >>= CHAR_BIT * sizeof(T) - 8;
		curc >>= CHAR_BIT * sizeof(T) - 8;
		curd >>= CHAR_BIT * sizeof(T) - 8;
		return{static_cast<size_t>(cura), static_cast<size_t>(curb), static_cast<size_t>(curc), static_cast<size_t>(curd)};
	}else return{static_cast<size_t>(cura), static_cast<size_t>(curb), static_cast<size_t>(curc), static_cast<size_t>(curd)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U),
	size_t> filtertop8(uint_least64_t curm, U cure)noexcept{
	// Filtering is simplified if possible.
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		int_least16_t curp{static_cast<int_least16_t>(cure)};
		if constexpr(isfltpmode || !issignmode){
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carry;
			__builtin_addcll(curm, curm, 0, &carry);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			__builtin_addcl(curm, curm, 0, &carry);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			uint_least32_t curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			__builtin_addcl(curmhi, curmhi, 0, &carry);
#endif
			unsigned short checkcarry;
			curo = __builtin_addcs(curo, curo, static_cast<unsigned short>(carry), &checkcarry);
			static_cast<void>(checkcarry);
#elif defined(_M_X64)
			unsigned char checkcarry{_addcarry_u16(_addcarry_u64(0, curm, curm, nullptr), curo, curo, &curo)};
			static_cast<void>(checkcarry);
#elif defined(_M_IX86)
			uint_least32_t curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			unsigned char checkcarry{_addcarry_u16(_addcarry_u32(0, curmhi, curmhi, nullptr), curo, curo, &curo)};
			static_cast<void>(checkcarry);
#else
			uint_least64_t curmtmp{curm};
			curm += curm;
			curo += curo;
			curo += curm < curmtmp;
#endif
			cure = curo;
		}
		curp >>= 16 - 1;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		uint_least64_t curq{static_cast<uint_least64_t>(curp)};// sign-extend
#else
		uint_least32_t curq{static_cast<uint_least32_t>(curp)};// sign-extend
#endif
		if constexpr(isfltpmode) cure >>= 1;
		if constexpr(issignmode){
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carry;
			__builtin_addcll(curm, curq, 0, &carry);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			__builtin_addcl(curm, curq, 0, &carry);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			uint_least32_t curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			__builtin_addcl(curmhi, curq, 0, &carry);
#endif
			unsigned short checkcarry;
			curo = __builtin_addcs(curo, static_cast<unsigned short>(curq), static_cast<unsigned short>(carry), &checkcarry);
			static_cast<void>(checkcarry);
#elif defined(_M_X64)
			unsigned char checkcarry{_addcarry_u16(_addcarry_u64(0, curm, curq, nullptr), curo, static_cast<uint_least16_t>(curq), &curo)};
			static_cast<void>(checkcarry);
#elif defined(_M_IX86)
			uint_least32_t curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			unsigned char checkcarry{_addcarry_u16(_addcarry_u32(0, curmhi, curq, nullptr), curo, static_cast<uint_least16_t>(curq), &curo)};
			static_cast<void>(checkcarry);
#else
			uint_least64_t curmtmp{curm};
			curm += curq;
			curo += static_cast<uint_least16_t>(curq);
			curo += curm < curmtmp || curm < curq;
#endif
			cure = curo;
		}
		cure ^= static_cast<U>(curq);
		return{static_cast<size_t>(cure >> 8 & 0xFFu)};
	}else if constexpr(isfltpmode && isabsvalue && !issignmode){// one-register filtering
		uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
		static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
		unsigned long long carry;
		__builtin_addcll(curm, curm, 0, &carry);
#else
		static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long carry;
		__builtin_addcl(curm, curm, 0, &carry);
#endif
#else
		static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long carry;
		uint_least32_t curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
		__builtin_addcl(curmhi, curmhi, 0, &carry);
#endif
		unsigned short checkcarry;
		curo = __builtin_addcs(curo, curo, static_cast<unsigned short>(carry), &checkcarry);
		static_cast<void>(checkcarry);
#elif defined(_M_X64)
		unsigned char checkcarry{_addcarry_u16(_addcarry_u64(0, curm, curm, nullptr), curo, curo, &curo)};
		static_cast<void>(checkcarry);
#elif defined(_M_IX86)
		uint_least32_t curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
		unsigned char checkcarry{_addcarry_u16(_addcarry_u32(0, curmhi, curmhi, nullptr), curo, curo, &curo)};
		static_cast<void>(checkcarry);
#else
		uint_least64_t curmtmp{curm};
		curm += curm;
		curo += curo;
		curo += curm < curmtmp;
#endif
		cure = curo;
		return{static_cast<size_t>(cure >> 8)};
	}else if constexpr(80 == CHAR_BIT * sizeof(T)){
		return{static_cast<size_t>(cure >> 8)};
	}else{
		// if unfiltered and cure isn't 16-bit, mask out the high bits
		return{static_cast<size_t>(cure >> 8 & 0xFFu)};
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U, typename W>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U) &&
	std::is_unsigned_v<W> &&
	64 >= CHAR_BIT * sizeof(W) &&
	8 < CHAR_BIT * sizeof(W),
	size_t> filtertop8(std::pair<uint_least64_t, W> cur)noexcept{
	// Use the function above.
	return{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(cur.first, cur.second)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U),
	std::pair<size_t, size_t>> filtertop8(uint_least64_t curma, U curea, uint_least64_t curmb, U cureb)noexcept{
	// Filtering is simplified if possible.
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		int_least16_t curpa{static_cast<int_least16_t>(curea)};
		int_least16_t curpb{static_cast<int_least16_t>(cureb)};
		if constexpr(isfltpmode || !issignmode){
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			__builtin_addcll(curma, curma, 0, &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			__builtin_addcl(curma, curma, 0, &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			__builtin_addcl(curmhia, curmhia, 0, &carrya);
#endif
			unsigned short checkcarrya;
			curoa = __builtin_addcs(curoa, curoa, static_cast<unsigned short>(carrya), &checkcarrya);
			static_cast<void>(checkcarrya);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			__builtin_addcll(curmb, curmb, 0, &carryb);
#else
			unsigned long carryb;
			__builtin_addcl(curmb, curmb, 0, &carryb);
#endif
#else
			unsigned long carryb;
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			__builtin_addcl(curmhib, curmhib, 0, &carryb);
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, curob, static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#elif defined(_M_X64)
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curma, nullptr), curoa, curoa, &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curmb, nullptr), curob, curob, &curob)};
			static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(0, curmhia, curmhia, nullptr), curoa, curoa, &curoa)};
			static_cast<void>(checkcarrya);
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(0, curmhib, curmhib, nullptr), curob, curob, &curob)};
			static_cast<void>(checkcarryb);
#else
			uint_least64_t curmtmpa{curma}, curmtmpb{curmb};
			curma += curma;
			curoa += curoa;
			curoa += curma < curmtmpa;
			curmb += curmb;
			curob += curob;
			curob += curmb < curmtmpb;
#endif
			curea = curoa;
			cureb = curob;
		}
		curpa >>= 16 - 1;
		curpb >>= 16 - 1;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		uint_least64_t curqa{static_cast<uint_least64_t>(curpa)};// sign-extend
		uint_least64_t curqb{static_cast<uint_least64_t>(curpb)};
#else
		uint_least32_t curqa{static_cast<uint_least32_t>(curpa)};// sign-extend
		uint_least32_t curqb{static_cast<uint_least32_t>(curpb)};
#endif
		if constexpr(isfltpmode){
			curea >>= 1;
			cureb >>= 1;
		}
		if constexpr(issignmode){
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			__builtin_addcll(curma, curqa, 0, &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			__builtin_addcl(curma, curqa, 0, &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			__builtin_addcl(curmhia, curqa, 0, &carrya);
#endif
			unsigned short checkcarrya;
			curoa = __builtin_addcs(curoa, static_cast<unsigned short>(curqa), static_cast<unsigned short>(carrya), &checkcarrya);
			static_cast<void>(checkcarrya);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			__builtin_addcll(curmb, curqb, 0, &carryb);
#else
			unsigned long carryb;
			__builtin_addcl(curmb, curqb, 0, &carryb);
#endif
#else
			unsigned long carryb;
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			__builtin_addcl(curmhib, curqb, 0, &carryb);
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, static_cast<unsigned short>(curqb), static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#elif defined(_M_X64)
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curqa, nullptr), curoa, static_cast<uint_least16_t>(curqa), &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curqb, nullptr), curob, static_cast<uint_least16_t>(curqb), &curob)};
			static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(0, curmhia, curqa, nullptr), curoa, static_cast<uint_least16_t>(curqa), &curoa)};
			static_cast<void>(checkcarrya);
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(0, curmhib, curqb, nullptr), curob, static_cast<uint_least16_t>(curqb), &curob)};
			static_cast<void>(checkcarryb);
#elif 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
			uint_least64_t curmtmpa{curma}, curmtmpb{curmb};
			curma += curqa;
			curoa += static_cast<uint_least16_t>(curqa);
			curoa += curma < curmtmpa || curma < curqa;
			curmb += curqb;
			curob += static_cast<uint_least16_t>(curqb);
			curob += curmb < curmtmpb || curmb < curqb;
#else
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			uint_least32_t curmlotmpa{curmloa}, curmhitmpa{curmhia};
			uint_least32_t curmlotmpb{curmlob}, curmhitmpb{curmhib};
			curmloa += curqa;
			curmhia += curqa;
			curmhia += curmloa < curmlotmpa || curmloa < curqa;
			curoa += static_cast<uint_least16_t>(curqa);
			curoa += curmhia < curmhitmpa || curmhia < curqa;
			curmlob += curqb;
			curmhib += curqb;
			curmhib += curmlob < curmlotmpb || curmlob < curqb;
			curob += static_cast<uint_least16_t>(curqb);
			curob += curmhib < curmhitmpb || curmhib < curqb;
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
			curea = curoa;
			cureb = curob;
		}
		curea ^= static_cast<U>(curqa);
		cureb ^= static_cast<U>(curqb);
		return{static_cast<size_t>(curea >> 8 & 0xFFu), static_cast<size_t>(cureb >> 8 & 0xFFu)};
	}else if constexpr(isfltpmode && isabsvalue && !issignmode){// one-register filtering
		uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
		uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
		static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
		unsigned long long carrya;
		__builtin_addcll(curma, curma, 0, &carrya);
#else
		static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long carrya;
		__builtin_addcl(curma, curma, 0, &carrya);
#endif
#else
		static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long carrya;
		uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		__builtin_addcl(curmhia, curmhia, 0, &carrya);
#endif
		unsigned short checkcarrya;
		curoa = __builtin_addcs(curoa, curoa, static_cast<unsigned short>(carrya), &checkcarrya);
		static_cast<void>(checkcarrya);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		unsigned long long carryb;
		__builtin_addcll(curmb, curmb, 0, &carryb);
#else
		unsigned long carryb;
		__builtin_addcl(curmb, curmb, 0, &carryb);
#endif
#else
		unsigned long carryb;
		uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		__builtin_addcl(curmhib, curmhib, 0, &carryb);
#endif
		unsigned short checkcarryb;
		curob = __builtin_addcs(curob, curob, static_cast<unsigned short>(carryb), &checkcarryb);
		static_cast<void>(checkcarryb);
#elif defined(_M_X64)
		unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curma, nullptr), curoa, curoa, &curoa)};
		static_cast<void>(checkcarrya);
		unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curmb, nullptr), curob, curob, &curob)};
		static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
		uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(0, curmhia, curmhia, nullptr), curoa, curoa, &curoa)};
		static_cast<void>(checkcarrya);
		uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(0, curmhib, curmhib, nullptr), curob, curob, &curob)};
		static_cast<void>(checkcarryb);
#else
		uint_least64_t curmtmpa{curma}, curmtmpb{curmb};
		curma += curma;
		curoa += curoa;
		curoa += curma < curmtmpa;
		curmb += curmb;
		curob += curob;
		curob += curmb < curmtmpb;
#endif
		curea = curoa;
		cureb = curob;
		return{static_cast<size_t>(curea >> 8), static_cast<size_t>(cureb >> 8)};
	}else if constexpr(80 == CHAR_BIT * sizeof(T)){
		return{static_cast<size_t>(curea >> 8), static_cast<size_t>(cureb >> 8)};
	}else{
		// if unfiltered and cure isn't 16-bit, mask out the high bits
		return{static_cast<size_t>(curea >> 8 & 0xFFu), static_cast<size_t>(cureb >> 8 & 0xFFu)};
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U, typename W>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U) &&
	std::is_unsigned_v<W> &&
	64 >= CHAR_BIT * sizeof(W) &&
	8 < CHAR_BIT * sizeof(W),
	std::pair<size_t, size_t>> filtertop8(std::pair<uint_least64_t, W> cura, std::pair<uint_least64_t, W> curb)noexcept{
	// Use the function above.
	return{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(cura.first, cura.second, curb.first, curb.second)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U),
	std::tuple<size_t, size_t, size_t, size_t>> filtertop8(uint_least64_t curma, U curea, uint_least64_t curmb, U cureb, uint_least64_t curmc, U curec, uint_least64_t curmd, U cured)noexcept{
	// Filtering is simplified if possible.
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		int_least16_t curpa{static_cast<int_least16_t>(curea)};
		int_least16_t curpb{static_cast<int_least16_t>(cureb)};
		int_least16_t curpc{static_cast<int_least16_t>(curec)};
		int_least16_t curpd{static_cast<int_least16_t>(cured)};
		if constexpr(isfltpmode || !issignmode){
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
			uint_least16_t curoc{static_cast<uint_least16_t>(curec)};
			uint_least16_t curod{static_cast<uint_least16_t>(cured)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			__builtin_addcll(curma, curma, 0, &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			__builtin_addcl(curma, curma, 0, &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			__builtin_addcl(curmhia, curmhia, 0, &carrya);
#endif
			unsigned short checkcarrya;
			curoa = __builtin_addcs(curoa, curoa, static_cast<unsigned short>(carrya), &checkcarrya);
			static_cast<void>(checkcarrya);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			__builtin_addcll(curmb, curmb, 0, &carryb);
#else
			unsigned long carryb;
			__builtin_addcl(curmb, curmb, 0, &carryb);
#endif
#else
			unsigned long carryb;
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			__builtin_addcl(curmhib, curmhib, 0, &carryb);
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, curob, static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryc;
			__builtin_addcll(curmc, curmc, 0, &carryc);
#else
			unsigned long carryc;
			__builtin_addcl(curmc, curmc, 0, &carryc);
#endif
#else
			unsigned long carryc;
			uint_least32_t curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
			__builtin_addcl(curmhic, curmhic, 0, &carryc);
#endif
			unsigned short checkcarryc;
			curoc = __builtin_addcs(curoc, curoc, static_cast<unsigned short>(carryc), &checkcarryc);
			static_cast<void>(checkcarryc);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryd;
			__builtin_addcll(curmd, curmd, 0, &carryd);
#else
			unsigned long carryd;
			__builtin_addcl(curmd, curmd, 0, &carryd);
#endif
#else
			unsigned long carryd;
			uint_least32_t curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
			__builtin_addcl(curmhid, curmhid, 0, &carryd);
#endif
			unsigned short checkcarryd;
			curod = __builtin_addcs(curod, curod, static_cast<unsigned short>(carryd), &checkcarryd);
			static_cast<void>(checkcarryd);
#elif defined(_M_X64)
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curma, nullptr), curoa, curoa, &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curmb, nullptr), curob, curob, &curob)};
			static_cast<void>(checkcarryb);
			unsigned char checkcarryc{_addcarry_u16(_addcarry_u64(0, curmc, curmc, nullptr), curoc, curoc, &curoc)};
			static_cast<void>(checkcarryc);
			unsigned char checkcarryd{_addcarry_u16(_addcarry_u64(0, curmd, curmd, nullptr), curod, curod, &curod)};
			static_cast<void>(checkcarryd);
#elif defined(_M_IX86)
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(0, curmhia, curmhia, nullptr), curoa, curoa, &curoa)};
			static_cast<void>(checkcarrya);
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(0, curmhib, curmhib, nullptr), curob, curob, &curob)};
			static_cast<void>(checkcarryb);
			uint_least32_t curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
			unsigned char checkcarryc{_addcarry_u16(_addcarry_u32(0, curmhic, curmhic, nullptr), curoc, curoc, &curoc)};
			static_cast<void>(checkcarryc);
			uint_least32_t curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
			unsigned char checkcarryd{_addcarry_u16(_addcarry_u32(0, curmhid, curmhid, nullptr), curod, curod, &curod)};
			static_cast<void>(checkcarryd);
#else
			uint_least64_t curmtmpa{curma}, curmtmpb{curmb}, curmtmpc{curmc}, curmtmpd{curmd};
			curma += curma;
			curoa += curoa;
			curoa += curma < curmtmpa;
			curmb += curmb;
			curob += curob;
			curob += curmb < curmtmpb;
			curmc += curmc;
			curoc += curoc;
			curoc += curmc < curmtmpc;
			curmd += curmd;
			curod += curod;
			curod += curmd < curmtmpd;
#endif
			curea = curoa;
			cureb = curob;
			curec = curoc;
			cured = curod;
		}
		curpa >>= 16 - 1;
		curpb >>= 16 - 1;
		curpc >>= 16 - 1;
		curpd >>= 16 - 1;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		uint_least64_t curqa{static_cast<uint_least64_t>(curpa)};// sign-extend
		uint_least64_t curqb{static_cast<uint_least64_t>(curpb)};
		uint_least64_t curqc{static_cast<uint_least64_t>(curpc)};
		uint_least64_t curqd{static_cast<uint_least64_t>(curpd)};
#else
		uint_least32_t curqa{static_cast<uint_least32_t>(curpa)};// sign-extend
		uint_least32_t curqb{static_cast<uint_least32_t>(curpb)};
		uint_least32_t curqc{static_cast<uint_least32_t>(curpc)};
		uint_least32_t curqd{static_cast<uint_least32_t>(curpd)};
#endif
		if constexpr(isfltpmode){
			curea >>= 1;
			cureb >>= 1;
			curec >>= 1;
			cured >>= 1;
		}
		if constexpr(issignmode){
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
			uint_least16_t curoc{static_cast<uint_least16_t>(curec)};
			uint_least16_t curod{static_cast<uint_least16_t>(cured)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			__builtin_addcll(curma, curqa, 0, &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			__builtin_addcl(curma, curqa, 0, &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			__builtin_addcl(curmhia, curqa, 0, &carrya);
#endif
			unsigned short checkcarrya;
			curoa = __builtin_addcs(curoa, static_cast<unsigned short>(curqa), static_cast<unsigned short>(carrya), &checkcarrya);
			static_cast<void>(checkcarrya);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			__builtin_addcll(curmb, curqb, 0, &carryb);
#else
			unsigned long carryb;
			__builtin_addcl(curmb, curqb, 0, &carryb);
#endif
#else
			unsigned long carryb;
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			__builtin_addcl(curmhib, curqb, 0, &carryb);
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, static_cast<unsigned short>(curqb), static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryc;
			__builtin_addcll(curmc, curqc, 0, &carryc);
#else
			unsigned long carryc;
			__builtin_addcl(curmc, curqc, 0, &carryc);
#endif
#else
			unsigned long carryc;
			uint_least32_t curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
			__builtin_addcl(curmhic, curqc, 0, &carryc);
#endif
			unsigned short checkcarryc;
			curoc = __builtin_addcs(curoc, static_cast<unsigned short>(curqc), static_cast<unsigned short>(carryc), &checkcarryc);
			static_cast<void>(checkcarryc);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryd;
			__builtin_addcll(curmd, curqd, 0, &carryd);
#else
			unsigned long carryd;
			__builtin_addcl(curmd, curqd, 0, &carryd);
#endif
#else
			unsigned long carryd;
			uint_least32_t curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
			__builtin_addcl(curmhid, curqd, 0, &carryd);
#endif
			unsigned short checkcarryd;
			curod = __builtin_addcs(curod, static_cast<unsigned short>(curqd), static_cast<unsigned short>(carryd), &checkcarryd);
			static_cast<void>(checkcarryd);
#elif defined(_M_X64)
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curqa, nullptr), curoa, static_cast<uint_least16_t>(curqa), &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curqb, nullptr), curob, static_cast<uint_least16_t>(curqb), &curob)};
			static_cast<void>(checkcarryb);
			unsigned char checkcarryc{_addcarry_u16(_addcarry_u64(0, curmc, curqc, nullptr), curoc, static_cast<uint_least16_t>(curqc), &curoc)};
			static_cast<void>(checkcarryc);
			unsigned char checkcarryd{_addcarry_u16(_addcarry_u64(0, curmd, curqd, nullptr), curod, static_cast<uint_least16_t>(curqd), &curod)};
			static_cast<void>(checkcarryd);
#elif defined(_M_IX86)
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(0, curmhia, curqa, nullptr), curoa, static_cast<uint_least16_t>(curqa), &curoa)};
			static_cast<void>(checkcarrya);
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(0, curmhib, curqb, nullptr), curob, static_cast<uint_least16_t>(curqb), &curob)};
			static_cast<void>(checkcarryb);
			uint_least32_t curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
			unsigned char checkcarryc{_addcarry_u16(_addcarry_u32(0, curmhic, curqc, nullptr), curoc, static_cast<uint_least16_t>(curqc), &curoc)};
			static_cast<void>(checkcarryc);
			uint_least32_t curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
			unsigned char checkcarryd{_addcarry_u16(_addcarry_u32(0, curmhid, curqd, nullptr), curod, static_cast<uint_least16_t>(curqd), &curod)};
			static_cast<void>(checkcarryd);
#elif 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
			uint_least64_t curmtmpa{curma}, curmtmpb{curmb}, curmtmpc{curmc}, curmtmpd{curmd};
			curma += curqa;
			curoa += static_cast<uint_least16_t>(curqa);
			curoa += curma < curmtmpa || curma < curqa;
			curmb += curqb;
			curob += static_cast<uint_least16_t>(curqb);
			curob += curmb < curmtmpb || curmb < curqb;
			curmc += curqc;
			curoc += static_cast<uint_least16_t>(curqc);
			curoc += curmc < curmtmpc || curmc < curqc;
			curmd += curqd;
			curod += static_cast<uint_least16_t>(curqd);
			curod += curmd < curmtmpd || curmd < curqd;
#else
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			uint_least32_t curmloc{static_cast<uint_least32_t>(curmc & 0xFFFFFFFFu)}, curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
			uint_least32_t curmlod{static_cast<uint_least32_t>(curmd & 0xFFFFFFFFu)}, curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
			uint_least32_t curmlotmpa{curmloa}, curmhitmpa{curmhia};
			uint_least32_t curmlotmpb{curmlob}, curmhitmpb{curmhib};
			uint_least32_t curmlotmpc{curmloc}, curmhitmpc{curmhic};
			uint_least32_t curmlotmpd{curmlod}, curmhitmpd{curmhid};
			curmloa += curqa;
			curmhia += curqa;
			curmhia += curmloa < curmlotmpa || curmloa < curqa;
			curoa += static_cast<uint_least16_t>(curqa);
			curoa += curmhia < curmhitmpa || curmhia < curqa;
			curmlob += curqb;
			curmhib += curqb;
			curmhib += curmlob < curmlotmpb || curmlob < curqb;
			curob += static_cast<uint_least16_t>(curqb);
			curob += curmhib < curmhitmpb || curmhib < curqb;
			curmloc += curqc;
			curmhic += curqc;
			curmhic += curmloc < curmlotmpc || curmloc < curqc;
			curoc += static_cast<uint_least16_t>(curqc);
			curoc += curmhic < curmhitmpc || curmhic < curqc;
			curmlod += curqd;
			curmhid += curqd;
			curmhid += curmlod < curmlotmpd || curmlod < curqd;
			curod += static_cast<uint_least16_t>(curqd);
			curod += curmhid < curmhitmpd || curmhid < curqd;
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
			alignas(8) uint_least32_t acurmc[2]{curmloc, curmhic};
			curmc = *reinterpret_cast<uint_least64_t *>(acurmc);// recompose
			alignas(8) uint_least32_t acurmd[2]{curmlod, curmhid};
			curmd = *reinterpret_cast<uint_least64_t *>(acurmd);// recompose
#endif
			curea = curoa;
			cureb = curob;
			curec = curoc;
			cured = curod;
		}
		curea ^= static_cast<U>(curqa);
		cureb ^= static_cast<U>(curqb);
		curec ^= static_cast<U>(curqc);
		cured ^= static_cast<U>(curqd);
		return{static_cast<size_t>(curea >> 8 & 0xFFu), static_cast<size_t>(cureb >> 8 & 0xFFu), static_cast<size_t>(curec >> 8 & 0xFFu), static_cast<size_t>(cured >> 8 & 0xFFu)};
	}else if constexpr(isfltpmode && isabsvalue && !issignmode){// one-register filtering
		uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
		uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
		uint_least16_t curoc{static_cast<uint_least16_t>(curec)};
		uint_least16_t curod{static_cast<uint_least16_t>(cured)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
		static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
		unsigned long long carrya;
		__builtin_addcll(curma, curma, 0, &carrya);
#else
		static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long carrya;
		__builtin_addcl(curma, curma, 0, &carrya);
#endif
#else
		static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long carrya;
		uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		__builtin_addcl(curmhia, curmhia, 0, &carrya);
#endif
		unsigned short checkcarrya;
		curoa = __builtin_addcs(curoa, curoa, static_cast<unsigned short>(carrya), &checkcarrya);
		static_cast<void>(checkcarrya);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		unsigned long long carryb;
		__builtin_addcll(curmb, curmb, 0, &carryb);
#else
		unsigned long carryb;
		__builtin_addcl(curmb, curmb, 0, &carryb);
#endif
#else
		unsigned long carryb;
		uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		__builtin_addcl(curmhib, curmhib, 0, &carryb);
#endif
		unsigned short checkcarryb;
		curob = __builtin_addcs(curob, curob, static_cast<unsigned short>(carryb), &checkcarryb);
		static_cast<void>(checkcarryb);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		unsigned long long carryc;
		__builtin_addcll(curmc, curmc, 0, &carryc);
#else
		unsigned long carryc;
		__builtin_addcl(curmc, curmc, 0, &carryc);
#endif
#else
		unsigned long carryc;
		uint_least32_t curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
		__builtin_addcl(curmhic, curmhic, 0, &carryc);
#endif
		unsigned short checkcarryc;
		curoc = __builtin_addcs(curoc, curoc, static_cast<unsigned short>(carryc), &checkcarryc);
		static_cast<void>(checkcarryc);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		unsigned long long carryd;
		__builtin_addcll(curmd, curmd, 0, &carryd);
#else
		unsigned long carryd;
		__builtin_addcl(curmd, curmd, 0, &carryd);
#endif
#else
		unsigned long carryd;
		uint_least32_t curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
		__builtin_addcl(curmhid, curmhid, 0, &carryd);
#endif
		unsigned short checkcarryd;
		curod = __builtin_addcs(curod, curod, static_cast<unsigned short>(carryd), &checkcarryd);
		static_cast<void>(checkcarryd);
#elif defined(_M_X64)
		unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curma, nullptr), curoa, curoa, &curoa)};
		static_cast<void>(checkcarrya);
		unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curmb, nullptr), curob, curob, &curob)};
		static_cast<void>(checkcarryb);
		unsigned char checkcarryc{_addcarry_u16(_addcarry_u64(0, curmc, curmc, nullptr), curoc, curoc, &curoc)};
		static_cast<void>(checkcarryc);
		unsigned char checkcarryd{_addcarry_u16(_addcarry_u64(0, curmd, curmd, nullptr), curod, curod, &curod)};
		static_cast<void>(checkcarryd);
#elif defined(_M_IX86)
		uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(0, curmhia, curmhia, nullptr), curoa, curoa, &curoa)};
		static_cast<void>(checkcarrya);
		uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(0, curmhib, curmhib, nullptr), curob, curob, &curob)};
		static_cast<void>(checkcarryb);
		uint_least32_t curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
		unsigned char checkcarryc{_addcarry_u16(_addcarry_u32(0, curmhic, curmhic, nullptr), curoc, curoc, &curoc)};
		static_cast<void>(checkcarryc);
		uint_least32_t curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
		unsigned char checkcarryd{_addcarry_u16(_addcarry_u32(0, curmhid, curmhid, nullptr), curod, curod, &curod)};
		static_cast<void>(checkcarryd);
#else
		uint_least64_t curmtmpa{curma}, curmtmpb{curmb}, curmtmpc{curmc}, curmtmpd{curmd};
		curma += curma;
		curoa += curoa;
		curoa += curma < curmtmpa;
		curmb += curmb;
		curob += curob;
		curob += curmb < curmtmpb;
		curmc += curmc;
		curoc += curoc;
		curoc += curmc < curmtmpc;
		curmd += curmd;
		curod += curod;
		curod += curmd < curmtmpd;
#endif
		curea = curoa;
		cureb = curob;
		curec = curoc;
		cured = curod;
		return{static_cast<size_t>(curea >> 8), static_cast<size_t>(cureb >> 8), static_cast<size_t>(curec >> 8), static_cast<size_t>(cured >> 8)};
	}else if constexpr(80 == CHAR_BIT * sizeof(T)){
		return{static_cast<size_t>(curea >> 8), static_cast<size_t>(cureb >> 8), static_cast<size_t>(curec >> 8), static_cast<size_t>(cured >> 8)};
	}else{
		// if unfiltered and cure isn't 16-bit, mask out the high bits
		return{static_cast<size_t>(curea >> 8 & 0xFFu), static_cast<size_t>(cureb >> 8 & 0xFFu), static_cast<size_t>(curec >> 8 & 0xFFu), static_cast<size_t>(cured >> 8 & 0xFFu)};
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U, typename W>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U) &&
	std::is_unsigned_v<W> &&
	64 >= CHAR_BIT * sizeof(W) &&
	8 < CHAR_BIT * sizeof(W),
	std::tuple<size_t, size_t, size_t, size_t>> filtertop8(std::pair<uint_least64_t, W> cura, std::pair<uint_least64_t, W> curb, std::pair<uint_least64_t, W> curc, std::pair<uint_least64_t, W> curd)noexcept{
	// Use the function above.
	return{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(cura.first, cura.second, curb.first, curb.second, curc.first, curc.second, curd.first, curd.second)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U),
	size_t> filterbelowtop8(uint_least64_t curm, U cure)noexcept{
	// Filtering is simplified if possible.
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		int_least16_t curp{static_cast<int_least16_t>(cure)};
		if constexpr(isabsvalue && !issignmode){
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carry;
			__builtin_addcll(curm, curm, 0, &carry);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			__builtin_addcl(curm, curm, 0, &carry);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			uint_least32_t curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			__builtin_addcl(curmhi, curmhi, 0, &carry);
#endif
			unsigned short checkcarry;
			curo = __builtin_addcs(curo, curo, static_cast<unsigned short>(carry), &checkcarry);
			static_cast<void>(checkcarry);
#elif defined(_M_X64)
			unsigned char checkcarry{_addcarry_u16(_addcarry_u64(0, curm, curm, nullptr), curo, curo, &curo)};
			static_cast<void>(checkcarry);
#elif defined(_M_IX86)
			uint_least32_t curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			unsigned char checkcarry{_addcarry_u16(_addcarry_u32(0, curmhi, curmhi, nullptr), curo, curo, &curo)};
			static_cast<void>(checkcarry);
#else
			uint_least64_t curmtmp{curm};
			curm += curm;
			curo += curo;
			curo += curm < curmtmp;
#endif
			cure = curo;
		}
		curp >>= 16 - 1;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		uint_least64_t curq{static_cast<uint_least64_t>(curp)};// sign-extend
#else
		uint_least32_t curq{static_cast<uint_least32_t>(curp)};// sign-extend
#endif
		if constexpr(issignmode){
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carry;
			__builtin_addcll(curm, curq, 0, &carry);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			__builtin_addcl(curm, curq, 0, &carry);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			uint_least32_t curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			__builtin_addcl(curmhi, curq, 0, &carry);
#endif
			unsigned short checkcarry;
			curo = __builtin_addcs(curo, static_cast<unsigned short>(curq), static_cast<unsigned short>(carry), &checkcarry);
			static_cast<void>(checkcarry);
#elif defined(_M_X64)
			unsigned char checkcarry{_addcarry_u16(_addcarry_u64(0, curm, curq, nullptr), curo, static_cast<uint_least16_t>(curq), &curo)};
			static_cast<void>(checkcarry);
#elif defined(_M_IX86)
			uint_least32_t curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			unsigned char checkcarry{_addcarry_u16(_addcarry_u32(0, curmhi, curq, nullptr), curo, static_cast<uint_least16_t>(curq), &curo)};
			static_cast<void>(checkcarry);
#else
			uint_least64_t curmtmp{curm};
			curm += curq;
			curo += static_cast<uint_least16_t>(curq);
			curo += curm < curmtmp || curm < curq;
#endif
			cure = curo;
		}
		cure ^= static_cast<U>(curq);
	}else if constexpr(isfltpmode && isabsvalue && !issignmode){// one-register filtering
		uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
		static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
		unsigned long long carry;
		__builtin_addcll(curm, curm, 0, &carry);
#else
		static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long carry;
		__builtin_addcl(curm, curm, 0, &carry);
#endif
#else
		static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long carry;
		uint_least32_t curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
		__builtin_addcl(curmhi, curmhi, 0, &carry);
#endif
		unsigned short checkcarry;
		curo = __builtin_addcs(curo, curo, static_cast<unsigned short>(carry), &checkcarry);
		static_cast<void>(checkcarry);
#elif defined(_M_X64)
		unsigned char checkcarry{_addcarry_u16(_addcarry_u64(0, curm, curm, nullptr), curo, curo, &curo)};
		static_cast<void>(checkcarry);
#elif defined(_M_IX86)
		uint_least32_t curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
		unsigned char checkcarry{_addcarry_u16(_addcarry_u32(0, curmhi, curmhi, nullptr), curo, curo, &curo)};
		static_cast<void>(checkcarry);
#else
		uint_least64_t curmtmp{curm};
		curm += curm;
		curo += curo;
		curo += curm < curmtmp;
#endif
		cure = curo;
	}
	return{static_cast<size_t>(cure & 0xFFu)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U, typename W>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U) &&
	std::is_unsigned_v<W> &&
	64 >= CHAR_BIT * sizeof(W) &&
	8 < CHAR_BIT * sizeof(W),
	size_t> filterbelowtop8(std::pair<uint_least64_t, W> cur)noexcept{
	// Use the function above.
	return{filterbelowtop8<isabsvalue, issignmode, isfltpmode, T, U>(cur.first, cur.second)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U),
	std::pair<size_t, size_t>> filterbelowtop8(uint_least64_t curma, U curea, uint_least64_t curmb, U cureb)noexcept{
	// Filtering is simplified if possible.
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		int_least16_t curpa{static_cast<int_least16_t>(curea)};
		int_least16_t curpb{static_cast<int_least16_t>(cureb)};
		if constexpr(isabsvalue && !issignmode){
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			__builtin_addcll(curma, curma, 0, &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			__builtin_addcl(curma, curma, 0, &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			__builtin_addcl(curmhia, curmhia, 0, &carrya);
#endif
			unsigned short checkcarrya;
			curoa = __builtin_addcs(curoa, curoa, static_cast<unsigned short>(carrya), &checkcarrya);
			static_cast<void>(checkcarrya);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			__builtin_addcll(curmb, curmb, 0, &carryb);
#else
			unsigned long carryb;
			__builtin_addcl(curmb, curmb, 0, &carryb);
#endif
#else
			unsigned long carryb;
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			__builtin_addcl(curmhib, curmhib, 0, &carryb);
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, curob, static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#elif defined(_M_X64)
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curma, nullptr), curoa, curoa, &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curmb, nullptr), curob, curob, &curob)};
			static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(0, curmhia, curmhia, nullptr), curoa, curoa, &curoa)};
			static_cast<void>(checkcarrya);
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(0, curmhib, curmhib, nullptr), curob, curob, &curob)};
			static_cast<void>(checkcarryb);
#else
			uint_least64_t curmtmpa{curma}, curmtmpb{curmb};
			curma += curma;
			curoa += curoa;
			curoa += curma < curmtmpa;
			curmb += curmb;
			curob += curob;
			curob += curmb < curmtmpb;
#endif
			curea = curoa;
			cureb = curob;
		}
		curpa >>= 16 - 1;
		curpb >>= 16 - 1;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		uint_least64_t curqa{static_cast<uint_least64_t>(curpa)};// sign-extend
		uint_least64_t curqb{static_cast<uint_least64_t>(curpb)};
#else
		uint_least32_t curqa{static_cast<uint_least32_t>(curpa)};// sign-extend
		uint_least32_t curqb{static_cast<uint_least32_t>(curpb)};
#endif
		if constexpr(issignmode){
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			__builtin_addcll(curma, curqa, 0, &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			__builtin_addcl(curma, curqa, 0, &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			__builtin_addcl(curmhia, curqa, 0, &carrya);
#endif
			unsigned short checkcarrya;
			curoa = __builtin_addcs(curoa, static_cast<unsigned short>(curqa), static_cast<unsigned short>(carrya), &checkcarrya);
			static_cast<void>(checkcarrya);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			__builtin_addcll(curmb, curqb, 0, &carryb);
#else
			unsigned long carryb;
			__builtin_addcl(curmb, curqb, 0, &carryb);
#endif
#else
			unsigned long carryb;
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			__builtin_addcl(curmhib, curqb, 0, &carryb);
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, static_cast<unsigned short>(curqb), static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#elif defined(_M_X64)
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curqa, nullptr), curoa, static_cast<uint_least16_t>(curqa), &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curqb, nullptr), curob, static_cast<uint_least16_t>(curqb), &curob)};
			static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(0, curmhia, curqa, nullptr), curoa, static_cast<uint_least16_t>(curqa), &curoa)};
			static_cast<void>(checkcarrya);
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(0, curmhib, curqb, nullptr), curob, static_cast<uint_least16_t>(curqb), &curob)};
			static_cast<void>(checkcarryb);
#elif 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
			uint_least64_t curmtmpa{curma}, curmtmpb{curmb};
			curma += curqa;
			curoa += static_cast<uint_least16_t>(curqa);
			curoa += curma < curmtmpa || curma < curqa;
			curmb += curqb;
			curob += static_cast<uint_least16_t>(curqb);
			curob += curmb < curmtmpb || curmb < curqb;
#else
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			uint_least32_t curmlotmpa{curmloa}, curmhitmpa{curmhia};
			uint_least32_t curmlotmpb{curmlob}, curmhitmpb{curmhib};
			curmloa += curqa;
			curmhia += curqa;
			curmhia += curmloa < curmlotmpa || curmloa < curqa;
			curoa += static_cast<uint_least16_t>(curqa);
			curoa += curmhia < curmhitmpa || curmhia < curqa;
			curmlob += curqb;
			curmhib += curqb;
			curmhib += curmlob < curmlotmpb || curmlob < curqb;
			curob += static_cast<uint_least16_t>(curqb);
			curob += curmhib < curmhitmpb || curmhib < curqb;
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
			curea = curoa;
			cureb = curob;
		}
		curea ^= static_cast<U>(curqa);
		cureb ^= static_cast<U>(curqb);
	}else if constexpr(isfltpmode && isabsvalue && !issignmode){// one-register filtering
		uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
		uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
		static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
		unsigned long long carrya;
		__builtin_addcll(curma, curma, 0, &carrya);
#else
		static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long carrya;
		__builtin_addcl(curma, curma, 0, &carrya);
#endif
#else
		static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long carrya;
		uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		__builtin_addcl(curmhia, curmhia, 0, &carrya);
#endif
		unsigned short checkcarrya;
		curoa = __builtin_addcs(curoa, curoa, static_cast<unsigned short>(carrya), &checkcarrya);
		static_cast<void>(checkcarrya);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		unsigned long long carryb;
		__builtin_addcll(curmb, curmb, 0, &carryb);
#else
		unsigned long carryb;
		__builtin_addcl(curmb, curmb, 0, &carryb);
#endif
#else
		unsigned long carryb;
		uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		__builtin_addcl(curmhib, curmhib, 0, &carryb);
#endif
		unsigned short checkcarryb;
		curob = __builtin_addcs(curob, curob, static_cast<unsigned short>(carryb), &checkcarryb);
		static_cast<void>(checkcarryb);
#elif defined(_M_X64)
		unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curma, nullptr), curoa, curoa, &curoa)};
		static_cast<void>(checkcarrya);
		unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curmb, nullptr), curob, curob, &curob)};
		static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
		uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(0, curmhia, curmhia, nullptr), curoa, curoa, &curoa)};
		static_cast<void>(checkcarrya);
		uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(0, curmhib, curmhib, nullptr), curob, curob, &curob)};
		static_cast<void>(checkcarryb);
#else
		uint_least64_t curmtmpa{curma}, curmtmpb{curmb};
		curma += curma;
		curoa += curoa;
		curoa += curma < curmtmpa;
		curmb += curmb;
		curob += curob;
		curob += curmb < curmtmpb;
#endif
		curea = curoa;
		cureb = curob;
	}
	return{static_cast<size_t>(curea & 0xFFu), static_cast<size_t>(cureb & 0xFFu)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U, typename W>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U) &&
	std::is_unsigned_v<W> &&
	64 >= CHAR_BIT * sizeof(W) &&
	8 < CHAR_BIT * sizeof(W),
	std::pair<size_t, size_t>> filterbelowtop8(std::pair<uint_least64_t, W> cura, std::pair<uint_least64_t, W> curb)noexcept{
	// Use the function above.
	return{filterbelowtop8<isabsvalue, issignmode, isfltpmode, T, U>(cura.first, cura.second, curb.first, curb.second)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U),
	std::tuple<size_t, size_t, size_t, size_t>> filterbelowtop8(uint_least64_t curma, U curea, uint_least64_t curmb, U cureb, uint_least64_t curmc, U curec, uint_least64_t curmd, U cured)noexcept{
	// Filtering is simplified if possible.
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		int_least16_t curpa{static_cast<int_least16_t>(curea)};
		int_least16_t curpb{static_cast<int_least16_t>(cureb)};
		int_least16_t curpc{static_cast<int_least16_t>(curec)};
		int_least16_t curpd{static_cast<int_least16_t>(cured)};
		if constexpr(isabsvalue && !issignmode){
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
			uint_least16_t curoc{static_cast<uint_least16_t>(curec)};
			uint_least16_t curod{static_cast<uint_least16_t>(cured)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			__builtin_addcll(curma, curma, 0, &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			__builtin_addcl(curma, curma, 0, &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			__builtin_addcl(curmhia, curmhia, 0, &carrya);
#endif
			unsigned short checkcarrya;
			curoa = __builtin_addcs(curoa, curoa, static_cast<unsigned short>(carrya), &checkcarrya);
			static_cast<void>(checkcarrya);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			__builtin_addcll(curmb, curmb, 0, &carryb);
#else
			unsigned long carryb;
			__builtin_addcl(curmb, curmb, 0, &carryb);
#endif
#else
			unsigned long carryb;
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			__builtin_addcl(curmhib, curmhib, 0, &carryb);
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, curob, static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryc;
			__builtin_addcll(curmc, curmc, 0, &carryc);
#else
			unsigned long carryc;
			__builtin_addcl(curmc, curmc, 0, &carryc);
#endif
#else
			unsigned long carryc;
			uint_least32_t curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
			__builtin_addcl(curmhic, curmhic, 0, &carryc);
#endif
			unsigned short checkcarryc;
			curoc = __builtin_addcs(curoc, curoc, static_cast<unsigned short>(carryc), &checkcarryc);
			static_cast<void>(checkcarryc);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryd;
			__builtin_addcll(curmd, curmd, 0, &carryd);
#else
			unsigned long carryd;
			__builtin_addcl(curmd, curmd, 0, &carryd);
#endif
#else
			unsigned long carryd;
			uint_least32_t curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
			__builtin_addcl(curmhid, curmhid, 0, &carryd);
#endif
			unsigned short checkcarryd;
			curod = __builtin_addcs(curod, curod, static_cast<unsigned short>(carryd), &checkcarryd);
			static_cast<void>(checkcarryd);
#elif defined(_M_X64)
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curma, nullptr), curoa, curoa, &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curmb, nullptr), curob, curob, &curob)};
			static_cast<void>(checkcarryb);
			unsigned char checkcarryc{_addcarry_u16(_addcarry_u64(0, curmc, curmc, nullptr), curoc, curoc, &curoc)};
			static_cast<void>(checkcarryc);
			unsigned char checkcarryd{_addcarry_u16(_addcarry_u64(0, curmd, curmd, nullptr), curod, curod, &curod)};
			static_cast<void>(checkcarryd);
#elif defined(_M_IX86)
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(0, curmhia, curmhia, nullptr), curoa, curoa, &curoa)};
			static_cast<void>(checkcarrya);
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(0, curmhib, curmhib, nullptr), curob, curob, &curob)};
			static_cast<void>(checkcarryb);
			uint_least32_t curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
			unsigned char checkcarryc{_addcarry_u16(_addcarry_u32(0, curmhic, curmhic, nullptr), curoc, curoc, &curoc)};
			static_cast<void>(checkcarryc);
			uint_least32_t curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
			unsigned char checkcarryd{_addcarry_u16(_addcarry_u32(0, curmhid, curmhid, nullptr), curod, curod, &curod)};
			static_cast<void>(checkcarryd);
#else
			uint_least64_t curmtmpa{curma}, curmtmpb{curmb}, curmtmpc{curmc}, curmtmpd{curmd};
			curma += curma;
			curoa += curoa;
			curoa += curma < curmtmpa;
			curmb += curmb;
			curob += curob;
			curob += curmb < curmtmpb;
			curmc += curmc;
			curoc += curoc;
			curoc += curmc < curmtmpc;
			curmd += curmd;
			curod += curod;
			curod += curmd < curmtmpd;
#endif
			curea = curoa;
			cureb = curob;
			curec = curoc;
			cured = curod;
		}
		curpa >>= 16 - 1;
		curpb >>= 16 - 1;
		curpc >>= 16 - 1;
		curpd >>= 16 - 1;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		uint_least64_t curqa{static_cast<uint_least64_t>(curpa)};// sign-extend
		uint_least64_t curqb{static_cast<uint_least64_t>(curpb)};
		uint_least64_t curqc{static_cast<uint_least64_t>(curpc)};
		uint_least64_t curqd{static_cast<uint_least64_t>(curpd)};
#else
		uint_least32_t curqa{static_cast<uint_least32_t>(curpa)};// sign-extend
		uint_least32_t curqb{static_cast<uint_least32_t>(curpb)};
		uint_least32_t curqc{static_cast<uint_least32_t>(curpc)};
		uint_least32_t curqd{static_cast<uint_least32_t>(curpd)};
#endif
		if constexpr(issignmode){
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
			uint_least16_t curoc{static_cast<uint_least16_t>(curec)};
			uint_least16_t curod{static_cast<uint_least16_t>(cured)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			__builtin_addcll(curma, curqa, 0, &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			__builtin_addcl(curma, curqa, 0, &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			__builtin_addcl(curmhia, curqa, 0, &carrya);
#endif
			unsigned short checkcarrya;
			curoa = __builtin_addcs(curoa, static_cast<unsigned short>(curqa), static_cast<unsigned short>(carrya), &checkcarrya);
			static_cast<void>(checkcarrya);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			__builtin_addcll(curmb, curqb, 0, &carryb);
#else
			unsigned long carryb;
			__builtin_addcl(curmb, curqb, 0, &carryb);
#endif
#else
			unsigned long carryb;
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			__builtin_addcl(curmhib, curqb, 0, &carryb);
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, static_cast<unsigned short>(curqb), static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryc;
			__builtin_addcll(curmc, curqc, 0, &carryc);
#else
			unsigned long carryc;
			__builtin_addcl(curmc, curqc, 0, &carryc);
#endif
#else
			unsigned long carryc;
			uint_least32_t curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
			__builtin_addcl(curmhic, curqc, 0, &carryc);
#endif
			unsigned short checkcarryc;
			curoc = __builtin_addcs(curoc, static_cast<unsigned short>(curqc), static_cast<unsigned short>(carryc), &checkcarryc);
			static_cast<void>(checkcarryc);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryd;
			__builtin_addcll(curmd, curqd, 0, &carryd);
#else
			unsigned long carryd;
			__builtin_addcl(curmd, curqd, 0, &carryd);
#endif
#else
			unsigned long carryd;
			uint_least32_t curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
			__builtin_addcl(curmhid, curqd, 0, &carryd);
#endif
			unsigned short checkcarryd;
			curod = __builtin_addcs(curod, static_cast<unsigned short>(curqd), static_cast<unsigned short>(carryd), &checkcarryd);
			static_cast<void>(checkcarryd);
#elif defined(_M_X64)
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curqa, nullptr), curoa, static_cast<uint_least16_t>(curqa), &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curqb, nullptr), curob, static_cast<uint_least16_t>(curqb), &curob)};
			static_cast<void>(checkcarryb);
			unsigned char checkcarryc{_addcarry_u16(_addcarry_u64(0, curmc, curqc, nullptr), curoc, static_cast<uint_least16_t>(curqc), &curoc)};
			static_cast<void>(checkcarryc);
			unsigned char checkcarryd{_addcarry_u16(_addcarry_u64(0, curmd, curqd, nullptr), curod, static_cast<uint_least16_t>(curqd), &curod)};
			static_cast<void>(checkcarryd);
#elif defined(_M_IX86)
			uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(0, curmhia, curqa, nullptr), curoa, static_cast<uint_least16_t>(curqa), &curoa)};
			static_cast<void>(checkcarrya);
			uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(0, curmhib, curqb, nullptr), curob, static_cast<uint_least16_t>(curqb), &curob)};
			static_cast<void>(checkcarryb);
			uint_least32_t curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
			unsigned char checkcarryc{_addcarry_u16(_addcarry_u32(0, curmhic, curqc, nullptr), curoc, static_cast<uint_least16_t>(curqc), &curoc)};
			static_cast<void>(checkcarryc);
			uint_least32_t curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
			unsigned char checkcarryd{_addcarry_u16(_addcarry_u32(0, curmhid, curqd, nullptr), curod, static_cast<uint_least16_t>(curqd), &curod)};
			static_cast<void>(checkcarryd);
#elif 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
			uint_least64_t curmtmpa{curma}, curmtmpb{curmb}, curmtmpc{curmc}, curmtmpd{curmd};
			curma += curqa;
			curoa += static_cast<uint_least16_t>(curqa);
			curoa += curma < curmtmpa || curma < curqa;
			curmb += curqb;
			curob += static_cast<uint_least16_t>(curqb);
			curob += curmb < curmtmpb || curmb < curqb;
			curmc += curqc;
			curoc += static_cast<uint_least16_t>(curqc);
			curoc += curmc < curmtmpc || curmc < curqc;
			curmd += curqd;
			curod += static_cast<uint_least16_t>(curqd);
			curod += curmd < curmtmpd || curmd < curqd;
#else
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			uint_least32_t curmloc{static_cast<uint_least32_t>(curmc & 0xFFFFFFFFu)}, curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
			uint_least32_t curmlod{static_cast<uint_least32_t>(curmd & 0xFFFFFFFFu)}, curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
			uint_least32_t curmlotmpa{curmloa}, curmhitmpa{curmhia};
			uint_least32_t curmlotmpb{curmlob}, curmhitmpb{curmhib};
			uint_least32_t curmlotmpc{curmloc}, curmhitmpc{curmhic};
			uint_least32_t curmlotmpd{curmlod}, curmhitmpd{curmhid};
			curmloa += curqa;
			curmhia += curqa;
			curmhia += curmloa < curmlotmpa || curmloa < curqa;
			curoa += static_cast<uint_least16_t>(curqa);
			curoa += curmhia < curmhitmpa || curmhia < curqa;
			curmlob += curqb;
			curmhib += curqb;
			curmhib += curmlob < curmlotmpb || curmlob < curqb;
			curob += static_cast<uint_least16_t>(curqb);
			curob += curmhib < curmhitmpb || curmhib < curqb;
			curmloc += curqc;
			curmhic += curqc;
			curmhic += curmloc < curmlotmpc || curmloc < curqc;
			curoc += static_cast<uint_least16_t>(curqc);
			curoc += curmhic < curmhitmpc || curmhic < curqc;
			curmlod += curqd;
			curmhid += curqd;
			curmhid += curmlod < curmlotmpd || curmlod < curqd;
			curod += static_cast<uint_least16_t>(curqd);
			curod += curmhid < curmhitmpd || curmhid < curqd;
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
			alignas(8) uint_least32_t acurmc[2]{curmloc, curmhic};
			curmc = *reinterpret_cast<uint_least64_t *>(acurmc);// recompose
			alignas(8) uint_least32_t acurmd[2]{curmlod, curmhid};
			curmd = *reinterpret_cast<uint_least64_t *>(acurmd);// recompose
#endif
			curea = curoa;
			cureb = curob;
			curec = curoc;
			cured = curod;
		}
		curea ^= static_cast<U>(curqa);
		cureb ^= static_cast<U>(curqb);
		curec ^= static_cast<U>(curqc);
		cured ^= static_cast<U>(curqd);
	}else if constexpr(isfltpmode && isabsvalue && !issignmode){// one-register filtering
		uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
		uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
		uint_least16_t curoc{static_cast<uint_least16_t>(curec)};
		uint_least16_t curod{static_cast<uint_least16_t>(cured)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
		static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
		unsigned long long carrya;
		__builtin_addcll(curma, curma, 0, &carrya);
#else
		static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long carrya;
		__builtin_addcl(curma, curma, 0, &carrya);
#endif
#else
		static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long carrya;
		uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		__builtin_addcl(curmhia, curmhia, 0, &carrya);
#endif
		unsigned short checkcarrya;
		curoa = __builtin_addcs(curoa, curoa, static_cast<unsigned short>(carrya), &checkcarrya);
		static_cast<void>(checkcarrya);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		unsigned long long carryb;
		__builtin_addcll(curmb, curmb, 0, &carryb);
#else
		unsigned long carryb;
		__builtin_addcl(curmb, curmb, 0, &carryb);
#endif
#else
		unsigned long carryb;
		uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		__builtin_addcl(curmhib, curmhib, 0, &carryb);
#endif
		unsigned short checkcarryb;
		curob = __builtin_addcs(curob, curob, static_cast<unsigned short>(carryb), &checkcarryb);
		static_cast<void>(checkcarryb);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		unsigned long long carryc;
		__builtin_addcll(curmc, curmc, 0, &carryc);
#else
		unsigned long carryc;
		__builtin_addcl(curmc, curmc, 0, &carryc);
#endif
#else
		unsigned long carryc;
		uint_least32_t curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
		__builtin_addcl(curmhic, curmhic, 0, &carryc);
#endif
		unsigned short checkcarryc;
		curoc = __builtin_addcs(curoc, curoc, static_cast<unsigned short>(carryc), &checkcarryc);
		static_cast<void>(checkcarryc);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		unsigned long long carryd;
		__builtin_addcll(curmd, curmd, 0, &carryd);
#else
		unsigned long carryd;
		__builtin_addcl(curmd, curmd, 0, &carryd);
#endif
#else
		unsigned long carryd;
		uint_least32_t curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
		__builtin_addcl(curmhid, curmhid, 0, &carryd);
#endif
		unsigned short checkcarryd;
		curod = __builtin_addcs(curod, curod, static_cast<unsigned short>(carryd), &checkcarryd);
		static_cast<void>(checkcarryd);
#elif defined(_M_X64)
		unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curma, nullptr), curoa, curoa, &curoa)};
		static_cast<void>(checkcarrya);
		unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curmb, nullptr), curob, curob, &curob)};
		static_cast<void>(checkcarryb);
		unsigned char checkcarryc{_addcarry_u16(_addcarry_u64(0, curmc, curmc, nullptr), curoc, curoc, &curoc)};
		static_cast<void>(checkcarryc);
		unsigned char checkcarryd{_addcarry_u16(_addcarry_u64(0, curmd, curmd, nullptr), curod, curod, &curod)};
		static_cast<void>(checkcarryd);
#elif defined(_M_IX86)
		uint_least32_t curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(0, curmhia, curmhia, nullptr), curoa, curoa, &curoa)};
		static_cast<void>(checkcarrya);
		uint_least32_t curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(0, curmhib, curmhib, nullptr), curob, curob, &curob)};
		static_cast<void>(checkcarryb);
		uint_least32_t curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
		unsigned char checkcarryc{_addcarry_u16(_addcarry_u32(0, curmhic, curmhic, nullptr), curoc, curoc, &curoc)};
		static_cast<void>(checkcarryc);
		uint_least32_t curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
		unsigned char checkcarryd{_addcarry_u16(_addcarry_u32(0, curmhid, curmhid, nullptr), curod, curod, &curod)};
		static_cast<void>(checkcarryd);
#else
		uint_least64_t curmtmpa{curma}, curmtmpb{curmb}, curmtmpc{curmc}, curmtmpd{curmd};
		curma += curma;
		curoa += curoa;
		curoa += curma < curmtmpa;
		curmb += curmb;
		curob += curob;
		curob += curmb < curmtmpb;
		curmc += curmc;
		curoc += curoc;
		curoc += curmc < curmtmpc;
		curmd += curmd;
		curod += curod;
		curod += curmd < curmtmpd;
#endif
		curea = curoa;
		cureb = curob;
		curec = curoc;
		cured = curod;
	}
	return{static_cast<size_t>(curea & 0xFFu), static_cast<size_t>(cureb & 0xFFu), static_cast<size_t>(curec & 0xFFu), static_cast<size_t>(cured & 0xFFu)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U, typename W>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U) &&
	std::is_unsigned_v<W> &&
	64 >= CHAR_BIT * sizeof(W) &&
	8 < CHAR_BIT * sizeof(W),
	std::tuple<size_t, size_t, size_t, size_t>> filterbelowtop8(std::pair<uint_least64_t, W> cura, std::pair<uint_least64_t, W> curb, std::pair<uint_least64_t, W> curc, std::pair<uint_least64_t, W> curd)noexcept{
	// Use the function above.
	return{filterbelowtop8<isabsvalue, issignmode, isfltpmode, T, U>(cura.first, cura.second, curb.first, curb.second, curc.first, curc.second, curd.first, curd.second)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U),
	size_t> filtershift8(U cur, unsigned shift)noexcept{
	// Filtering is simplified if possible.
	// This should never filter the top part for non-absolute floating-point inputs.
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curp{static_cast<std::make_signed_t<T>>(cur)};
		if constexpr(isabsvalue && !issignmode){
			T curo{static_cast<T>(cur)};
			curo += curo;
			cur = curo;
		}
		curp >>= CHAR_BIT * sizeof(T) - 1;
		U curq{static_cast<T>(curp)};
		if constexpr(issignmode){
			T curo{static_cast<T>(cur)};
			curo += static_cast<T>(curq);
			cur = curo;
		}
		cur ^= curq;
	}else if constexpr(isfltpmode && isabsvalue && !issignmode){// one-register filtering
		cur = rotateleftportable<1>(static_cast<T>(cur));
	}
	cur >>= shift;
	return{static_cast<size_t>(cur & 0xFFu)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
		std::is_same_v<longdoubletest96, T> ||
		std::is_same_v<longdoubletest80, T> ||
		std::is_same_v<long double, T> &&
		64 == LDBL_MANT_DIG &&
		16384 == LDBL_MAX_EXP &&
		128 >= CHAR_BIT * sizeof(long double) &&
		64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U),
	size_t> filtershift8(uint_least64_t curm, U cure, unsigned shift)noexcept{
	// Filtering is simplified if possible.
	// This should never filter the top 16 bits.
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		int_least16_t curp{static_cast<int_least16_t>(cure)};
		if constexpr(isabsvalue && !issignmode) curm += curm;
		curp >>= 16 - 1;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		uint_least64_t curq{static_cast<uint_least64_t>(curp)};// sign-extend
#else
		uint_least32_t curq{static_cast<uint_least32_t>(curp)};// sign-extend
#endif
		if constexpr(issignmode){
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
			curm += curq;
#elif (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl)
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymid, checkcarry;
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			curmlo = __builtin_addcl(curmlo, curmlo, 0, &carrymid);
			curmhi = __builtin_addcl(curmhi, curmhi, carrymid, &checkcarry);
			static_cast<void>(checkcarry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#elif defined(_M_IX86)
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			unsigned char checkcarry{_addcarry_u32(_addcarry_u32(0, curmlo, curmlo, &curmlo), curmhi, curmhi, &curmhi)};
			static_cast<void>(checkcarry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#else
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			uint_least32_t curmlotmp{curmlo};
			curmlo += curq;
			curmhi += curq;
			curmhi += curmlo < curmlotmp || curmlo < curq;
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
		}
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		curm ^= curq;
#else
		uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
		curmlo ^= curq;
		curmhi ^= curq;
		alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
		curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
	}else if constexpr(isfltpmode && isabsvalue && !issignmode){// one-register filtering
		uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
		static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
		unsigned short carrysign;
		curo = __builtin_addcs(curo, curo, 0, &carrysign);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
		unsigned long long checkcarry;
		curm = __builtin_addcll(curm, curm, static_cast<unsigned long long>(carrysign), &checkcarry);
#else
		static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long checkcarry;
		curm = __builtin_addcl(curm, curm, static_cast<unsigned long>(carrysign), &checkcarry);
#endif
		static_cast<void>(checkcarry);
#else
		static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long carrymid, checkcarry;
		uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
		curmlo = __builtin_addcl(curmlo, curmlo, static_cast<unsigned long>(carrysign), &carrymid);
		curmhi = __builtin_addcl(curmhi, curmhi, carrymid, &checkcarry);
		static_cast<void>(checkcarry);
		alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
		curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
#elif defined(_M_X64)
		unsigned char checkcarry{_addcarry_u64(_addcarry_u16(0, curo, curo, &curo), curm, curm, &curm)};
		static_cast<void>(checkcarry);
#elif defined(_M_IX86)
		uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
		unsigned char checkcarry{_addcarry_u32(_addcarry_u32(_addcarry_u16(0, curo, curo, &curo), curmlo, curmlo, &curmlo), curmhi, curmhi, &curmhi)};
		static_cast<void>(checkcarry);
		alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
		curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#else
		uint_least16_t curotmp{curo};
		curo += curo;
		curm += curm;
		curm += curo < curotmp;
#endif
	}
	curm >>= shift;
	return{static_cast<size_t>(curm & 0xFFu)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U, typename W>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U) &&
	std::is_unsigned_v<W> &&
	64 >= CHAR_BIT * sizeof(W) &&
	8 < CHAR_BIT * sizeof(W),
	size_t> filtershift8(std::pair<uint_least64_t, W> cur, unsigned shift)noexcept{
	// Use the function above.
	return{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(cur.first, cur.second, shift)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U),
	std::pair<size_t, size_t>> filtershift8(U cura, U curb, unsigned shift)noexcept{
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		if constexpr(isabsvalue && !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		if constexpr(isabsvalue && !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			cura = curoa;
			curb = curob;
		}
		cura ^= curqa;
		curb ^= curqb;
	}else if constexpr(isfltpmode && isabsvalue && !issignmode){// one-register filtering
		cura = rotateleftportable<1>(static_cast<T>(cura));
		curb = rotateleftportable<1>(static_cast<T>(curb));
	}
	cura >>= shift;
	curb >>= shift;
	return{static_cast<size_t>(cura & 0xFFu), static_cast<size_t>(curb & 0xFFu)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
		std::is_same_v<longdoubletest96, T> ||
		std::is_same_v<longdoubletest80, T> ||
		std::is_same_v<long double, T> &&
		64 == LDBL_MANT_DIG &&
		16384 == LDBL_MAX_EXP &&
		128 >= CHAR_BIT * sizeof(long double) &&
		64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U),
	std::pair<size_t, size_t>> filtershift8(uint_least64_t curma, U curea, uint_least64_t curmb, U cureb, unsigned shift)noexcept{
	// Filtering is simplified if possible.
	// This should never filter the top 16 bits.
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		int_least16_t curpa{static_cast<int_least16_t>(curea)};
		int_least16_t curpb{static_cast<int_least16_t>(cureb)};
		if constexpr(isabsvalue && !issignmode){
			curma += curma;
			curmb += curmb;
		}
		curpa >>= 16 - 1;
		curpb >>= 16 - 1;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		uint_least64_t curqa{static_cast<uint_least64_t>(curpa)};// sign-extend
		uint_least64_t curqb{static_cast<uint_least64_t>(curpb)};
#else
		uint_least32_t curqa{static_cast<uint_least32_t>(curpa)};// sign-extend
		uint_least32_t curqb{static_cast<uint_least32_t>(curpb)};
#endif
		if constexpr(issignmode){
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
			curma += curqa;
			curmb += curqb;
#elif (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl)
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymida, checkcarrya;
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			curmloa = __builtin_addcl(curmloa, curmloa, 0, &carrymida);
			curmhia = __builtin_addcl(curmhia, curmhia, carrymida, &checkcarrya);
			static_cast<void>(checkcarrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			unsigned long carrymidb, checkcarryb;
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			curmlob = __builtin_addcl(curmlob, curmlob, 0, &carrymidb);
			curmhib = __builtin_addcl(curmhib, curmhib, carrymidb, &checkcarryb);
			static_cast<void>(checkcarryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#elif defined(_M_IX86)
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u32(_addcarry_u32(0, curmloa, curmloa, &curmloa), curmhia, curmhia, &curmhia)};
			static_cast<void>(checkcarrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u32(_addcarry_u32(0, curmlob, curmlob, &curmlob), curmhib, curmhib, &curmhib)};
			static_cast<void>(checkcarryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#else
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			uint_least32_t curmlotmpa{curmloa}, curmlotmpb{curmlob};
			curmloa += curqa;
			curmhia += curqa;
			curmhia += curmloa < curmlotmpa || curmloa < curqa;
			curmlob += curqb;
			curmhib += curqb;
			curmhib += curmlob < curmlotmpb || curmlob < curqb;
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
		}
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		curma ^= curqa;
		curmb ^= curqb;
#else
		uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		curmloa ^= curqa;
		curmhia ^= curqa;
		curmlob ^= curqb;
		curmhib ^= curqb;
		alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
		curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
		alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
		curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
	}else if constexpr(isfltpmode && isabsvalue && !issignmode){// one-register filtering
		uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
		uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
		static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
		unsigned short carrysigna;
		curoa = __builtin_addcs(curoa, curoa, 0, &carrysigna);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
		unsigned long long checkcarrya;
		curma = __builtin_addcll(curma, curma, static_cast<unsigned long long>(carrysigna), &checkcarrya);
#else
		static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long checkcarrya;
		curma = __builtin_addcl(curma, curma, static_cast<unsigned long>(carrysigna), &checkcarrya);
#endif
		static_cast<void>(checkcarrya);
#else
		static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long carrymida, checkcarrya;
		uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		curmloa = __builtin_addcl(curmloa, curmloa, static_cast<unsigned long>(carrysigna), &carrymida);
		curmhia = __builtin_addcl(curmhia, curmhia, carrymida, &checkcarrya);
		static_cast<void>(checkcarrya);
		alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
		curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
#endif
		unsigned short carrysignb;
		curob = __builtin_addcs(curob, curob, 0, &carrysignb);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		unsigned long long checkcarryb;
		curmb = __builtin_addcll(curmb, curmb, static_cast<unsigned long long>(carrysignb), &checkcarryb);
#else
		unsigned long checkcarryb;
		curmb = __builtin_addcl(curmb, curmb, static_cast<unsigned long>(carrysignb), &checkcarryb);
#endif
		static_cast<void>(checkcarryb);
#else
		unsigned long carrymidb, checkcarryb;
		uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		curmlob = __builtin_addcl(curmlob, curmlob, static_cast<unsigned long>(carrysignb), &carrymidb);
		curmhib = __builtin_addcl(curmhib, curmhib, carrymidb, &checkcarryb);
		static_cast<void>(checkcarryb);
		alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
		curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
#elif defined(_M_X64)
		unsigned char checkcarrya{_addcarry_u64(_addcarry_u16(0, curoa, curoa, &curoa), curma, curma, &curma)};
		static_cast<void>(checkcarrya);
		unsigned char checkcarryb{_addcarry_u64(_addcarry_u16(0, curob, curob, &curob), curmb, curmb, &curmb)};
		static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
		uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		unsigned char checkcarrya{_addcarry_u32(_addcarry_u32(_addcarry_u16(0, curoa, curoa, &curoa), curmloa, curmloa, &curmloa), curmhia, curmhia, &curmhia)};
		static_cast<void>(checkcarrya);
		alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
		curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
		uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		unsigned char checkcarryb{_addcarry_u32(_addcarry_u32(_addcarry_u16(0, curob, curob, &curob), curmlob, curmlob, &curmlob), curmhib, curmhib, &curmhib)};
		static_cast<void>(checkcarryb);
		alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
		curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#else
		uint_least16_t curotmpa{curoa}, curotmpb{curob};
		curoa += curoa;
		curma += curma;
		curma += curoa < curotmpa;
		curob += curob;
		curmb += curmb;
		curmb += curob < curotmpb;
#endif
	}
	curma >>= shift;
	curmb >>= shift;
	return{static_cast<size_t>(curma & 0xFFu), static_cast<size_t>(curmb & 0xFFu)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U, typename W>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U) &&
	std::is_unsigned_v<W> &&
	64 >= CHAR_BIT * sizeof(W) &&
	8 < CHAR_BIT * sizeof(W),
	std::pair<size_t, size_t>> filtershift8(std::pair<uint_least64_t, W> cura, std::pair<uint_least64_t, W> curb, unsigned shift)noexcept{
	// Use the function above.
	return{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(cura.first, cura.second, curb.first, curb.second, shift)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U),
	std::tuple<size_t, size_t, size_t, size_t>> filtershift8(U cura, U curb, U curc, U curd, unsigned shift)noexcept{
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		if constexpr(isabsvalue && !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		if constexpr(isabsvalue && !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		std::make_signed_t<T> curpc{static_cast<std::make_signed_t<T>>(curc)};
		if constexpr(isabsvalue && !issignmode){
			T curoc{static_cast<T>(curc)};
			curoc += curoc;
			curc = curoc;
		}
		curpc >>= CHAR_BIT * sizeof(T) - 1;
		U curqc{static_cast<T>(curpc)};
		std::make_signed_t<T> curpd{static_cast<std::make_signed_t<T>>(curd)};
		if constexpr(isabsvalue && !issignmode){
			T curod{static_cast<T>(curd)};
			curod += curod;
			curd = curod;
		}
		curpd >>= CHAR_BIT * sizeof(T) - 1;
		U curqd{static_cast<T>(curpd)};
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			T curoc{static_cast<T>(curc)};
			T curod{static_cast<T>(curd)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			curoc += static_cast<T>(curqc);
			curod += static_cast<T>(curqd);
			cura = curoa;
			curb = curob;
			curc = curoc;
			curd = curod;
		}
		cura ^= curqa;
		curb ^= curqb;
		curc ^= curqc;
		curd ^= curqd;
	}else if constexpr(isfltpmode && isabsvalue && !issignmode){// one-register filtering
		cura = rotateleftportable<1>(static_cast<T>(cura));
		curb = rotateleftportable<1>(static_cast<T>(curb));
		curc = rotateleftportable<1>(static_cast<T>(curc));
		curd = rotateleftportable<1>(static_cast<T>(curd));
	}
	cura >>= shift;
	curb >>= shift;
	curc >>= shift;
	curd >>= shift;
	return{static_cast<size_t>(cura & 0xFFu), static_cast<size_t>(curb & 0xFFu), static_cast<size_t>(curc & 0xFFu), static_cast<size_t>(curd & 0xFFu)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
		std::is_same_v<longdoubletest96, T> ||
		std::is_same_v<longdoubletest80, T> ||
		std::is_same_v<long double, T> &&
		64 == LDBL_MANT_DIG &&
		16384 == LDBL_MAX_EXP &&
		128 >= CHAR_BIT * sizeof(long double) &&
		64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U),
	std::tuple<size_t, size_t, size_t, size_t>> filtershift8(uint_least64_t curma, U curea, uint_least64_t curmb, U cureb, uint_least64_t curmc, U curec, uint_least64_t curmd, U cured, unsigned shift)noexcept{
	// Filtering is simplified if possible.
	// This should never filter the top 16 bits.
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		int_least16_t curpa{static_cast<int_least16_t>(curea)};
		int_least16_t curpb{static_cast<int_least16_t>(cureb)};
		int_least16_t curpc{static_cast<int_least16_t>(curec)};
		int_least16_t curpd{static_cast<int_least16_t>(cured)};
		if constexpr(isabsvalue && !issignmode){
			curma += curma;
			curmb += curmb;
			curmc += curmc;
			curmd += curmd;
		}
		curpa >>= 16 - 1;
		curpb >>= 16 - 1;
		curpc >>= 16 - 1;
		curpd >>= 16 - 1;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		uint_least64_t curqa{static_cast<uint_least64_t>(curpa)};// sign-extend
		uint_least64_t curqb{static_cast<uint_least64_t>(curpb)};
		uint_least64_t curqc{static_cast<uint_least64_t>(curpc)};
		uint_least64_t curqd{static_cast<uint_least64_t>(curpd)};
#else
		uint_least32_t curqa{static_cast<uint_least32_t>(curpa)};// sign-extend
		uint_least32_t curqb{static_cast<uint_least32_t>(curpb)};
		uint_least32_t curqc{static_cast<uint_least32_t>(curpc)};
		uint_least32_t curqd{static_cast<uint_least32_t>(curpd)};
#endif
		if constexpr(issignmode){
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
			curma += curqa;
			curmb += curqb;
			curmc += curqc;
			curmd += curqd;
#elif (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl)
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymida, checkcarrya;
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			curmloa = __builtin_addcl(curmloa, curmloa, 0, &carrymida);
			curmhia = __builtin_addcl(curmhia, curmhia, carrymida, &checkcarrya);
			static_cast<void>(checkcarrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			unsigned long carrymidb, checkcarryb;
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			curmlob = __builtin_addcl(curmlob, curmlob, 0, &carrymidb);
			curmhib = __builtin_addcl(curmhib, curmhib, carrymidb, &checkcarryb);
			static_cast<void>(checkcarryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymidc, checkcarryc;
			uint_least32_t curmloc{static_cast<uint_least32_t>(curmc & 0xFFFFFFFFu)}, curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
			curmloc = __builtin_addcl(curmloc, curmloc, 0, &carrymidc);
			curmhic = __builtin_addcl(curmhic, curmhic, carrymidc, &checkcarryc);
			static_cast<void>(checkcarryc);
			alignas(8) uint_least32_t acurmc[2]{curmloc, curmhic};
			curmc = *reinterpret_cast<uint_least64_t *>(acurmc);// recompose
			unsigned long carrymidd, checkcarryd;
			uint_least32_t curmlod{static_cast<uint_least32_t>(curmd & 0xFFFFFFFFu)}, curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
			curmlod = __builtin_addcl(curmlod, curmlod, 0, &carrymidd);
			curmhid = __builtin_addcl(curmhid, curmhid, carrymidd, &checkcarryd);
			static_cast<void>(checkcarryd);
			alignas(8) uint_least32_t acurmd[2]{curmlod, curmhid};
			curmd = *reinterpret_cast<uint_least64_t *>(acurmd);// recompose
#elif defined(_M_IX86)
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u32(_addcarry_u32(0, curmloa, curmloa, &curmloa), curmhia, curmhia, &curmhia)};
			static_cast<void>(checkcarrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u32(_addcarry_u32(0, curmlob, curmlob, &curmlob), curmhib, curmhib, &curmhib)};
			static_cast<void>(checkcarryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
			uint_least32_t curmloc{static_cast<uint_least32_t>(curmc & 0xFFFFFFFFu)}, curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
			unsigned char checkcarryc{_addcarry_u32(_addcarry_u32(0, curmloc, curmloc, &curmloc), curmhic, curmhic, &curmhic)};
			static_cast<void>(checkcarryc);
			alignas(8) uint_least32_t acurmc[2]{curmloc, curmhic};
			curmc = *reinterpret_cast<uint_least64_t *>(acurmc);// recompose
			uint_least32_t curmlod{static_cast<uint_least32_t>(curmd & 0xFFFFFFFFu)}, curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
			unsigned char checkcarryd{_addcarry_u32(_addcarry_u32(0, curmlod, curmlod, &curmlod), curmhid, curmhid, &curmhid)};
			static_cast<void>(checkcarryd);
			alignas(8) uint_least32_t acurmd[2]{curmlod, curmhid};
			curmd = *reinterpret_cast<uint_least64_t *>(acurmd);// recompose
#else
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			uint_least32_t curmloc{static_cast<uint_least32_t>(curmc & 0xFFFFFFFFu)}, curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
			uint_least32_t curmlod{static_cast<uint_least32_t>(curmd & 0xFFFFFFFFu)}, curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
			uint_least32_t curmlotmpa{curmloa}, curmlotmpb{curmlob}, curmlotmpc{curmloc}, curmlotmpd{curmlod};
			curmloa += curqa;
			curmhia += curqa;
			curmhia += curmloa < curmlotmpa || curmloa < curqa;
			curmlob += curqb;
			curmhib += curqb;
			curmhib += curmlob < curmlotmpb || curmlob < curqb;
			curmloc += curqc;
			curmhic += curqc;
			curmhic += curmloc < curmlotmpc || curmloc < curqc;
			curmlod += curqd;
			curmhid += curqd;
			curmhid += curmlod < curmlotmpd || curmlod < curqd;
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
			alignas(8) uint_least32_t acurmc[2]{curmloc, curmhic};
			curmc = *reinterpret_cast<uint_least64_t *>(acurmc);// recompose
			alignas(8) uint_least32_t acurmd[2]{curmlod, curmhid};
			curmd = *reinterpret_cast<uint_least64_t *>(acurmd);// recompose
#endif
		}
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		curma ^= curqa;
		curmb ^= curqb;
		curmc ^= curqc;
		curmd ^= curqd;
#else
		uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		uint_least32_t curmloc{static_cast<uint_least32_t>(curmc & 0xFFFFFFFFu)}, curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
		uint_least32_t curmlod{static_cast<uint_least32_t>(curmd & 0xFFFFFFFFu)}, curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
		curmloa ^= curqa;
		curmhia ^= curqa;
		curmlob ^= curqb;
		curmhib ^= curqb;
		curmloc ^= curqc;
		curmhic ^= curqc;
		curmlod ^= curqd;
		curmhid ^= curqd;
		alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
		curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
		alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
		curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
		alignas(8) uint_least32_t acurmc[2]{curmloc, curmhic};
		curmc = *reinterpret_cast<uint_least64_t *>(acurmc);// recompose
		alignas(8) uint_least32_t acurmd[2]{curmlod, curmhid};
		curmd = *reinterpret_cast<uint_least64_t *>(acurmd);// recompose
#endif
	}else if constexpr(isfltpmode && isabsvalue && !issignmode){// one-register filtering
		uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
		uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
		uint_least16_t curoc{static_cast<uint_least16_t>(curec)};
		uint_least16_t curod{static_cast<uint_least16_t>(cured)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
		static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
		unsigned short carrysigna;
		curoa = __builtin_addcs(curoa, curoa, 0, &carrysigna);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
		unsigned long long checkcarrya;
		curma = __builtin_addcll(curma, curma, static_cast<unsigned long long>(carrysigna), &checkcarrya);
#else
		static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long checkcarrya;
		curma = __builtin_addcl(curma, curma, static_cast<unsigned long>(carrysigna), &checkcarrya);
#endif
		static_cast<void>(checkcarrya);
#else
		static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
		unsigned long carrymida, checkcarrya;
		uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		curmloa = __builtin_addcl(curmloa, curmloa, static_cast<unsigned long>(carrysigna), &carrymida);
		curmhia = __builtin_addcl(curmhia, curmhia, carrymida, &checkcarrya);
		static_cast<void>(checkcarrya);
		alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
		curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
#endif
		unsigned short carrysignb;
		curob = __builtin_addcs(curob, curob, 0, &carrysignb);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		unsigned long long checkcarryb;
		curmb = __builtin_addcll(curmb, curmb, static_cast<unsigned long long>(carrysignb), &checkcarryb);
#else
		unsigned long checkcarryb;
		curmb = __builtin_addcl(curmb, curmb, static_cast<unsigned long>(carrysignb), &checkcarryb);
#endif
		static_cast<void>(checkcarryb);
#else
		unsigned long carrymidb, checkcarryb;
		uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		curmlob = __builtin_addcl(curmlob, curmlob, static_cast<unsigned long>(carrysignb), &carrymidb);
		curmhib = __builtin_addcl(curmhib, curmhib, carrymidb, &checkcarryb);
		static_cast<void>(checkcarryb);
		alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
		curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
		unsigned short carrysignc;
		curoc = __builtin_addcs(curoc, curoc, 0, &carrysignc);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		unsigned long long checkcarryc;
		curmc = __builtin_addcll(curmc, curmc, static_cast<unsigned long long>(carrysignc), &checkcarryc);
#else
		unsigned long checkcarryc;
		curmc = __builtin_addcl(curmc, curmc, static_cast<unsigned long>(carrysignc), &checkcarryc);
#endif
		static_cast<void>(checkcarryc);
#else
		unsigned long carrymidc, checkcarryc;
		uint_least32_t curmloc{static_cast<uint_least32_t>(curmc & 0xFFFFFFFFu)}, curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
		curmloc = __builtin_addcl(curmloc, curmloc, static_cast<unsigned long>(carrysignc), &carrymidc);
		curmhic = __builtin_addcl(curmhic, curmhic, carrymidc, &checkcarryc);
		static_cast<void>(checkcarryc);
		alignas(8) uint_least32_t acurmc[2]{curmloc, curmhic};
		curmc = *reinterpret_cast<uint_least64_t *>(acurmc);// recompose
#endif
		unsigned short carrysignd;
		curod = __builtin_addcs(curod, curod, 0, &carrysignd);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		unsigned long long checkcarryd;
		curmd = __builtin_addcll(curmd, curmd, static_cast<unsigned long long>(carrysignd), &checkcarryd);
#else
		unsigned long checkcarryd;
		curmd = __builtin_addcl(curmd, curmd, static_cast<unsigned long>(carrysignd), &checkcarryd);
#endif
		static_cast<void>(checkcarryd);
#else
		unsigned long carrymidd, checkcarryd;
		uint_least32_t curmlod{static_cast<uint_least32_t>(curmd & 0xFFFFFFFFu)}, curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
		curmlod = __builtin_addcl(curmlod, curmlod, static_cast<unsigned long>(carrysignd), &carrymidd);
		curmhid = __builtin_addcl(curmhid, curmhid, carrymidd, &checkcarryd);
		static_cast<void>(checkcarryd);
		alignas(8) uint_least32_t acurmd[2]{curmlod, curmhid};
		curmd = *reinterpret_cast<uint_least64_t *>(acurmd);// recompose
#endif
#elif defined(_M_X64)
		unsigned char checkcarrya{_addcarry_u64(_addcarry_u16(0, curoa, curoa, &curoa), curma, curma, &curma)};
		static_cast<void>(checkcarrya);
		unsigned char checkcarryb{_addcarry_u64(_addcarry_u16(0, curob, curob, &curob), curmb, curmb, &curmb)};
		static_cast<void>(checkcarryb);
		unsigned char checkcarryc{_addcarry_u64(_addcarry_u16(0, curoc, curoc, &curoc), curmc, curmc, &curmc)};
		static_cast<void>(checkcarryc);
		unsigned char checkcarryd{_addcarry_u64(_addcarry_u16(0, curod, curod, &curod), curmd, curmd, &curmd)};
		static_cast<void>(checkcarryd);
#elif defined(_M_IX86)
		uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		unsigned char checkcarrya{_addcarry_u32(_addcarry_u32(_addcarry_u16(0, curoa, curoa, &curoa), curmloa, curmloa, &curmloa), curmhia, curmhia, &curmhia)};
		static_cast<void>(checkcarrya);
		alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
		curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
		uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		unsigned char checkcarryb{_addcarry_u32(_addcarry_u32(_addcarry_u16(0, curob, curob, &curob), curmlob, curmlob, &curmlob), curmhib, curmhib, &curmhib)};
		static_cast<void>(checkcarryb);
		alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
		curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
		uint_least32_t curmloc{static_cast<uint_least32_t>(curmc & 0xFFFFFFFFu)}, curmhic{static_cast<uint_least32_t>(curmc >> 32)};// decompose
		unsigned char checkcarryc{_addcarry_u32(_addcarry_u32(_addcarry_u16(0, curoc, curoc, &curoc), curmloc, curmloc, &curmloc), curmhic, curmhic, &curmhic)};
		static_cast<void>(checkcarryc);
		alignas(8) uint_least32_t acurmc[2]{curmloc, curmhic};
		curmc = *reinterpret_cast<uint_least64_t *>(acurmc);// recompose
		uint_least32_t curmlod{static_cast<uint_least32_t>(curmd & 0xFFFFFFFFu)}, curmhid{static_cast<uint_least32_t>(curmd >> 32)};// decompose
		unsigned char checkcarryd{_addcarry_u32(_addcarry_u32(_addcarry_u16(0, curod, curod, &curod), curmlod, curmlod, &curmlod), curmhid, curmhid, &curmhid)};
		static_cast<void>(checkcarryd);
		alignas(8) uint_least32_t acurmd[2]{curmlod, curmhid};
		curmd = *reinterpret_cast<uint_least64_t *>(acurmd);// recompose
#else
		uint_least16_t curotmpa{curoa}, curotmpb{curob}, curotmpc{curoc}, curotmpd{curod};
		curoa += curoa;
		curma += curma;
		curma += curoa < curotmpa;
		curob += curob;
		curmb += curmb;
		curmb += curob < curotmpb;
		curoc += curoc;
		curmc += curmc;
		curmc += curoc < curotmpc;
		curod += curod;
		curmd += curmd;
		curmd += curod < curotmpd;
#endif
	}
	curma >>= shift;
	curmb >>= shift;
	curmc >>= shift;
	curmd >>= shift;
	return{static_cast<size_t>(curma & 0xFFu), static_cast<size_t>(curmb & 0xFFu),static_cast<size_t>(curmc & 0xFFu), static_cast<size_t>(curmd & 0xFFu)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U, typename W>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U) &&
	8 < CHAR_BIT * sizeof(U) &&
	std::is_unsigned_v<W> &&
	64 >= CHAR_BIT * sizeof(W) &&
	8 < CHAR_BIT * sizeof(W),
	std::tuple<size_t, size_t, size_t, size_t>> filtershift8(std::pair<uint_least64_t, W> cura, std::pair<uint_least64_t, W> curb, std::pair<uint_least64_t, W> curc, std::pair<uint_least64_t, W> curd, unsigned shift)noexcept{
	// Use the function above.
	return{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(cura.first, cura.second, curb.first, curb.second, curc.first, curc.second, curd.first, curd.second, shift)};
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(uint_least64_t &curm, U &cure)noexcept{
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		int_least16_t curp{static_cast<int_least16_t>(cure)};
		if constexpr(isfltpmode){
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
			curo += curo;
			cure = curo;
		}else if constexpr(!issignmode){
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carry;
			curm = __builtin_addcll(curm, curm, 0, &carry);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			curm = __builtin_addcl(curm, curm, 0, &carry);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymid, carry;
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			curmlo = __builtin_addcl(curmlo, curmlo, 0, &carrymid);
			curmhi = __builtin_addcl(curmhi, curmhi, carrymid, &carry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
			unsigned short checkcarry;
			curo = __builtin_addcs(curo, curo, static_cast<unsigned short>(carry), &checkcarry);
			static_cast<void>(checkcarry);
#elif defined(_M_X64)
			unsigned char checkcarry{_addcarry_u16(_addcarry_u64(0, curm, curm, &curm), curo, curo, &curo)};
			static_cast<void>(checkcarry);
#elif defined(_M_IX86)
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			unsigned char checkcarry{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmlo, curmlo, &curmlo), curmhi, curmhi, &curmhi), curo, curo, &curo)};
			static_cast<void>(checkcarry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#else
			uint_least64_t curmtmp{curm};
			curm += curm;
			curo += static_cast<uint_least16_t>(curo);
			curo += curm < curmtmp;
#endif
			cure = curo;
		}
		curp >>= 16 - 1;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		uint_least64_t curq{static_cast<uint_least64_t>(curp)};// sign-extend
#else
		uint_least32_t curq{static_cast<uint_least32_t>(curp)};// sign-extend
#endif
		if constexpr(isfltpmode) cure >>= 1;
		if constexpr(issignmode){
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carry;
			curm = __builtin_addcll(curm, curq, 0, &carry);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			curm = __builtin_addcl(curm, curq, 0, &carry);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymid, carry;
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			curmlo = __builtin_addcl(curmlo, curq, 0, &carrymid);
			curmhi = __builtin_addcl(curmhi, curq, carrymid, &carry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
			unsigned short checkcarry;
			curo = __builtin_addcs(curo, static_cast<unsigned short>(curq), static_cast<unsigned short>(carry), &checkcarry);
			static_cast<void>(checkcarry);
#elif defined(_M_X64)
			unsigned char checkcarry{_addcarry_u16(_addcarry_u64(0, curm, curq, &curm), curo, static_cast<uint_least16_t>(curq), &curo)};
			static_cast<void>(checkcarry);
#elif defined(_M_IX86)
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			unsigned char checkcarry{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmlo, curq, &curmlo), curmhi, curq, &curmhi), curo, static_cast<uint_least16_t>(curq), &curo)};
			static_cast<void>(checkcarry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#elif 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
			uint_least64_t curmtmp{curm};
			curm += curq;
			curo += static_cast<uint_least16_t>(curq);
			curo += curm < curmtmp || curm < curq;
#else
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			uint_least32_t curmlotmp{curmlo}, curmhitmp{curmhi};
			curmlo += curq;
			curmhi += curq;
			curmhi += curmlo < curmlotmp || curmlo < curq;
			curo += static_cast<uint_least16_t>(curq);
			curo += curmhi < curmhitmp || curmhi < curq;
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
			cure = curo;
		}
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		curm ^= curq;
#else
		uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
		curmlo ^= curq;
		curmhi ^= curq;
		alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
		curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
		cure ^= static_cast<U>(curq);
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		if constexpr(issignmode) cure &= ~static_cast<uint_least16_t>(0) >> 1;
		else{
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
			unsigned short carrysign;
			curo = __builtin_addcs(curo, curo, 0, &carrysign);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carry;
			curm = __builtin_addcll(curm, curm, static_cast<unsigned long long>(carrysign), &carry);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			curm = __builtin_addcl(curm, curm, static_cast<unsigned long>(carrysign), &carry);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymid, carry;
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			curmlo = __builtin_addcl(curmlo, curmlo, static_cast<unsigned long>(carrysign), &carrymid);
			curmhi = __builtin_addcl(curmhi, curmhi, carrymid, &carry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
			unsigned short checkcarry;
			curo = __builtin_addcs(curo, 0, static_cast<unsigned short>(carry), &checkcarry);
			static_cast<void>(checkcarry);
#elif defined(_M_X64)
			unsigned char checkcarry{_addcarry_u16(_addcarry_u64(_addcarry_u16(0, curo, curo, &curo), curm, curm, &curm), curo, 0, &curo)};
			static_cast<void>(checkcarry);
#elif defined(_M_IX86)
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			unsigned char checkcarry{_addcarry_u16(_addcarry_u32(_addcarry_u32(_addcarry_u16(0, curo, curo, &curo), curmlo, curmlo, &curmlo), curmhi, curmhi, &curmhi), curo, 0, &curo)};
			static_cast<void>(checkcarry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#else
			uint_least16_t curotmp{curo};
			curo += curo;
			uint_least64_t curmtmp{curm};
			curm += curm;
			curm += curo < curotmp;
			curo += curm < curmtmp;
#endif
			cure = curo;
		}
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(uint_least64_t &curm, U &cure, T *out)noexcept{
	// do not pass a nullptr here
	assert(out);
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		int_least16_t curp{static_cast<int_least16_t>(cure)};
		if constexpr(isfltpmode){
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(out) + 1) = static_cast<W>(cure);
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
			curo += curo;
			cure = curo;
		}else if constexpr(!issignmode){
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(out) + 1) = static_cast<W>(cure);
			*reinterpret_cast<uint_least64_t *>(out) = curm;
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carry;
			curm = __builtin_addcll(curm, curm, 0, &carry);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			curm = __builtin_addcl(curm, curm, 0, &carry);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymid, carry;
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			curmlo = __builtin_addcl(curmlo, curmlo, 0, &carrymid);
			curmhi = __builtin_addcl(curmhi, curmhi, carrymid, &carry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
			unsigned short checkcarry;
			curo = __builtin_addcs(curo, curo, static_cast<unsigned short>(carry), &checkcarry);
			static_cast<void>(checkcarry);
#elif defined(_M_X64)
			unsigned char checkcarry{_addcarry_u16(_addcarry_u64(0, curm, curm, &curm), curo, curo, &curo)};
			static_cast<void>(checkcarry);
#elif defined(_M_IX86)
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			unsigned char checkcarry{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmlo, curmlo, &curmlo), curmhi, curmhi, &curmhi), curo, curo, &curo)};
			static_cast<void>(checkcarry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#else
			uint_least64_t curmtmp{curm};
			curm += curm;
			curo += static_cast<uint_least16_t>(curo);
			curo += curm < curmtmp;
#endif
			cure = curo;
		}
		curp >>= 16 - 1;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		uint_least64_t curq{static_cast<uint_least64_t>(curp)};// sign-extend
#else
		uint_least32_t curq{static_cast<uint_least32_t>(curp)};// sign-extend
#endif
		if constexpr(isfltpmode){
			cure >>= 1;
			*reinterpret_cast<uint_least64_t *>(out) = curm;
		}
		if constexpr(issignmode){
			if constexpr(!isfltpmode){
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(out) + 1) = static_cast<W>(cure);
				*reinterpret_cast<uint_least64_t *>(out) = curm;
			}
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carry;
			curm = __builtin_addcll(curm, curq, 0, &carry);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			curm = __builtin_addcl(curm, curq, 0, &carry);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymid, carry;
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			curmlo = __builtin_addcl(curmlo, curq, 0, &carrymid);
			curmhi = __builtin_addcl(curmhi, curq, carrymid, &carry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
			unsigned short checkcarry;
			curo = __builtin_addcs(curo, static_cast<unsigned short>(curq), static_cast<unsigned short>(carry), &checkcarry);
			static_cast<void>(checkcarry);
#elif defined(_M_X64)
			unsigned char checkcarry{_addcarry_u16(_addcarry_u64(0, curm, curq, &curm), curo, static_cast<uint_least16_t>(curq), &curo)};
			static_cast<void>(checkcarry);
#elif defined(_M_IX86)
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			unsigned char checkcarry{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmlo, curq, &curmlo), curmhi, curq, &curmhi), curo, static_cast<uint_least16_t>(curq), &curo)};
			static_cast<void>(checkcarry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#elif 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
			uint_least64_t curmtmp{curm};
			curm += curq;
			curo += static_cast<uint_least16_t>(curq);
			curo += curm < curmtmp || curm < curq;
#else
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			uint_least32_t curmlotmp{curmlo}, curmhitmp{curmhi};
			curmlo += curq;
			curmhi += curq;
			curmhi += curmlo < curmlotmp || curmlo < curq;
			curo += static_cast<uint_least16_t>(curq);
			curo += curmhi < curmhitmp || curmhi < curq;
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
			cure = curo;
		}
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		curm ^= curq;
#else
		uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
		curmlo ^= curq;
		curmhi ^= curq;
		alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
		curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
		cure ^= static_cast<U>(curq);
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(out) + 1) = static_cast<W>(cure);
		if constexpr(issignmode){
			cure &= ~static_cast<uint_least16_t>(0) >> 1;
			*reinterpret_cast<uint_least64_t *>(out) = curm;
		}else{
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
			unsigned short carrysign;
			curo = __builtin_addcs(curo, curo, 0, &carrysign);
			*reinterpret_cast<uint_least64_t *>(out) = curm;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carry;
			curm = __builtin_addcll(curm, curm, static_cast<unsigned long long>(carrysign), &carry);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			curm = __builtin_addcl(curm, curm, static_cast<unsigned long>(carrysign), &carry);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymid, carry;
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			curmlo = __builtin_addcl(curmlo, curmlo, static_cast<unsigned long>(carrysign), &carrymid);
			curmhi = __builtin_addcl(curmhi, curmhi, carrymid, &carry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
			unsigned short checkcarry;
			curo = __builtin_addcs(curo, 0, static_cast<unsigned short>(carry), &checkcarry);
			static_cast<void>(checkcarry);
#elif defined(_M_X64)
			unsigned char carrysign{_addcarry_u16(0, curo, curo, &curo)};
			*reinterpret_cast<uint_least64_t *>(out) = curm;
			unsigned char checkcarry{_addcarry_u16(_addcarry_u64(carrysign, curm, curm, &curm), curo, 0, &curo)};
			static_cast<void>(checkcarry);
#elif defined(_M_IX86)
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			unsigned char carrysign{_addcarry_u16(0, curo, curo, &curo)};
			*reinterpret_cast<uint_least64_t *>(out) = curm;
			unsigned char checkcarry{_addcarry_u16(_addcarry_u32(_addcarry_u32(carrysign, curmlo, curmlo, &curmlo), curmhi, curmhi, &curmhi), curo, 0, &curo)};
			static_cast<void>(checkcarry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#else
			uint_least16_t curotmp{curo};
			curo += curo;
			*reinterpret_cast<uint_least64_t *>(out) = curm;
			uint_least64_t curmtmp{curm};
			curm += curm;
			curm += curo < curotmp;
			curo += curm < curmtmp;
#endif
			cure = curo;
		}
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(uint_least64_t &curm, U &cure, T *out, T *dst)noexcept{
	// do not pass a nullptr here
	assert(out);
	assert(dst);
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		int_least16_t curp{static_cast<int_least16_t>(cure)};
		if constexpr(isfltpmode){
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(out) + 1) = static_cast<W>(cure);
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(dst) + 1) = static_cast<W>(cure);
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
			curo += curo;
			cure = curo;
		}else if constexpr(!issignmode){
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(out) + 1) = static_cast<W>(cure);
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(dst) + 1) = static_cast<W>(cure);
			*reinterpret_cast<uint_least64_t *>(out) = curm;
			*reinterpret_cast<uint_least64_t *>(dst) = curm;
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carry;
			curm = __builtin_addcll(curm, curm, 0, &carry);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			curm = __builtin_addcl(curm, curm, 0, &carry);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymid, carry;
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			curmlo = __builtin_addcl(curmlo, curmlo, 0, &carrymid);
			curmhi = __builtin_addcl(curmhi, curmhi, carrymid, &carry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
			unsigned short checkcarry;
			curo = __builtin_addcs(curo, curo, static_cast<unsigned short>(carry), &checkcarry);
			static_cast<void>(checkcarry);
#elif defined(_M_X64)
			unsigned char checkcarry{_addcarry_u16(_addcarry_u64(0, curm, curm, &curm), curo, curo, &curo)};
			static_cast<void>(checkcarry);
#elif defined(_M_IX86)
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			unsigned char checkcarry{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmlo, curmlo, &curmlo), curmhi, curmhi, &curmhi), curo, curo, &curo)};
			static_cast<void>(checkcarry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#else
			uint_least64_t curmtmp{curm};
			curm += curm;
			curo += static_cast<uint_least16_t>(curo);
			curo += curm < curmtmp;
#endif
			cure = curo;
		}
		curp >>= 16 - 1;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		uint_least64_t curq{static_cast<uint_least64_t>(curp)};// sign-extend
#else
		uint_least32_t curq{static_cast<uint_least32_t>(curp)};// sign-extend
#endif
		if constexpr(isfltpmode){
			cure >>= 1;
			*reinterpret_cast<uint_least64_t *>(out) = curm;
			*reinterpret_cast<uint_least64_t *>(dst) = curm;
		}
		if constexpr(issignmode){
			if constexpr(!isfltpmode){
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(out) + 1) = static_cast<W>(cure);
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(dst) + 1) = static_cast<W>(cure);
				*reinterpret_cast<uint_least64_t *>(out) = curm;
				*reinterpret_cast<uint_least64_t *>(dst) = curm;
			}
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carry;
			curm = __builtin_addcll(curm, curq, 0, &carry);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			curm = __builtin_addcl(curm, curq, 0, &carry);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymid, carry;
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			curmlo = __builtin_addcl(curmlo, curq, 0, &carrymid);
			curmhi = __builtin_addcl(curmhi, curq, carrymid, &carry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
			unsigned short checkcarry;
			curo = __builtin_addcs(curo, static_cast<unsigned short>(curq), static_cast<unsigned short>(carry), &checkcarry);
			static_cast<void>(checkcarry);
#elif defined(_M_X64)
			unsigned char checkcarry{_addcarry_u16(_addcarry_u64(0, curm, curq, &curm), curo, static_cast<uint_least16_t>(curq), &curo)};
			static_cast<void>(checkcarry);
#elif defined(_M_IX86)
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			unsigned char checkcarry{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmlo, curq, &curmlo), curmhi, curq, &curmhi), curo, static_cast<uint_least16_t>(curq), &curo)};
			static_cast<void>(checkcarry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#elif 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
			uint_least64_t curmtmp{curm};
			curm += curq;
			curo += static_cast<uint_least16_t>(curq);
			curo += curm < curmtmp || curm < curq;
#else
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			uint_least32_t curmlotmp{curmlo}, curmhitmp{curmhi};
			curmlo += curq;
			curmhi += curq;
			curmhi += curmlo < curmlotmp || curmlo < curq;
			curo += static_cast<uint_least16_t>(curq);
			curo += curmhi < curmhitmp || curmhi < curq;
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
			cure = curo;
		}
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		curm ^= curq;
#else
		uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
		curmlo ^= curq;
		curmhi ^= curq;
		alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
		curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
		cure ^= static_cast<U>(curq);
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(out) + 1) = static_cast<W>(cure);
		*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(dst) + 1) = static_cast<W>(cure);
		if constexpr(issignmode){
			cure &= ~static_cast<uint_least16_t>(0) >> 1;
			*reinterpret_cast<uint_least64_t *>(out) = curm;
			*reinterpret_cast<uint_least64_t *>(dst) = curm;
		}else{
			uint_least16_t curo{static_cast<uint_least16_t>(cure)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
			unsigned short carrysign;
			curo = __builtin_addcs(curo, curo, 0, &carrysign);
			*reinterpret_cast<uint_least64_t *>(out) = curm;
			*reinterpret_cast<uint_least64_t *>(dst) = curm;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carry;
			curm = __builtin_addcll(curm, curm, static_cast<unsigned long long>(carrysign), &carry);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carry;
			curm = __builtin_addcl(curm, curm, static_cast<unsigned long>(carrysign), &carry);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymid, carry;
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			curmlo = __builtin_addcl(curmlo, curmlo, static_cast<unsigned long>(carrysign), &carrymid);
			curmhi = __builtin_addcl(curmhi, curmhi, carrymid, &carry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#endif
			unsigned short checkcarry;
			curo = __builtin_addcs(curo, 0, static_cast<unsigned short>(carry), &checkcarry);
			static_cast<void>(checkcarry);
#elif defined(_M_X64)
			unsigned char carrysign{_addcarry_u16(0, curo, curo, &curo)};
			*reinterpret_cast<uint_least64_t *>(out) = curm;
			*reinterpret_cast<uint_least64_t *>(dst) = curm;
			unsigned char checkcarry{_addcarry_u16(_addcarry_u64(carrysign, curm, curm, &curm), curo, 0, &curo)};
			static_cast<void>(checkcarry);
#elif defined(_M_IX86)
			uint_least32_t curmlo{static_cast<uint_least32_t>(curm & 0xFFFFFFFFu)}, curmhi{static_cast<uint_least32_t>(curm >> 32)};// decompose
			unsigned char carrysign{_addcarry_u16(0, curo, curo, &curo)};
			*reinterpret_cast<uint_least64_t *>(out) = curm;
			*reinterpret_cast<uint_least64_t *>(dst) = curm;
			unsigned char checkcarry{_addcarry_u16(_addcarry_u32(_addcarry_u32(carrysign, curmlo, curmlo, &curmlo), curmhi, curmhi, &curmhi), curo, 0, &curo)};
			static_cast<void>(checkcarry);
			alignas(8) uint_least32_t acurm[2]{curmlo, curmhi};
			curm = *reinterpret_cast<uint_least64_t *>(acurm);// recompose
#else
			uint_least16_t curotmp{curo};
			curo += curo;
			*reinterpret_cast<uint_least64_t *>(out) = curm;
			*reinterpret_cast<uint_least64_t *>(dst) = curm;
			uint_least64_t curmtmp{curm};
			curm += curm;
			curm += curo < curotmp;
			curo += curm < curmtmp;
#endif
			cure = curo;
		}
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(uint_least64_t &curma, U &curea, uint_least64_t &curmb, U &cureb)noexcept{
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		int_least16_t curpa{static_cast<int_least16_t>(curea)};
		int_least16_t curpb{static_cast<int_least16_t>(cureb)};
		if constexpr(isfltpmode){
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			curoa += curoa;
			curea = curoa;
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
			curob += curob;
			cureb = curob;
		}else if constexpr(!issignmode){
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			curma = __builtin_addcll(curma, curma, 0, &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			curma = __builtin_addcl(curma, curma, 0, &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymida, carrya;
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			curmloa = __builtin_addcl(curmloa, curmloa, 0, &carrymida);
			curmhia = __builtin_addcl(curmhia, curmhia, carrymida, &carrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
#endif
			unsigned short checkcarrya;
			curoa = __builtin_addcs(curoa, curoa, static_cast<unsigned short>(carrya), &checkcarrya);
			static_cast<void>(checkcarrya);
			unsigned long carryb;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			curmb = __builtin_addcll(curmb, curmb, 0, &carryb);
#else
			unsigned long carryb;
			curmb = __builtin_addcl(curmb, curmb, 0, &carryb);
#endif
#else
			unsigned long carrymidb, carryb;
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			curmlob = __builtin_addcl(curmlob, curmlob, 0, &carrymidb);
			curmhib = __builtin_addcl(curmhib, curmhib, carrymidb, &carryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, curob, static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#elif defined(_M_X64)
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curma, &curma), curoa, curoa, &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curmb, &curmb), curob, curob, &curob)};
			static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmloa, curmloa, &curmloa), curmhia, curmhia, &curmhia), curoa, curoa, &curoa)};
			static_cast<void>(checkcarrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmlob, curmlob, &curmlob), curmhib, curmhib, &curmhib), curob, curob, &curob)};
			static_cast<void>(checkcarryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#else
			uint_least64_t curmtmpa{curma};
			curma += curma;
			curoa += static_cast<uint_least16_t>(curoa);
			curoa += curma < curmtmpa;
			uint_least64_t curmtmpb{curmb};
			curmb += curmb;
			curob += static_cast<uint_least16_t>(curob);
			curob += curmb < curmtmpb;
#endif
			curea = curoa;
			cureb = curob;
		}
		curpa >>= 16 - 1;
		curpb >>= 16 - 1;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		uint_least64_t curqa{static_cast<uint_least64_t>(curpa)};// sign-extend
		uint_least64_t curqb{static_cast<uint_least64_t>(curpb)};
#else
		uint_least32_t curqa{static_cast<uint_least32_t>(curpa)};// sign-extend
		uint_least32_t curqb{static_cast<uint_least32_t>(curpb)};
#endif
		if constexpr(isfltpmode){
			curea >>= 1;
			cureb >>= 1;
		}
		if constexpr(issignmode){
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			curma = __builtin_addcll(curma, curqa, 0, &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			curma = __builtin_addcl(curma, curqa, 0, &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymida, carrya;
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			curmloa = __builtin_addcl(curmloa, curqa, 0, &carrymida);
			curmhia = __builtin_addcl(curmhia, curqa, carrymida, &carrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
#endif
			unsigned long carryb;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			curmb = __builtin_addcll(curmb, curqb, 0, &carryb);
#else
			unsigned long carryb;
			curmb = __builtin_addcl(curmb, curqb, 0, &carryb);
#endif
#else
			unsigned long carrymidb, carryb;
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			curmlob = __builtin_addcl(curmlob, curqb, 0, &carrymidb);
			curmhib = __builtin_addcl(curmhib, curqb, carrymidb, &carryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, static_cast<unsigned short>(curqb), static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#elif defined(_M_X64)
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curqa, &curma), curoa, static_cast<uint_least16_t>(curqa), &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curqb, &curmb), curob, static_cast<uint_least16_t>(curqb), &curob)};
			static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmloa, curqa, &curmloa), curmhia, curqa, &curmhia), curoa, static_cast<uint_least16_t>(curqa), &curoa)};
			static_cast<void>(checkcarrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmlob, curqb, &curmlob), curmhib, curqb, &curmhib), curob, static_cast<uint_least16_t>(curqb), &curob)};
			static_cast<void>(checkcarryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#elif 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
			uint_least64_t curmtmpa{curma};
			curma += curqa;
			curoa += static_cast<uint_least16_t>(curqa);
			curoa += curma < curmtmpa || curma < curqa;
			uint_least64_t curmtmpb{curmb};
			curmb += curqb;
			curob += static_cast<uint_least16_t>(curqb);
			curob += curmb < curmtmpb || curmb < curqb;
#else
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			uint_least32_t curmlotmpa{curmloa}, curmhitmpa{curmhia};
			curmloa += curqa;
			curmhia += curqa;
			curmhia += curmloa < curmlotmpa || curmloa < curqa;
			curoa += static_cast<uint_least16_t>(curqa);
			curoa += curmhia < curmhitmpa || curmhia < curqa;
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			uint_least32_t curmlotmpb{curmlob}, curmhitmpb{curmhib};
			curmlob += curqb;
			curmhib += curqb;
			curmhib += curmlob < curmlotmpb || curmlob < curqb;
			curob += static_cast<uint_least16_t>(curqb);
			curob += curmhib < curmhitmpb || curmhib < curqb;
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
			curea = curoa;
			cureb = curob;
		}
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		curma ^= curqa;
		curmb ^= curqb;
#else
		uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		curmloa ^= curqa;
		curmhia ^= curqa;
		alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
		curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
		uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		curmlob ^= curqb;
		curmhib ^= curqb;
		alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
		curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
		curea ^= static_cast<U>(curqa);
		cureb ^= static_cast<U>(curqb);
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		if constexpr(issignmode){
			curea &= ~static_cast<uint_least16_t>(0) >> 1;
			cureb &= ~static_cast<uint_least16_t>(0) >> 1;
		}else{
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
			unsigned short carrysigna;
			curoa = __builtin_addcs(curoa, curoa, 0, &carrysigna);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			curma = __builtin_addcll(curma, curma, static_cast<unsigned long long>(carrysigna), &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			curma = __builtin_addcl(curma, curma, static_cast<unsigned long>(carrysigna), &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymida, carrya;
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			curmloa = __builtin_addcl(curmloa, curmloa, static_cast<unsigned long>(carrysigna), &carrymida);
			curmhia = __builtin_addcl(curmhia, curmhia, carrymida, &carrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
#endif
			unsigned short checkcarrya;
			curoa = __builtin_addcs(curoa, 0, static_cast<unsigned short>(carrya), &checkcarrya);
			static_cast<void>(checkcarrya);
			unsigned short carrysignb;
			curob = __builtin_addcs(curob, curob, 0, &carrysignb);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			curmb = __builtin_addcll(curmb, curmb, static_cast<unsigned long long>(carrysignb), &carryb);
#else
			unsigned long carryb;
			curmb = __builtin_addcl(curmb, curmb, static_cast<unsigned long>(carrysignb), &carryb);
#endif
#else
			unsigned long carrymidb, carryb;
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			curmlob = __builtin_addcl(curmlob, curmlob, static_cast<unsigned long>(carrysignb), &carrymidb);
			curmhib = __builtin_addcl(curmhib, curmhib, carrymidb, &carryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, 0, static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#elif defined(_M_X64)
			unsigned char carrysigna{_addcarry_u16(0, curoa, curoa, &curoa)};
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(carrysigna, curma, curma, &curma), curoa, 0, &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char carrysignb{_addcarry_u16(0, curob, curob, &curob)};
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(carrysignb, curmb, curmb, &curmb), curob, 0, &curob)};
			static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char carrysigna{_addcarry_u16(0, curoa, curoa, &curoa)};
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(_addcarry_u32(carrysigna, curmloa, curmloa, &curmloa), curmhia, curmhia, &curmhia), curoa, 0, &curoa)};
			static_cast<void>(checkcarrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char carrysignb{_addcarry_u16(0, curob, curob, &curob)};
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(_addcarry_u32(carrysignb, curmlob, curmlob, &curmlob), curmhib, curmhib, &curmhib), curob, 0, &curob)};
			static_cast<void>(checkcarryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#else
			uint_least16_t curotmpa{curoa};
			curoa += curoa;
			uint_least64_t curmtmpa{curma};
			curma += curma;
			curma += curoa < curotmpa;
			curoa += curma < curmtmpa;
			uint_least16_t curotmpb{curob};
			curob += curob;
			uint_least64_t curmtmpb{curmb};
			curmb += curmb;
			curmb += curob < curotmpb;
			curob += curmb < curmtmpb;
#endif
			curea = curoa;
			cureb = curob;
		}
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(uint_least64_t &curma, U &curea, T *outa, uint_least64_t &curmb, U &cureb, T *outb)noexcept{
	// do not pass a nullptr here
	assert(outa);
	assert(outb);
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		int_least16_t curpa{static_cast<int_least16_t>(curea)};
		int_least16_t curpb{static_cast<int_least16_t>(cureb)};
		if constexpr(isfltpmode){
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outa) + 1) = static_cast<W>(curea);
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			curoa += curoa;
			curea = curoa;
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outb) + 1) = static_cast<W>(cureb);
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
			curob += curob;
			cureb = curob;
		}else if constexpr(!issignmode){
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outa) + 1) = static_cast<W>(curea);
			*reinterpret_cast<uint_least64_t *>(outa) = curma;
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outb) + 1) = static_cast<W>(cureb);
			*reinterpret_cast<uint_least64_t *>(outb) = curmb;
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			curma = __builtin_addcll(curma, curma, 0, &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			curma = __builtin_addcl(curma, curma, 0, &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymida, carrya;
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			curmloa = __builtin_addcl(curmloa, curmloa, 0, &carrymida);
			curmhia = __builtin_addcl(curmhia, curmhia, carrymida, &carrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
#endif
			unsigned short checkcarrya;
			curoa = __builtin_addcs(curoa, curoa, static_cast<unsigned short>(carrya), &checkcarrya);
			static_cast<void>(checkcarrya);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			curmb = __builtin_addcll(curmb, curmb, 0, &carryb);
#else
			unsigned long carryb;
			curmb = __builtin_addcl(curmb, curmb, 0, &carryb);
#endif
#else
			unsigned long carrymidb, carryb;
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			curmlob = __builtin_addcl(curmlob, curmlob, 0, &carrymidb);
			curmhib = __builtin_addcl(curmhib, curmhib, carrymidb, &carryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, curob, static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#elif defined(_M_X64)
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curma, &curma), curoa, curoa, &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curmb, &curmb), curob, curob, &curob)};
			static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmloa, curmloa, &curmloa), curmhia, curmhia, &curmhia), curoa, curoa, &curoa)};
			static_cast<void>(checkcarrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmlob, curmlob, &curmlob), curmhib, curmhib, &curmhib), curob, curob, &curob)};
			static_cast<void>(checkcarryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#else
			uint_least64_t curmtmpa{curma};
			curma += curma;
			curoa += static_cast<uint_least16_t>(curoa);
			curoa += curma < curmtmpa;
			uint_least64_t curmtmpb{curmb};
			curmb += curmb;
			curob += static_cast<uint_least16_t>(curob);
			curob += curmb < curmtmpb;
#endif
			curea = curoa;
			cureb = curob;
		}
		curpa >>= 16 - 1;
		curpb >>= 16 - 1;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		uint_least64_t curqa{static_cast<uint_least64_t>(curpa)};// sign-extend
		uint_least64_t curqb{static_cast<uint_least64_t>(curpb)};
#else
		uint_least32_t curqa{static_cast<uint_least32_t>(curpa)};// sign-extend
		uint_least32_t curqb{static_cast<uint_least32_t>(curpb)};
#endif
		if constexpr(isfltpmode){
			curea >>= 1;
			cureb >>= 1;
			*reinterpret_cast<uint_least64_t *>(outa) = curma;
			*reinterpret_cast<uint_least64_t *>(outb) = curmb;
		}
		if constexpr(issignmode){
			if constexpr(!isfltpmode){
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outa) + 1) = static_cast<W>(curea);
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outb) + 1) = static_cast<W>(cureb);
				*reinterpret_cast<uint_least64_t *>(outa) = curma;
				*reinterpret_cast<uint_least64_t *>(outb) = curmb;
			}
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			curma = __builtin_addcll(curma, curqa, 0, &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			curma = __builtin_addcl(curma, curqa, 0, &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymida, carrya;
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			curmloa = __builtin_addcl(curmloa, curqa, 0, &carrymida);
			curmhia = __builtin_addcl(curmhia, curqa, carrymida, &carrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
#endif
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			curmb = __builtin_addcll(curmb, curqb, 0, &carryb);
#else
			unsigned long carryb;
			curmb = __builtin_addcl(curmb, curqb, 0, &carryb);
#endif
#else
			unsigned long carrymidb, carryb;
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			curmlob = __builtin_addcl(curmlob, curqb, 0, &carrymidb);
			curmhib = __builtin_addcl(curmhib, curqb, carrymidb, &carryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, static_cast<unsigned short>(curqb), static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#elif defined(_M_X64)
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curqa, &curma), curoa, static_cast<uint_least16_t>(curqa), &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curqb, &curmb), curob, static_cast<uint_least16_t>(curqb), &curob)};
			static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmloa, curqa, &curmloa), curmhia, curqa, &curmhia), curoa, static_cast<uint_least16_t>(curqa), &curoa)};
			static_cast<void>(checkcarrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmlob, curqb, &curmlob), curmhib, curqb, &curmhib), curob, static_cast<uint_least16_t>(curqb), &curob)};
			static_cast<void>(checkcarryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#elif 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
			uint_least64_t curmtmpa{curma};
			curma += curqa;
			curoa += static_cast<uint_least16_t>(curqa);
			curoa += curma < curmtmpa || curma < curqa;
			uint_least64_t curmtmpb{curmb};
			curmb += curqb;
			curob += static_cast<uint_least16_t>(curqb);
			curob += curmb < curmtmpb || curmb < curqb;
#else
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			uint_least32_t curmlotmpa{curmloa}, curmhitmpa{curmhia};
			curmloa += curqa;
			curmhia += curqa;
			curmhia += curmloa < curmlotmpa || curmloa < curqa;
			curoa += static_cast<uint_least16_t>(curqa);
			curoa += curmhia < curmhitmpa || curmhia < curqa;
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			uint_least32_t curmlotmpb{curmlob}, curmhitmpb{curmhib};
			curmlob += curqb;
			curmhib += curqb;
			curmhib += curmlob < curmlotmpb || curmlob < curqb;
			curob += static_cast<uint_least16_t>(curqb);
			curob += curmhib < curmhitmpb || curmhib < curqb;
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
			curea = curoa;
			cureb = curob;
		}
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		curma ^= curqa;
		curmb ^= curqb;
#else
		uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		curmloa ^= curqa;
		curmhia ^= curqa;
		alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
		curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
		uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		curmlob ^= curqb;
		curmhib ^= curqb;
		alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
		curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
		curea ^= static_cast<U>(curqa);
		cureb ^= static_cast<U>(curqb);
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outa) + 1) = static_cast<W>(curea);
		*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outb) + 1) = static_cast<W>(cureb);
		if constexpr(issignmode){
			curea &= ~static_cast<uint_least16_t>(0) >> 1;
			cureb &= ~static_cast<uint_least16_t>(0) >> 1;
			*reinterpret_cast<uint_least64_t *>(outa) = curma;
			*reinterpret_cast<uint_least64_t *>(outb) = curmb;
		}else{
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
			unsigned short carrysigna;
			curoa = __builtin_addcs(curoa, curoa, 0, &carrysigna);
			*reinterpret_cast<uint_least64_t *>(outa) = curma;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			curma = __builtin_addcll(curma, curma, static_cast<unsigned long long>(carrysigna), &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			curma = __builtin_addcl(curma, curma, static_cast<unsigned long>(carrysigna), &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymida, carrya;
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			curmloa = __builtin_addcl(curmloa, curmloa, static_cast<unsigned long>(carrysigna), &carrymida);
			curmhia = __builtin_addcl(curmhia, curmhia, carrymida, &carrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
#endif
			unsigned short checkcarrya;
			curoa = __builtin_addcs(curoa, 0, static_cast<unsigned short>(carrya), &checkcarrya);
			static_cast<void>(checkcarrya);
			unsigned short carrysignb;
			curob = __builtin_addcs(curob, curob, 0, &carrysignb);
			*reinterpret_cast<uint_least64_t *>(outb) = curmb;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			curmb = __builtin_addcll(curmb, curmb, static_cast<unsigned long long>(carrysignb), &carryb);
#else
			unsigned long carryb;
			curmb = __builtin_addcl(curmb, curmb, static_cast<unsigned long>(carrysignb), &carryb);
#endif
#else
			unsigned long carrymidb, carryb;
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			curmlob = __builtin_addcl(curmlob, curmlob, static_cast<unsigned long>(carrysignb), &carrymidb);
			curmhib = __builtin_addcl(curmhib, curmhib, carrymidb, &carryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, 0, static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#elif defined(_M_X64)
			unsigned char carrysigna{_addcarry_u16(0, curoa, curoa, &curoa)};
			*reinterpret_cast<uint_least64_t *>(outa) = curma;
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(carrysigna, curma, curma, &curma), curoa, 0, &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char carrysignb{_addcarry_u16(0, curob, curob, &curob)};
			*reinterpret_cast<uint_least64_t *>(outb) = curmb;
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(carrysignb, curmb, curmb, &curmb), curob, 0, &curob)};
			static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char carrysigna{_addcarry_u16(0, curoa, curoa, &curoa)};
			*reinterpret_cast<uint_least64_t *>(outa) = curma;
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(_addcarry_u32(carrysigna, curmloa, curmloa, &curmloa), curmhia, curmhia, &curmhia), curoa, 0, &curoa)};
			static_cast<void>(checkcarrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char carrysignb{_addcarry_u16(0, curob, curob, &curob)};
			*reinterpret_cast<uint_least64_t *>(outb) = curmb;
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(_addcarry_u32(carrysignb, curmlob, curmlob, &curmlob), curmhib, curmhib, &curmhib), curob, 0, &curob)};
			static_cast<void>(checkcarryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#else
			uint_least16_t curotmpa{curoa};
			curoa += curoa;
			*reinterpret_cast<uint_least64_t *>(outa) = curma;
			uint_least64_t curmtmpa{curma};
			curma += curma;
			curma += curoa < curotmpa;
			curoa += curma < curmtmpa;
			uint_least16_t curotmpb{curob};
			curob += curob;
			*reinterpret_cast<uint_least64_t *>(outb) = curmb;
			uint_least64_t curmtmpb{curmb};
			curmb += curmb;
			curmb += curob < curotmpb;
			curob += curmb < curmtmpb;
#endif
			curea = curoa;
			cureb = curob;
		}
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(uint_least64_t &curma, U &curea, T *outa, T *dsta, uint_least64_t &curmb, U &cureb, T *outb, T *dstb)noexcept{
	// do not pass a nullptr here
	assert(outa);
	assert(dsta);
	assert(outb);
	assert(dstb);
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		int_least16_t curpa{static_cast<int_least16_t>(curea)};
		int_least16_t curpb{static_cast<int_least16_t>(cureb)};
		if constexpr(isfltpmode){
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outa) + 1) = static_cast<W>(curea);
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(dsta) + 1) = static_cast<W>(curea);
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			curoa += curoa;
			curea = curoa;
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outb) + 1) = static_cast<W>(cureb);
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(dstb) + 1) = static_cast<W>(cureb);
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
			curob += curob;
			cureb = curob;
		}else if constexpr(!issignmode){
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outa) + 1) = static_cast<W>(curea);
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(dsta) + 1) = static_cast<W>(curea);
			*reinterpret_cast<uint_least64_t *>(outa) = curma;
			*reinterpret_cast<uint_least64_t *>(dsta) = curma;
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outb) + 1) = static_cast<W>(cureb);
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(dstb) + 1) = static_cast<W>(cureb);
			*reinterpret_cast<uint_least64_t *>(outb) = curmb;
			*reinterpret_cast<uint_least64_t *>(dstb) = curmb;
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			curma = __builtin_addcll(curma, curma, 0, &carrya);
#else
			unsigned long carrya;
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			curma = __builtin_addcl(curma, curma, 0, &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymida, carrya;
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			curmloa = __builtin_addcl(curmloa, curmloa, 0, &carrymida);
			curmhia = __builtin_addcl(curmhia, curmhia, carrymida, &carrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
#endif
			unsigned short checkcarrya;
			curoa = __builtin_addcs(curoa, curoa, static_cast<unsigned short>(carrya), &checkcarrya);
			static_cast<void>(checkcarrya);
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			curmb = __builtin_addcll(curmb, curmb, 0, &carryb);
#else
			unsigned long carryb;
			curmb = __builtin_addcl(curmb, curmb, 0, &carryb);
#endif
#else
			unsigned long carrymidb, carryb;
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			curmlob = __builtin_addcl(curmlob, curmlob, 0, &carrymidb);
			curmhib = __builtin_addcl(curmhib, curmhib, carrymidb, &carryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, curob, static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#elif defined(_M_X64)
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curma, &curma), curoa, curoa, &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curmb, &curmb), curob, curob, &curob)};
			static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmloa, curmloa, &curmloa), curmhia, curmhia, &curmhia), curoa, curoa, &curoa)};
			static_cast<void>(checkcarrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmlob, curmlob, &curmlob), curmhib, curmhib, &curmhib), curob, curob, &curob)};
			static_cast<void>(checkcarryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#else
			uint_least64_t curmtmpa{curma};
			curma += curma;
			curoa += static_cast<uint_least16_t>(curoa);
			curoa += curma < curmtmpa;
			uint_least64_t curmtmpb{curmb};
			curmb += curmb;
			curob += static_cast<uint_least16_t>(curob);
			curob += curmb < curmtmpb;
#endif
			curea = curoa;
			cureb = curob;
		}
		curpa >>= 16 - 1;
		curpb >>= 16 - 1;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		uint_least64_t curqa{static_cast<uint_least64_t>(curpa)};// sign-extend
		uint_least64_t curqb{static_cast<uint_least64_t>(curpb)};
#else
		uint_least32_t curqa{static_cast<uint_least32_t>(curpa)};// sign-extend
		uint_least32_t curqb{static_cast<uint_least32_t>(curpb)};
#endif
		if constexpr(isfltpmode){
			curea >>= 1;
			cureb >>= 1;
			*reinterpret_cast<uint_least64_t *>(outa) = curma;
			*reinterpret_cast<uint_least64_t *>(dsta) = curma;
			*reinterpret_cast<uint_least64_t *>(outb) = curmb;
			*reinterpret_cast<uint_least64_t *>(dstb) = curmb;
		}
		if constexpr(issignmode){
			if constexpr(!isfltpmode){
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outa) + 1) = static_cast<W>(curea);
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(dsta) + 1) = static_cast<W>(curea);
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outb) + 1) = static_cast<W>(cureb);
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(dstb) + 1) = static_cast<W>(cureb);
				*reinterpret_cast<uint_least64_t *>(outa) = curma;
				*reinterpret_cast<uint_least64_t *>(dsta) = curma;
				*reinterpret_cast<uint_least64_t *>(outb) = curmb;
				*reinterpret_cast<uint_least64_t *>(dstb) = curmb;
			}
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			curma = __builtin_addcll(curma, curqa, 0, &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			curma = __builtin_addcl(curma, curqa, 0, &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymida, carrya;
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			curmloa = __builtin_addcl(curmloa, curqa, 0, &carrymida);
			curmhia = __builtin_addcl(curmhia, curqa, carrymida, &carrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
#endif
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			curmb = __builtin_addcll(curmb, curqb, 0, &carryb);
#else
			unsigned long carryb;
			curmb = __builtin_addcl(curmb, curqb, 0, &carryb);
#endif
#else
			unsigned long carrymidb, carryb;
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			curmlob = __builtin_addcl(curmlob, curqb, 0, &carrymidb);
			curmhib = __builtin_addcl(curmhib, curqb, carrymidb, &carryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, static_cast<unsigned short>(curqb), static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#elif defined(_M_X64)
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(0, curma, curqa, &curma), curoa, static_cast<uint_least16_t>(curqa), &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(0, curmb, curqb, &curmb), curob, static_cast<uint_least16_t>(curqb), &curob)};
			static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmloa, curqa, &curmloa), curmhia, curqa, &curmhia), curoa, static_cast<uint_least16_t>(curqa), &curoa)};
			static_cast<void>(checkcarrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(_addcarry_u32(0, curmlob, curqb, &curmlob), curmhib, curqb, &curmhib), curob, static_cast<uint_least16_t>(curqb), &curob)};
			static_cast<void>(checkcarryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#elif 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
			uint_least64_t curmtmpa{curma};
			curma += curqa;
			curoa += static_cast<uint_least16_t>(curqa);
			curoa += curma < curmtmpa || curma < curqa;
			uint_least64_t curmtmpb{curmb};
			curmb += curqb;
			curob += static_cast<uint_least16_t>(curqb);
			curob += curmb < curmtmpb || curmb < curqb;
#else
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			uint_least32_t curmlotmpa{curmloa}, curmhitmpa{curmhia};
			curmloa += curqa;
			curmhia += curqa;
			curmhia += curmloa < curmlotmpa || curmloa < curqa;
			curoa += static_cast<uint_least16_t>(curqa);
			curoa += curmhia < curmhitmpa || curmhia < curqa;
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			uint_least32_t curmlotmpb{curmlob}, curmhitmpb{curmhib};
			curmlob += curqb;
			curmhib += curqb;
			curmhib += curmlob < curmlotmpb || curmlob < curqb;
			curob += static_cast<uint_least16_t>(curqb);
			curob += curmhib < curmhitmpb || curmhib < curqb;
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
			curea = curoa;
			cureb = curob;
		}
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
		curma ^= curqa;
		curmb ^= curqb;
#else
		uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
		curmloa ^= curqa;
		curmhia ^= curqa;
		alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
		curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
		uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
		curmlob ^= curqb;
		curmhib ^= curqb;
		alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
		curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
		curea ^= static_cast<U>(curqa);
		cureb ^= static_cast<U>(curqb);
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outa) + 1) = static_cast<W>(curea);
		*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(dsta) + 1) = static_cast<W>(curea);
		*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(outb) + 1) = static_cast<W>(cureb);
		*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(dstb) + 1) = static_cast<W>(cureb);
		if constexpr(issignmode){
			curea &= ~static_cast<uint_least16_t>(0) >> 1;
			cureb &= ~static_cast<uint_least16_t>(0) >> 1;
			*reinterpret_cast<uint_least64_t *>(outa) = curma;
			*reinterpret_cast<uint_least64_t *>(dsta) = curma;
			*reinterpret_cast<uint_least64_t *>(outb) = curmb;
			*reinterpret_cast<uint_least64_t *>(dstb) = curmb;
		}else{
			uint_least16_t curoa{static_cast<uint_least16_t>(curea)};
			uint_least16_t curob{static_cast<uint_least16_t>(cureb)};
#if (defined(__GNUC__) || defined(__clang__) || defined(__xlC__) && (defined(__VEC__) || defined(__ALTIVEC__))) && defined(__has_builtin) && __has_builtin(__builtin_addcl) && __has_builtin(__builtin_addcs)
			static_assert(16 == CHAR_BIT * sizeof(short), "unexpected size of type short");
			unsigned short carrysigna;
			curoa = __builtin_addcs(curoa, curoa, 0, &carrysigna);
			*reinterpret_cast<uint_least64_t *>(outa) = curma;
			*reinterpret_cast<uint_least64_t *>(dsta) = curma;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			static_assert(64 == CHAR_BIT * sizeof(long long), "unexpected size of type long long");
			unsigned long long carrya;
			curma = __builtin_addcll(curma, curma, static_cast<unsigned long long>(carrysigna), &carrya);
#else
			static_assert(64 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrya;
			curma = __builtin_addcl(curma, curma, static_cast<unsigned long>(carrysigna), &carrya);
#endif
#else
			static_assert(32 == CHAR_BIT * sizeof(long), "unexpected size of type long");
			unsigned long carrymida, carrya;
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			curmloa = __builtin_addcl(curmloa, curmloa, static_cast<unsigned long>(carrysigna), &carrymida);
			curmhia = __builtin_addcl(curmhia, curmhia, carrymida, &carrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
#endif
			unsigned short checkcarrya;
			curoa = __builtin_addcs(curoa, 0, static_cast<unsigned short>(carrya), &checkcarrya);
			static_cast<void>(checkcarrya);
			unsigned short carrysignb;
			curob = __builtin_addcs(curob, curob, 0, &carrysignb);
			*reinterpret_cast<uint_least64_t *>(outb) = curmb;
			*reinterpret_cast<uint_least64_t *>(dstb) = curmb;
#if 0xFFFFFFFFFFFFFFFFu <= UINTPTR_MAX
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
			unsigned long long carryb;
			curmb = __builtin_addcll(curmb, curmb, static_cast<unsigned long long>(carrysignb), &carryb);
#else
			unsigned long carryb;
			curmb = __builtin_addcl(curmb, curmb, static_cast<unsigned long>(carrysignb), &carryb);
#endif
#else
			unsigned long carrymidb, carryb;
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			curmlob = __builtin_addcl(curmlob, curmlob, static_cast<unsigned long>(carrysignb), &carrymidb);
			curmhib = __builtin_addcl(curmhib, curmhib, carrymidb, &carryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#endif
			unsigned short checkcarryb;
			curob = __builtin_addcs(curob, 0, static_cast<unsigned short>(carryb), &checkcarryb);
			static_cast<void>(checkcarryb);
#elif defined(_M_X64)
			unsigned char carrysigna{_addcarry_u16(0, curoa, curoa, &curoa)};
			*reinterpret_cast<uint_least64_t *>(outa) = curma;
			*reinterpret_cast<uint_least64_t *>(dsta) = curma;
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u64(carrysigna, curma, curma, &curma), curoa, 0, &curoa)};
			static_cast<void>(checkcarrya);
			unsigned char carrysignb{_addcarry_u16(0, curob, curob, &curob)};
			*reinterpret_cast<uint_least64_t *>(outb) = curmb;
			*reinterpret_cast<uint_least64_t *>(dstb) = curmb;
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u64(carrysignb, curmb, curmb, &curmb), curob, 0, &curob)};
			static_cast<void>(checkcarryb);
#elif defined(_M_IX86)
			uint_least32_t curmloa{static_cast<uint_least32_t>(curma & 0xFFFFFFFFu)}, curmhia{static_cast<uint_least32_t>(curma >> 32)};// decompose
			unsigned char carrysigna{_addcarry_u16(0, curoa, curoa, &curoa)};
			*reinterpret_cast<uint_least64_t *>(outa) = curma;
			*reinterpret_cast<uint_least64_t *>(dsta) = curma;
			unsigned char checkcarrya{_addcarry_u16(_addcarry_u32(_addcarry_u32(carrysigna, curmloa, curmloa, &curmloa), curmhia, curmhia, &curmhia), curoa, 0, &curoa)};
			static_cast<void>(checkcarrya);
			alignas(8) uint_least32_t acurma[2]{curmloa, curmhia};
			curma = *reinterpret_cast<uint_least64_t *>(acurma);// recompose
			uint_least32_t curmlob{static_cast<uint_least32_t>(curmb & 0xFFFFFFFFu)}, curmhib{static_cast<uint_least32_t>(curmb >> 32)};// decompose
			unsigned char carrysignb{_addcarry_u16(0, curob, curob, &curob)};
			*reinterpret_cast<uint_least64_t *>(outb) = curmb;
			*reinterpret_cast<uint_least64_t *>(dstb) = curmb;
			unsigned char checkcarryb{_addcarry_u16(_addcarry_u32(_addcarry_u32(carrysignb, curmlob, curmlob, &curmlob), curmhib, curmhib, &curmhib), curob, 0, &curob)};
			static_cast<void>(checkcarryb);
			alignas(8) uint_least32_t acurmb[2]{curmlob, curmhib};
			curmb = *reinterpret_cast<uint_least64_t *>(acurmb);// recompose
#else
			uint_least16_t curotmpa{curoa};
			curoa += curoa;
			*reinterpret_cast<uint_least64_t *>(outa) = curma;
			*reinterpret_cast<uint_least64_t *>(dsta) = curma;
			uint_least64_t curmtmpa{curma};
			curma += curma;
			curma += curoa < curotmpa;
			curoa += curma < curmtmpa;
			uint_least16_t curotmpb{curob};
			curob += curob;
			*reinterpret_cast<uint_least64_t *>(outb) = curmb;
			*reinterpret_cast<uint_least64_t *>(dstb) = curmb;
			uint_least64_t curmtmpb{curmb};
			curmb += curmb;
			curmb += curob < curotmpb;
			curob += curmb < curmtmpb;
#endif
			curea = curoa;
			cureb = curob;
		}
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(U &cur)noexcept{
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curp{static_cast<std::make_signed_t<T>>(cur)};
		if constexpr(isfltpmode || !issignmode){
			T curo{static_cast<T>(cur)};
			curo += curo;
			cur = curo;
		}
		curp >>= CHAR_BIT * sizeof(T) - 1;
		U curq{static_cast<T>(curp)};
		if constexpr(isfltpmode) cur >>= 1;
		if constexpr(issignmode){
			T curo{static_cast<T>(cur)};
			curo += static_cast<T>(curq);
			cur = curo;
		}
		cur ^= curq;
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		if constexpr(!issignmode) cur &= ~static_cast<T>(0) >> 1;
		else cur = rotateleftportable<1>(static_cast<T>(cur));
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(U &cur, T *out)noexcept{
	// do not pass a nullptr here
	assert(out);
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curp{static_cast<std::make_signed_t<T>>(cur)};
		*out = cur;
		if constexpr(isfltpmode || !issignmode){
			T curo{static_cast<T>(cur)};
			curo += curo;
			cur = curo;
		}
		curp >>= CHAR_BIT * sizeof(T) - 1;
		U curq{static_cast<T>(curp)};
		if constexpr(isfltpmode) cur >>= 1;
		if constexpr(issignmode){
			T curo{static_cast<T>(cur)};
			curo += static_cast<T>(curq);
			cur = curo;
		}
		cur ^= curq;
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		*out = cur;
		if constexpr(issignmode) cur &= ~static_cast<T>(0) >> 1;
		else cur = rotateleftportable<1>(static_cast<T>(cur));
	}else *out = cur;
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(U &cur, T *out, T *dst)noexcept{
	// do not pass a nullptr here
	assert(out);
	assert(dst);
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curp{static_cast<std::make_signed_t<T>>(cur)};
		*out = cur;
		*dst = cur;
		if constexpr(isfltpmode || !issignmode){
			T curo{static_cast<T>(cur)};
			curo += curo;
			cur = curo;
		}
		curp >>= CHAR_BIT * sizeof(T) - 1;
		U curq{static_cast<T>(curp)};
		if constexpr(isfltpmode) cur >>= 1;
		if constexpr(issignmode){
			T curo{static_cast<T>(cur)};
			curo += static_cast<T>(curq);
			cur = curo;
		}
		cur ^= curq;
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		*out = cur;
		*dst = cur;
		if constexpr(issignmode) cur &= ~static_cast<T>(0) >> 1;
		else cur = rotateleftportable<1>(static_cast<T>(cur));
	}else{
		*out = cur;
		*dst = cur;
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(U &cura, U &curb)noexcept{
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		if constexpr(isfltpmode || !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		if constexpr(isfltpmode || !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		if constexpr(isfltpmode){
			cura >>= 1;
			curb >>= 1;
		}
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			cura = curoa;
			curb = curob;
		}
		cura ^= curqa;
		curb ^= curqb;
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		if constexpr(issignmode){
			cura &= ~static_cast<T>(0) >> 1;
			curb &= ~static_cast<T>(0) >> 1;
		}else{
			cura = rotateleftportable<1>(static_cast<T>(cura));
			curb = rotateleftportable<1>(static_cast<T>(curb));
		}
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(U &cura, T *outa, U &curb, T *outb)noexcept{
	// do not pass a nullptr here
	assert(outa);
	assert(outb);
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		*outa = static_cast<T>(cura);
		if constexpr(isfltpmode || !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		*outb = static_cast<T>(curb);
		if constexpr(isfltpmode || !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		if constexpr(isfltpmode){
			cura >>= 1;
			curb >>= 1;
		}
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			cura = curoa;
			curb = curob;
		}
		cura ^= curqa;
		curb ^= curqb;
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		*outa = static_cast<T>(cura);
		if constexpr(issignmode) cura &= ~static_cast<T>(0) >> 1;
		else cura = rotateleftportable<1>(static_cast<T>(cura));
		*outb = static_cast<T>(curb);
		if constexpr(issignmode) curb &= ~static_cast<T>(0) >> 1;
		else curb = rotateleftportable<1>(static_cast<T>(curb));
	}else{
		*outa = static_cast<T>(cura);
		*outb = static_cast<T>(curb);
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(U &cura, T *outa, T *dsta, U &curb, T *outb, T *dstb)noexcept{
	// do not pass a nullptr here
	assert(outa);
	assert(dsta);
	assert(outb);
	assert(dstb);
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		*outa = static_cast<T>(cura);
		*dsta = static_cast<T>(cura);
		if constexpr(isfltpmode || !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		*outb = static_cast<T>(curb);
		*dstb = static_cast<T>(curb);
		if constexpr(isfltpmode || !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		if constexpr(isfltpmode){
			cura >>= 1;
			curb >>= 1;
		}
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			cura = curoa;
			curb = curob;
		}
		cura ^= curqa;
		curb ^= curqb;
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		*outa = static_cast<T>(cura);
		*dsta = static_cast<T>(cura);
		if constexpr(issignmode) cura &= ~static_cast<T>(0) >> 1;
		else cura = rotateleftportable<1>(static_cast<T>(cura));
		*outb = static_cast<T>(curb);
		*dstb = static_cast<T>(curb);
		if constexpr(issignmode) curb &= ~static_cast<T>(0) >> 1;
		else curb = rotateleftportable<1>(static_cast<T>(curb));
	}else{
		*outa = static_cast<T>(cura);
		*dsta = static_cast<T>(cura);
		*outb = static_cast<T>(curb);
		*dstb = static_cast<T>(curb);
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(U &cura, U &curb, U &curc)noexcept{
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		if constexpr(isfltpmode || !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		if constexpr(isfltpmode || !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		std::make_signed_t<T> curpc{static_cast<std::make_signed_t<T>>(curc)};
		if constexpr(isfltpmode || !issignmode){
			T curoc{static_cast<T>(curc)};
			curoc += curoc;
			curc = curoc;
		}
		curpc >>= CHAR_BIT * sizeof(T) - 1;
		U curqc{static_cast<T>(curpc)};
		if constexpr(isfltpmode){
			cura >>= 1;
			curb >>= 1;
			curc >>= 1;
		}
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			T curoc{static_cast<T>(curc)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			curoc += static_cast<T>(curqc);
			cura = curoa;
			curb = curob;
			curc = curoc;
		}
		cura ^= curqa;
		curb ^= curqb;
		curc ^= curqc;
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		if constexpr(issignmode){
			cura &= ~static_cast<T>(0) >> 1;
			curb &= ~static_cast<T>(0) >> 1;
			curc &= ~static_cast<T>(0) >> 1;
		}else{
			cura = rotateleftportable<1>(static_cast<T>(cura));
			curb = rotateleftportable<1>(static_cast<T>(curb));
			curc = rotateleftportable<1>(static_cast<T>(curc));
		}
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(U &cura, T *outa, U &curb, T *outb, U &curc, T *outc)noexcept{
	// do not pass a nullptr here
	assert(outa);
	assert(outb);
	assert(outc);
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		*outa = static_cast<T>(cura);
		if constexpr(isfltpmode || !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		*outb = static_cast<T>(curb);
		if constexpr(isfltpmode || !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		std::make_signed_t<T> curpc{static_cast<std::make_signed_t<T>>(curc)};
		*outc = static_cast<T>(curc);
		if constexpr(isfltpmode || !issignmode){
			T curoc{static_cast<T>(curc)};
			curoc += curoc;
			curc = curoc;
		}
		curpc >>= CHAR_BIT * sizeof(T) - 1;
		U curqc{static_cast<T>(curpc)};
		if constexpr(isfltpmode){
			cura >>= 1;
			curb >>= 1;
			curc >>= 1;
		}
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			T curoc{static_cast<T>(curc)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			curoc += static_cast<T>(curqc);
			cura = curoa;
			curb = curob;
			curc = curoc;
		}
		cura ^= curqa;
		curb ^= curqb;
		curc ^= curqc;
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		*outa = static_cast<T>(cura);
		if constexpr(issignmode) cura &= ~static_cast<T>(0) >> 1;
		else cura = rotateleftportable<1>(static_cast<T>(cura));
		*outb = static_cast<T>(curb);
		if constexpr(issignmode) curb &= ~static_cast<T>(0) >> 1;
		else curb = rotateleftportable<1>(static_cast<T>(curb));
		*outc = static_cast<T>(curc);
		if constexpr(issignmode) curb &= ~static_cast<T>(0) >> 1;
		else curc = rotateleftportable<1>(static_cast<T>(curc));
	}else{
		*outa = static_cast<T>(cura);
		*outb = static_cast<T>(curb);
		*outc = static_cast<T>(curc);
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(U &cura, T *outa, T *dsta, U &curb, T *outb, T *dstb, U &curc, T *outc, T *dstc)noexcept{
	// do not pass a nullptr here
	assert(outa);
	assert(dsta);
	assert(outb);
	assert(dstb);
	assert(outc);
	assert(dstc);
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		*outa = static_cast<T>(cura);
		*dsta = static_cast<T>(cura);
		if constexpr(isfltpmode || !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		*outb = static_cast<T>(curb);
		*dstb = static_cast<T>(curb);
		if constexpr(isfltpmode || !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		std::make_signed_t<T> curpc{static_cast<std::make_signed_t<T>>(curc)};
		*outc = static_cast<T>(curc);
		*dstc = static_cast<T>(curc);
		if constexpr(isfltpmode || !issignmode){
			T curoc{static_cast<T>(curc)};
			curoc += curoc;
			curc = curoc;
		}
		curpc >>= CHAR_BIT * sizeof(T) - 1;
		U curqc{static_cast<T>(curpc)};
		if constexpr(isfltpmode){
			cura >>= 1;
			curb >>= 1;
			curc >>= 1;
		}
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			T curoc{static_cast<T>(curc)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			curoc += static_cast<T>(curqc);
			cura = curoa;
			curb = curob;
			curc = curoc;
		}
		cura ^= curqa;
		curb ^= curqb;
		curc ^= curqc;
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		*outa = static_cast<T>(cura);
		*dsta = static_cast<T>(cura);
		if constexpr(issignmode) cura &= ~static_cast<T>(0) >> 1;
		else cura = rotateleftportable<1>(static_cast<T>(cura));
		*outb = static_cast<T>(curb);
		*dstb = static_cast<T>(curb);
		if constexpr(issignmode) curb &= ~static_cast<T>(0) >> 1;
		else curb = rotateleftportable<1>(static_cast<T>(curb));
		*outc = static_cast<T>(curc);
		*dstc = static_cast<T>(curc);
		if constexpr(issignmode) curc &= ~static_cast<T>(0) >> 1;
		else curc = rotateleftportable<1>(static_cast<T>(curc));
	}else{
		*outa = static_cast<T>(cura);
		*dsta = static_cast<T>(cura);
		*outb = static_cast<T>(curb);
		*dstb = static_cast<T>(curb);
		*outc = static_cast<T>(curc);
		*dstc = static_cast<T>(curc);
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(U &cura, U &curb, U &curc, U &curd)noexcept{
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		if constexpr(isfltpmode || !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		if constexpr(isfltpmode || !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		std::make_signed_t<T> curpc{static_cast<std::make_signed_t<T>>(curc)};
		if constexpr(isfltpmode || !issignmode){
			T curoc{static_cast<T>(curc)};
			curoc += curoc;
			curc = curoc;
		}
		curpc >>= CHAR_BIT * sizeof(T) - 1;
		U curqc{static_cast<T>(curpc)};
		std::make_signed_t<T> curpd{static_cast<std::make_signed_t<T>>(curd)};
		if constexpr(isfltpmode || !issignmode){
			T curod{static_cast<T>(curd)};
			curod += curod;
			curd = curod;
		}
		curpd >>= CHAR_BIT * sizeof(T) - 1;
		U curqd{static_cast<T>(curpd)};
		if constexpr(isfltpmode){
			cura >>= 1;
			curb >>= 1;
			curc >>= 1;
			curd >>= 1;
		}
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			T curoc{static_cast<T>(curc)};
			T curod{static_cast<T>(curd)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			curoc += static_cast<T>(curqc);
			curod += static_cast<T>(curqd);
			cura = curoa;
			curb = curob;
			curc = curoc;
			curd = curod;
		}
		cura ^= curqa;
		curb ^= curqb;
		curc ^= curqc;
		curd ^= curqd;
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		if constexpr(issignmode){
			cura &= ~static_cast<T>(0) >> 1;
			curb &= ~static_cast<T>(0) >> 1;
			curc &= ~static_cast<T>(0) >> 1;
			curd &= ~static_cast<T>(0) >> 1;
		}else{
			cura = rotateleftportable<1>(static_cast<T>(cura));
			curb = rotateleftportable<1>(static_cast<T>(curb));
			curc = rotateleftportable<1>(static_cast<T>(curc));
			curd = rotateleftportable<1>(static_cast<T>(curd));
		}
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(U &cura, T *outa, U &curb, T *outb, U &curc, T *outc, U &curd, T *outd)noexcept{
	// do not pass a nullptr here
	assert(outa);
	assert(outb);
	assert(outc);
	assert(outd);
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		*outa = static_cast<T>(cura);
		if constexpr(isfltpmode || !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		*outb = static_cast<T>(curb);
		if constexpr(isfltpmode || !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		std::make_signed_t<T> curpc{static_cast<std::make_signed_t<T>>(curc)};
		*outc = static_cast<T>(curc);
		if constexpr(isfltpmode || !issignmode){
			T curoc{static_cast<T>(curc)};
			curoc += curoc;
			curc = curoc;
		}
		curpc >>= CHAR_BIT * sizeof(T) - 1;
		U curqc{static_cast<T>(curpc)};
		std::make_signed_t<T> curpd{static_cast<std::make_signed_t<T>>(curd)};
		*outd = static_cast<T>(curd);
		if constexpr(isfltpmode || !issignmode){
			T curod{static_cast<T>(curd)};
			curod += curod;
			curd = curod;
		}
		curpd >>= CHAR_BIT * sizeof(T) - 1;
		U curqd{static_cast<T>(curpd)};
		if constexpr(isfltpmode){
			cura >>= 1;
			curb >>= 1;
			curc >>= 1;
			curd >>= 1;
		}
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			T curoc{static_cast<T>(curc)};
			T curod{static_cast<T>(curd)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			curoc += static_cast<T>(curqc);
			curod += static_cast<T>(curqd);
			cura = curoa;
			curb = curob;
			curc = curoc;
			curd = curod;
		}
		cura ^= curqa;
		curb ^= curqb;
		curc ^= curqc;
		curd ^= curqd;
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		*outa = static_cast<T>(cura);
		if constexpr(issignmode) cura &= ~static_cast<T>(0) >> 1;
		else cura = rotateleftportable<1>(static_cast<T>(cura));
		*outb = static_cast<T>(curb);
		if constexpr(issignmode) curb &= ~static_cast<T>(0) >> 1;
		else curb = rotateleftportable<1>(static_cast<T>(curb));
		*outc = static_cast<T>(curc);
		if constexpr(issignmode) curc &= ~static_cast<T>(0) >> 1;
		else curc = rotateleftportable<1>(static_cast<T>(curc));
		*outd = static_cast<T>(curd);
		if constexpr(issignmode) curd &= ~static_cast<T>(0) >> 1;
		else curd = rotateleftportable<1>(static_cast<T>(curd));
	}else{
		*outa = static_cast<T>(cura);
		*outb = static_cast<T>(curb);
		*outc = static_cast<T>(curc);
		*outd = static_cast<T>(curd);
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(U &cura, T *outa, T *dsta, U &curb, T *outb, T *dstb, U &curc, T *outc, T *dstc, U &curd, T *outd, T *dstd)noexcept{
	// do not pass a nullptr here
	assert(outa);
	assert(dsta);
	assert(outb);
	assert(dstb);
	assert(outc);
	assert(dstc);
	assert(outd);
	assert(dstd);
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		*outa = static_cast<T>(cura);
		*dsta = static_cast<T>(cura);
		if constexpr(isfltpmode || !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		*outb = static_cast<T>(curb);
		*dstb = static_cast<T>(curb);
		if constexpr(isfltpmode || !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		std::make_signed_t<T> curpc{static_cast<std::make_signed_t<T>>(curc)};
		*outc = static_cast<T>(curc);
		*dstc = static_cast<T>(curc);
		if constexpr(isfltpmode || !issignmode){
			T curoc{static_cast<T>(curc)};
			curoc += curoc;
			curc = curoc;
		}
		curpc >>= CHAR_BIT * sizeof(T) - 1;
		U curqc{static_cast<T>(curpc)};
		std::make_signed_t<T> curpd{static_cast<std::make_signed_t<T>>(curd)};
		*outd = static_cast<T>(curd);
		*dstd = static_cast<T>(curd);
		if constexpr(isfltpmode || !issignmode){
			T curod{static_cast<T>(curd)};
			curod += curod;
			curd = curod;
		}
		curpd >>= CHAR_BIT * sizeof(T) - 1;
		U curqd{static_cast<T>(curpd)};
		if constexpr(isfltpmode){
			cura >>= 1;
			curb >>= 1;
			curc >>= 1;
			curd >>= 1;
		}
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			T curoc{static_cast<T>(curc)};
			T curod{static_cast<T>(curd)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			curoc += static_cast<T>(curqc);
			curod += static_cast<T>(curqd);
			cura = curoa;
			curb = curob;
			curc = curoc;
			curd = curod;
		}
		cura ^= curqa;
		curb ^= curqb;
		curc ^= curqc;
		curd ^= curqd;
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		*outa = static_cast<T>(cura);
		*dsta = static_cast<T>(cura);
		if constexpr(issignmode) cura &= ~static_cast<T>(0) >> 1;
		else cura = rotateleftportable<1>(static_cast<T>(cura));
		*outb = static_cast<T>(curb);
		*dstb = static_cast<T>(curb);
		if constexpr(issignmode) curb &= ~static_cast<T>(0) >> 1;
		else curb = rotateleftportable<1>(static_cast<T>(curb));
		*outc = static_cast<T>(curc);
		*dstc = static_cast<T>(curc);
		if constexpr(issignmode) curc &= ~static_cast<T>(0) >> 1;
		else curc = rotateleftportable<1>(static_cast<T>(curc));
		*outd = static_cast<T>(curd);
		*dstd = static_cast<T>(curd);
		if constexpr(issignmode) curd &= ~static_cast<T>(0) >> 1;
		else curd = rotateleftportable<1>(static_cast<T>(curd));
	}else{
		*outa = static_cast<T>(cura);
		*dsta = static_cast<T>(cura);
		*outb = static_cast<T>(curb);
		*dstb = static_cast<T>(curb);
		*outc = static_cast<T>(curc);
		*dstc = static_cast<T>(curc);
		*outd = static_cast<T>(curd);
		*dstd = static_cast<T>(curd);
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(U &cura, U &curb, U &curc, U &curd, U &cure, U &curf, U &curg, U &curh)noexcept{
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		if constexpr(isfltpmode || !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		if constexpr(isfltpmode || !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		std::make_signed_t<T> curpc{static_cast<std::make_signed_t<T>>(curc)};
		if constexpr(isfltpmode || !issignmode){
			T curoc{static_cast<T>(curc)};
			curoc += curoc;
			curc = curoc;
		}
		curpc >>= CHAR_BIT * sizeof(T) - 1;
		U curqc{static_cast<T>(curpc)};
		std::make_signed_t<T> curpd{static_cast<std::make_signed_t<T>>(curd)};
		if constexpr(isfltpmode || !issignmode){
			T curod{static_cast<T>(curd)};
			curod += curod;
			curd = curod;
		}
		curpd >>= CHAR_BIT * sizeof(T) - 1;
		U curqd{static_cast<T>(curpd)};
		std::make_signed_t<T> curpe{static_cast<std::make_signed_t<T>>(cure)};
		if constexpr(isfltpmode || !issignmode){
			T curoe{static_cast<T>(cure)};
			curoe += curoe;
			cure = curoe;
		}
		curpe >>= CHAR_BIT * sizeof(T) - 1;
		U curqe{static_cast<T>(curpe)};
		std::make_signed_t<T> curpf{static_cast<std::make_signed_t<T>>(curf)};
		if constexpr(isfltpmode || !issignmode){
			T curof{static_cast<T>(curf)};
			curof += curof;
			curf = curof;
		}
		curpf >>= CHAR_BIT * sizeof(T) - 1;
		U curqf{static_cast<T>(curpf)};
		std::make_signed_t<T> curpg{static_cast<std::make_signed_t<T>>(curg)};
		if constexpr(isfltpmode || !issignmode){
			T curog{static_cast<T>(curg)};
			curog += curog;
			curg = curog;
		}
		curpg >>= CHAR_BIT * sizeof(T) - 1;
		U curqg{static_cast<T>(curpg)};
		std::make_signed_t<T> curph{static_cast<std::make_signed_t<T>>(curh)};
		if constexpr(isfltpmode || !issignmode){
			T curoh{static_cast<T>(curh)};
			curoh += curoh;
			curh = curoh;
		}
		curph >>= CHAR_BIT * sizeof(T) - 1;
		U curqh{static_cast<T>(curph)};
		if constexpr(isfltpmode){
			cura >>= 1;
			curb >>= 1;
			curc >>= 1;
			curd >>= 1;
			cure >>= 1;
			curf >>= 1;
			curg >>= 1;
			curh >>= 1;
		}
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			T curoc{static_cast<T>(curc)};
			T curod{static_cast<T>(curd)};
			T curoe{static_cast<T>(cure)};
			T curof{static_cast<T>(curf)};
			T curog{static_cast<T>(curg)};
			T curoh{static_cast<T>(curh)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			curoc += static_cast<T>(curqc);
			curod += static_cast<T>(curqd);
			curoe += static_cast<T>(curqe);
			curof += static_cast<T>(curqf);
			curog += static_cast<T>(curqg);
			curoh += static_cast<T>(curqh);
			cura = curoa;
			curb = curob;
			curc = curoc;
			curd = curod;
			cure = curoe;
			curf = curof;
			curg = curog;
			curh = curoh;
		}
		cura ^= curqa;
		curb ^= curqb;
		curc ^= curqc;
		curd ^= curqd;
		cure ^= curqe;
		curf ^= curqf;
		curg ^= curqg;
		curh ^= curqh;
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		if constexpr(issignmode){
			cura &= ~static_cast<T>(0) >> 1;
			curb &= ~static_cast<T>(0) >> 1;
			curc &= ~static_cast<T>(0) >> 1;
			curd &= ~static_cast<T>(0) >> 1;
			cure &= ~static_cast<T>(0) >> 1;
			curf &= ~static_cast<T>(0) >> 1;
			curg &= ~static_cast<T>(0) >> 1;
			curh &= ~static_cast<T>(0) >> 1;
		}else{
			cura = rotateleftportable<1>(static_cast<T>(cura));
			curb = rotateleftportable<1>(static_cast<T>(curb));
			curc = rotateleftportable<1>(static_cast<T>(curc));
			curd = rotateleftportable<1>(static_cast<T>(curd));
			cure = rotateleftportable<1>(static_cast<T>(cure));
			curf = rotateleftportable<1>(static_cast<T>(curf));
			curg = rotateleftportable<1>(static_cast<T>(curg));
			curh = rotateleftportable<1>(static_cast<T>(curh));
		}
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(U &cura, T *outa, U &curb, T *outb, U &curc, T *outc, U &curd, T *outd, U &cure, T *oute, U &curf, T *outf, U &curg, T *outg, U &curh, T *outh)noexcept{
	// do not pass a nullptr here
	assert(outa);
	assert(outb);
	assert(outc);
	assert(outd);
	assert(oute);
	assert(outf);
	assert(outg);
	assert(outh);
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		*outa = static_cast<T>(cura);
		if constexpr(isfltpmode || !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		*outb = static_cast<T>(curb);
		if constexpr(isfltpmode || !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		std::make_signed_t<T> curpc{static_cast<std::make_signed_t<T>>(curc)};
		*outc = static_cast<T>(curc);
		if constexpr(isfltpmode || !issignmode){
			T curoc{static_cast<T>(curc)};
			curoc += curoc;
			curc = curoc;
		}
		curpc >>= CHAR_BIT * sizeof(T) - 1;
		U curqc{static_cast<T>(curpc)};
		std::make_signed_t<T> curpd{static_cast<std::make_signed_t<T>>(curd)};
		*outd = static_cast<T>(curd);
		if constexpr(isfltpmode || !issignmode){
			T curod{static_cast<T>(curd)};
			curod += curod;
			curd = curod;
		}
		curpd >>= CHAR_BIT * sizeof(T) - 1;
		U curqd{static_cast<T>(curpd)};
		std::make_signed_t<T> curpe{static_cast<std::make_signed_t<T>>(cure)};
		*oute = static_cast<T>(cure);
		if constexpr(isfltpmode || !issignmode){
			T curoe{static_cast<T>(cure)};
			curoe += curoe;
			cure = curoe;
		}
		curpe >>= CHAR_BIT * sizeof(T) - 1;
		U curqe{static_cast<T>(curpe)};
		std::make_signed_t<T> curpf{static_cast<std::make_signed_t<T>>(curf)};
		*outf = static_cast<T>(curf);
		if constexpr(isfltpmode || !issignmode){
			T curof{static_cast<T>(curf)};
			curof += curof;
			curf = curof;
		}
		curpf >>= CHAR_BIT * sizeof(T) - 1;
		U curqf{static_cast<T>(curpf)};
		std::make_signed_t<T> curpg{static_cast<std::make_signed_t<T>>(curg)};
		*outg = static_cast<T>(curg);
		if constexpr(isfltpmode || !issignmode){
			T curog{static_cast<T>(curg)};
			curog += curog;
			curg = curog;
		}
		curpg >>= CHAR_BIT * sizeof(T) - 1;
		U curqg{static_cast<T>(curpg)};
		std::make_signed_t<T> curph{static_cast<std::make_signed_t<T>>(curh)};
		*outh = static_cast<T>(curh);
		if constexpr(isfltpmode || !issignmode){
			T curoh{static_cast<T>(curh)};
			curoh += curoh;
			curh = curoh;
		}
		curph >>= CHAR_BIT * sizeof(T) - 1;
		U curqh{static_cast<T>(curph)};
		if constexpr(isfltpmode){
			cura >>= 1;
			curb >>= 1;
			curc >>= 1;
			curd >>= 1;
			cure >>= 1;
			curf >>= 1;
			curg >>= 1;
			curh >>= 1;
		}
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			T curoc{static_cast<T>(curc)};
			T curod{static_cast<T>(curd)};
			T curoe{static_cast<T>(cure)};
			T curof{static_cast<T>(curf)};
			T curog{static_cast<T>(curg)};
			T curoh{static_cast<T>(curh)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			curoc += static_cast<T>(curqc);
			curod += static_cast<T>(curqd);
			curoe += static_cast<T>(curqe);
			curof += static_cast<T>(curqf);
			curog += static_cast<T>(curqg);
			curoh += static_cast<T>(curqh);
			cura = curoa;
			curb = curob;
			curc = curoc;
			curd = curod;
			cure = curoe;
			curf = curof;
			curg = curog;
			curh = curoh;
		}
		cura ^= curqa;
		curb ^= curqb;
		curc ^= curqc;
		curd ^= curqd;
		cure ^= curqe;
		curf ^= curqf;
		curg ^= curqg;
		curh ^= curqh;
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		*outa = static_cast<T>(cura);
		if constexpr(issignmode) cura &= ~static_cast<T>(0) >> 1;
		else cura = rotateleftportable<1>(static_cast<T>(cura));
		*outb = static_cast<T>(curb);
		if constexpr(issignmode) curb &= ~static_cast<T>(0) >> 1;
		else curb = rotateleftportable<1>(static_cast<T>(curb));
		*outc = static_cast<T>(curc);
		if constexpr(issignmode) curc &= ~static_cast<T>(0) >> 1;
		else curc = rotateleftportable<1>(static_cast<T>(curc));
		*outd = static_cast<T>(curd);
		if constexpr(issignmode) curd &= ~static_cast<T>(0) >> 1;
		else curd = rotateleftportable<1>(static_cast<T>(curd));
		*oute = static_cast<T>(cure);
		if constexpr(issignmode) cure &= ~static_cast<T>(0) >> 1;
		else cure = rotateleftportable<1>(static_cast<T>(cure));
		*outf = static_cast<T>(curf);
		if constexpr(issignmode) curf &= ~static_cast<T>(0) >> 1;
		else curf = rotateleftportable<1>(static_cast<T>(curf));
		*outg = static_cast<T>(curg);
		if constexpr(issignmode) curg &= ~static_cast<T>(0) >> 1;
		else curg = rotateleftportable<1>(static_cast<T>(curg));
		*outh = static_cast<T>(curh);
		if constexpr(issignmode) curh &= ~static_cast<T>(0) >> 1;
		else curh = rotateleftportable<1>(static_cast<T>(curh));
	}else{
		*outa = static_cast<T>(cura);
		*outb = static_cast<T>(curb);
		*outc = static_cast<T>(curc);
		*outd = static_cast<T>(curd);
		*oute = static_cast<T>(cure);
		*outf = static_cast<T>(curf);
		*outg = static_cast<T>(curg);
		*outh = static_cast<T>(curh);
	}
}

template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T, typename U>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	std::is_unsigned_v<U> &&
	64 >= CHAR_BIT * sizeof(U),
	void> filterinput(U &cura, T *outa, T *dsta, U &curb, T *outb, T *dstb, U &curc, T *outc, T *dstc, U &curd, T *outd, T *dstd, U &cure, T *oute, T *dste, U &curf, T *outf, T *dstf, U &curg, T *outg, T *dstg, U &curh, T *outh, T *dsth)noexcept{
	// do not pass a nullptr here
	assert(outa);
	assert(dsta);
	assert(outb);
	assert(dstb);
	assert(outc);
	assert(dstc);
	assert(outd);
	assert(dstd);
	assert(oute);
	assert(dste);
	assert(outf);
	assert(dstf);
	assert(outg);
	assert(dstg);
	assert(outh);
	assert(dsth);
	if constexpr(isfltpmode != isabsvalue){// two-register filtering
		std::make_signed_t<T> curpa{static_cast<std::make_signed_t<T>>(cura)};
		*outa = static_cast<T>(cura);
		*dsta = static_cast<T>(cura);
		if constexpr(isfltpmode || !issignmode){
			T curoa{static_cast<T>(cura)};
			curoa += curoa;
			cura = curoa;
		}
		curpa >>= CHAR_BIT * sizeof(T) - 1;
		U curqa{static_cast<T>(curpa)};
		std::make_signed_t<T> curpb{static_cast<std::make_signed_t<T>>(curb)};
		*outb = static_cast<T>(curb);
		*dstb = static_cast<T>(curb);
		if constexpr(isfltpmode || !issignmode){
			T curob{static_cast<T>(curb)};
			curob += curob;
			curb = curob;
		}
		curpb >>= CHAR_BIT * sizeof(T) - 1;
		U curqb{static_cast<T>(curpb)};
		std::make_signed_t<T> curpc{static_cast<std::make_signed_t<T>>(curc)};
		*outc = static_cast<T>(curc);
		*dstc = static_cast<T>(curc);
		if constexpr(isfltpmode || !issignmode){
			T curoc{static_cast<T>(curc)};
			curoc += curoc;
			curc = curoc;
		}
		curpc >>= CHAR_BIT * sizeof(T) - 1;
		U curqc{static_cast<T>(curpc)};
		std::make_signed_t<T> curpd{static_cast<std::make_signed_t<T>>(curd)};
		*outd = static_cast<T>(curd);
		*dstd = static_cast<T>(curd);
		if constexpr(isfltpmode || !issignmode){
			T curod{static_cast<T>(curd)};
			curod += curod;
			curd = curod;
		}
		curpd >>= CHAR_BIT * sizeof(T) - 1;
		U curqd{static_cast<T>(curpd)};
		std::make_signed_t<T> curpe{static_cast<std::make_signed_t<T>>(cure)};
		*oute = static_cast<T>(cure);
		*dste = static_cast<T>(cure);
		if constexpr(isfltpmode || !issignmode){
			T curoe{static_cast<T>(cure)};
			curoe += curoe;
			cure = curoe;
		}
		curpe >>= CHAR_BIT * sizeof(T) - 1;
		U curqe{static_cast<T>(curpe)};
		std::make_signed_t<T> curpf{static_cast<std::make_signed_t<T>>(curf)};
		*outf = static_cast<T>(curf);
		*dstf = static_cast<T>(curf);
		if constexpr(isfltpmode || !issignmode){
			T curof{static_cast<T>(curf)};
			curof += curof;
			curf = curof;
		}
		curpf >>= CHAR_BIT * sizeof(T) - 1;
		U curqf{static_cast<T>(curpf)};
		std::make_signed_t<T> curpg{static_cast<std::make_signed_t<T>>(curg)};
		*outg = static_cast<T>(curg);
		*dstg = static_cast<T>(curg);
		if constexpr(isfltpmode || !issignmode){
			T curog{static_cast<T>(curg)};
			curog += curog;
			curg = curog;
		}
		curpg >>= CHAR_BIT * sizeof(T) - 1;
		U curqg{static_cast<T>(curpg)};
		std::make_signed_t<T> curph{static_cast<std::make_signed_t<T>>(curh)};
		*outh = static_cast<T>(curh);
		*dsth = static_cast<T>(curh);
		if constexpr(isfltpmode || !issignmode){
			T curoh{static_cast<T>(curh)};
			curoh += curoh;
			curh = curoh;
		}
		curph >>= CHAR_BIT * sizeof(T) - 1;
		U curqh{static_cast<T>(curph)};
		if constexpr(isfltpmode){
			cura >>= 1;
			curb >>= 1;
			curc >>= 1;
			curd >>= 1;
			cure >>= 1;
			curf >>= 1;
			curg >>= 1;
			curh >>= 1;
		}
		if constexpr(issignmode){
			T curoa{static_cast<T>(cura)};
			T curob{static_cast<T>(curb)};
			T curoc{static_cast<T>(curc)};
			T curod{static_cast<T>(curd)};
			T curoe{static_cast<T>(cure)};
			T curof{static_cast<T>(curf)};
			T curog{static_cast<T>(curg)};
			T curoh{static_cast<T>(curh)};
			curoa += static_cast<T>(curqa);
			curob += static_cast<T>(curqb);
			curoc += static_cast<T>(curqc);
			curod += static_cast<T>(curqd);
			curoe += static_cast<T>(curqe);
			curof += static_cast<T>(curqf);
			curog += static_cast<T>(curqg);
			curoh += static_cast<T>(curqh);
			cura = curoa;
			curb = curob;
			curc = curoc;
			curd = curod;
			cure = curoe;
			curf = curof;
			curg = curog;
			curh = curoh;
		}
		cura ^= curqa;
		curb ^= curqb;
		curc ^= curqc;
		curd ^= curqd;
		cure ^= curqe;
		curf ^= curqf;
		curg ^= curqg;
		curh ^= curqh;
	}else if constexpr(isfltpmode && isabsvalue){// one-register filtering
		*outa = static_cast<T>(cura);
		*dsta = static_cast<T>(cura);
		if constexpr(issignmode) cura &= ~static_cast<T>(0) >> 1;
		else cura = rotateleftportable<1>(static_cast<T>(cura));
		*outb = static_cast<T>(curb);
		*dstb = static_cast<T>(curb);
		if constexpr(issignmode) curb &= ~static_cast<T>(0) >> 1;
		else curb = rotateleftportable<1>(static_cast<T>(curb));
		*outc = static_cast<T>(curc);
		*dstc = static_cast<T>(curc);
		if constexpr(issignmode) curc &= ~static_cast<T>(0) >> 1;
		else curc = rotateleftportable<1>(static_cast<T>(curc));
		*outd = static_cast<T>(curd);
		*dstd = static_cast<T>(curd);
		if constexpr(issignmode) curd &= ~static_cast<T>(0) >> 1;
		else curd = rotateleftportable<1>(static_cast<T>(curd));
		*oute = static_cast<T>(cure);
		*dste = static_cast<T>(cure);
		if constexpr(issignmode) cure &= ~static_cast<T>(0) >> 1;
		else cure = rotateleftportable<1>(static_cast<T>(cure));
		*outf = static_cast<T>(curf);
		*dstf = static_cast<T>(curf);
		if constexpr(issignmode) curf &= ~static_cast<T>(0) >> 1;
		else curf = rotateleftportable<1>(static_cast<T>(curf));
		*outg = static_cast<T>(curg);
		*dstg = static_cast<T>(curg);
		if constexpr(issignmode) curg &= ~static_cast<T>(0) >> 1;
		else curg = rotateleftportable<1>(static_cast<T>(curg));
		*outh = static_cast<T>(curh);
		*dsth = static_cast<T>(curh);
		if constexpr(issignmode) curh &= ~static_cast<T>(0) >> 1;
		else curh = rotateleftportable<1>(static_cast<T>(curh));
	}else{
		*outa = static_cast<T>(cura);
		*dsta = static_cast<T>(cura);
		*outb = static_cast<T>(curb);
		*dstb = static_cast<T>(curb);
		*outc = static_cast<T>(curc);
		*dstc = static_cast<T>(curc);
		*outd = static_cast<T>(curd);
		*dstd = static_cast<T>(curd);
		*oute = static_cast<T>(cure);
		*dste = static_cast<T>(cure);
		*outf = static_cast<T>(curf);
		*dstf = static_cast<T>(curf);
		*outg = static_cast<T>(curg);
		*dstg = static_cast<T>(curg);
		*outh = static_cast<T>(curh);
		*dsth = static_cast<T>(curh);
	}
}

// Helper functions to implement the offset transforms

// version for the companion thread
template<bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, bool ismultistage = false>
RSBD8_FUNC_INLINE unsigned generateoffsetssinglemtc(size_t count, size_t offsets[], size_t offsetscompanion[])noexcept{
	// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
	// isdescsort is frequently optimised away in this part, e.g.: isdescsort * 2 - 1 generates 1 or -1
	// Determining the starting point depends on several factors here.
	static size_t constexpr offsetsstride{8 * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode) - (!ismultistage && !isabsvalue && issignmode && isfltpmode)};// shrink the offsets size if possible
	// do not pass a nullptr here
	assert(offsets);
	assert(offsetscompanion);
	size_t *t{offsets + (offsetsstride - 1)// high-to-low or low-to-high
		- (issignmode && !isabsvalue) * ((offsetsstride + isfltpmode) / 2 - isdescsort)
		- (isdescsort && (!issignmode || isabsvalue)) * (offsetsstride - 1)
		- (isfltpmode && !issignmode && isabsvalue) * (1 - isdescsort * 2)};
	size_t *u{offsetscompanion + (offsetsstride - 1)// high-to-low or low-to-high
		- (issignmode && !isabsvalue) * ((offsetsstride + isfltpmode) / 2 - isdescsort)
		- (isdescsort && (!issignmode || isabsvalue)) * (offsetsstride - 1)
		- (isfltpmode && !issignmode && isabsvalue) * (1 - isdescsort * 2)};
	if constexpr(isrevorder) std::swap(t, u);
	unsigned b;// return value, indicates if a carry-out has occurred and all inputs are valued the same
	size_t offset{count - (*u + *t)};
	*u = count;// high half, the last offset always starts at the end
	if constexpr(!isabsvalue && issignmode){// handle the sign bit, virtually offset the top part by half the range here
		u += isdescsort * 2 - 1;
		t += isdescsort * 2 - 1;
		unsigned j{256 / 2 - 1 - (!ismultistage && isfltpmode && isdescsort)};// the regular floating-point mode has one less iteration on the signed half (-0. elimination)
		b = count < offset;// carry-out can only happen once per cycle here, so optimise that
		do{
			size_t difference{*u + *t};
			*u = offset;// high half
			t[1 - isdescsort * 2] = offset + 1;// low half
			u += isdescsort * 2 - 1;
			t += isdescsort * 2 - 1;
			offset -= difference;
			addcarryofless(b, count, difference);
		}while(--j);
		t[1 - isdescsort * 2] = offset + 1;// low half
	}else{// unsigned or signed absolute
		if constexpr(isfltpmode && !issignmode && isabsvalue){// starts at one removed from the initial index
			// custom loop for the special mode: absolute floating-point, but negative inputs will sort just below their positive counterparts
			u += 1 - isdescsort * 2;// step back
			t += 1 - isdescsort * 2;
			unsigned j{256 / 4 - 1};// double the number of items per loop
			b = count < offset;// carry-out can only happen once per cycle here, so optimise that
			do{
				size_t difference{*u + *t};// even
				*u = offset;// even, high half
				t[isdescsort * 2 - 1] = offset + 1;// odd, low half
				offset -= difference;
				addcarryofless(b, count, difference);
				difference = u[isdescsort * 6 - 3] + t[isdescsort * 6 - 3];// odd
				u[isdescsort * 6 - 3] = offset;// odd, high half
				*t = offset + 1;// even, low half
				u += isdescsort * 4 - 2;// step forward twice
				t += isdescsort * 4 - 2;
				offset -= difference;
				addcarryofless(b, count, difference);
			}while(--j);
			size_t difference{*u + *t};// even
			*u = offset;// even, high half
			t[isdescsort * 2 - 1] = offset + 1;// odd, low half
			offset -= difference;
			addcarryofless(b, count, difference);
			*t = offset + 1;// even, low half
		}else{// all other modes
			u += isdescsort * 2 - 1;
			t += isdescsort * 2 - 1;
			// 127 / 2 is only rounded down in the companion thread
			// the floating-point case (-1 item) is for the companion thread
			unsigned j{256 / 2 - 1 - 127 / 2 * (isabsvalue && issignmode) - isfltpmode};
			b = count < offset;// carry-out can only happen once per cycle here, so optimise that
			do{
				size_t difference{*u + *t};
				*u = offset;// even, high half
				t[1 - isdescsort * 2] = offset + 1;// odd, low half
				u += isdescsort * 2 - 1;
				t += isdescsort * 2 - 1;
				offset -= difference;
				addcarryofless(b, count, difference);
			}while(--j);
			t[1 - isdescsort * 2] = offset + 1;// odd, low half
		}
	}
	return{b};
}

// version for the companion thread
template<bool isdescsort, bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_INLINE std::pair<unsigned, unsigned> generateoffsetsmultimtc(size_t count, size_t offsets[], size_t offsetscompanion[])noexcept{
	// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
	// Determining the starting point depends on several factors here.
	static size_t constexpr typebitsize{
		(std::is_same_v<longdoubletest128, T> ||
		std::is_same_v<longdoubletest96, T> ||
		std::is_same_v<longdoubletest80, T> ||
		std::is_same_v<long double, T> &&
		64 == LDBL_MANT_DIG &&
		16384 == LDBL_MAX_EXP &&
		128 >= CHAR_BIT * sizeof(long double) &&
		64 < CHAR_BIT * sizeof(long double))? 80 : CHAR_BIT * sizeof(T)};
	// do not pass a nullptr here
	assert(offsets);
	assert(offsetscompanion);
	size_t *tbase{offsets + (typebitsize / 8 - 1) * 256};// point at the top set of the offsets
	size_t *ubase{offsetscompanion + (typebitsize / 8 - 1) * 256};
	unsigned skipsteps{};
	unsigned paritybool;// only the main thread may initialise at 0 or 1 for the parity
	if constexpr(issignmode){// start off with signed handling on the top
		paritybool = generateoffsetssinglemtc<isdescsort, false, isabsvalue, issignmode, isfltpmode, true>(count, tbase, ubase);
		tbase -= 256;
		ubase -= 256;
		skipsteps |= paritybool << (typebitsize / 8 - 1);
	}else paritybool = 0;
	if constexpr(16 < typebitsize || !issignmode || !(isfltpmode && !issignmode && isabsvalue)){
		int k{typebitsize / 8 - 1 - issignmode};
		do{// handle these sets like regular unsigned
			unsigned b{generateoffsetssinglemtc<isdescsort, false, false, false, false, true>(count, tbase, ubase)};
			tbase -= 256;
			ubase -= 256;
			paritybool ^= b;
			skipsteps |= b << k;
			--k;
		}while((isfltpmode && !issignmode && isabsvalue)? 0 < k : 0 <= k);
	}else{// handle this set like regular unsigned
		unsigned b{generateoffsetssinglemtc<isdescsort, false, false, false, false, true>(count, tbase, ubase)};
		if constexpr(isfltpmode && !issignmode && isabsvalue){	
			tbase -= 256;
			ubase -= 256;
		}
		if constexpr(issignmode){
			paritybool ^= b;
			skipsteps |= b;
		}else{
			paritybool = b;
			skipsteps += b * 2;
		}
	}
	if constexpr(isfltpmode && !issignmode && isabsvalue){	// handle the least significant bit
		unsigned b{generateoffsetssinglemtc<isdescsort, false, isabsvalue, issignmode, isfltpmode, true>(count, tbase, ubase)};
		paritybool ^= b;
		skipsteps |= b;
	}
	return{skipsteps, paritybool};// paritybool will be 1 for when the swap count is odd
}

// version for the main thread when multithreading is used
template<bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, bool ismultistage = false>
RSBD8_FUNC_INLINE unsigned generateoffsetssinglemain(size_t count, size_t offsets[], size_t offsetscompanion[])noexcept{
	// do not pass a nullptr here
	assert(offsets);
	assert(offsetscompanion);
	// isdescsort is frequently optimised away in this part, e.g.: isdescsort * 2 - 1 generates 1 or -1
	// Determining the starting point depends of several factors here.
	static size_t constexpr offsetsstride{8 * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode) - (!ismultistage && !isabsvalue && issignmode && isfltpmode)};// shrink the offsets size if possible
	size_t *t{offsets// low-to-high or high-to-low
		+ (issignmode && !isabsvalue) * ((offsetsstride + isfltpmode) / 2 - isdescsort)
		+ (isdescsort && (!issignmode || isabsvalue)) * (offsetsstride - 1)
		+ (isfltpmode && !issignmode && isabsvalue) * (1 - isdescsort * 2)};
	size_t *u{offsetscompanion// low-to-high or high-to-low
		+ (issignmode && !isabsvalue) * ((offsetsstride + isfltpmode) / 2 - isdescsort)
		+ (isdescsort && (!issignmode || isabsvalue)) * (offsetsstride - 1)
		+ (isfltpmode && !issignmode && isabsvalue) * (1 - isdescsort * 2)};
	if constexpr(isrevorder) std::swap(t, u);
	size_t offset{*t + *u};
	*t = 0;// low half, the first offset always starts at zero
	unsigned b;// return value, indicates if a carry-out has occurred and all inputs are valued the same
	if constexpr(!isabsvalue && issignmode){// handle the sign bit, virtually offset the top part by half the range here
		t += 1 - isdescsort * 2;
		u += 1 - isdescsort * 2;
		unsigned j{256 / 2 - 1 - (!ismultistage && isfltpmode && !isdescsort)};// the regular floating-point mode has one less iteration on the signed half (-0. elimination)
		b = count < offset;// carry-out can only happen once per cycle here, so optimise that
		do{
			size_t difference{*t + *u};
			*t = offset;// low half
			u[isdescsort * 2 - 1] = offset - 1;// high half
			t += 1 - isdescsort * 2;
			u += 1 - isdescsort * 2;
			offset += difference;
			addcarryofless(b, count, difference);
		}while(--j);
		u[isdescsort * 2 - 1] = offset - 1;// high half
	}else{// unsigned or signed absolute
		if constexpr(isfltpmode && !issignmode && isabsvalue){// starts at one removed from the initial index
			// custom loop for the special mode: absolute floating-point, but negative inputs will sort just below their positive counterparts
			t += isdescsort * 2 - 1;// step back
			u += isdescsort * 2 - 1;
			unsigned j{256 / 4 - 1};// double the number of items per loop
			b = count < offset;// carry-out can only happen once per cycle here, so optimise that
			do{
				size_t difference{*t + *u};// even
				*t = offset;// even, low half
				u[1 - isdescsort * 2] = offset - 1;// odd, high half
				offset += difference;
				addcarryofless(b, count, difference);
				difference = t[3 - isdescsort * 6] + u[3 - isdescsort * 6];// odd
				t[3 - isdescsort * 6] = offset;// odd, low half
				*u = offset - 1;// even, high half
				t += 2 - isdescsort * 4;// step forward twice
				u += 2 - isdescsort * 4;
				offset += difference;
				addcarryofless(b, count, difference);
			}while(--j);
			size_t difference{*t + *u};// even
			*t = offset;// even, low half
			u[1 - isdescsort * 2] = offset - 1;// odd, high half
			offset += difference;
			addcarryofless(b, count, difference);
			*u = offset - 1;// even, high half
		}else{// all other modes
			t += 1 - isdescsort * 2;
			u += 1 - isdescsort * 2;
			// 127 / 2 is only rounded down in the companion thread
			// the floating-point case (-1 item) is for the companion thread
			unsigned j{256 / 2 - 1 - (127 + 1) / 2 * (isabsvalue && issignmode)};
			b = count < offset;// carry-out can only happen once per cycle here, so optimise that
			do{
				size_t difference{*t + *u};
				*t = offset;// even, low half
				u[isdescsort * 2 - 1] = offset - 1;// odd, high half
				t += 1 - isdescsort * 2;
				u += 1 - isdescsort * 2;
				offset += difference;
				addcarryofless(b, count, difference);
			}while(--j);
			u[isdescsort * 2 - 1] = offset - 1;// odd, high half
		}
	}
	return{b};
}

// version for the main thread when no multithreading is used
template<bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, bool ismultistage = false>
RSBD8_FUNC_INLINE unsigned generateoffsetssinglemain(size_t count, size_t offsets[])noexcept{
	// do not pass a nullptr here
	assert(offsets);
	// isdescsort is frequently optimised away in this part, e.g.: isdescsort * 2 - 1 generates 1 or -1
	// Determining the starting point depends of several factors here.
	static size_t constexpr offsetsstride{8 * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode) - (!ismultistage && !isabsvalue && issignmode && isfltpmode)};// shrink the offsets size if possible
	size_t *t{offsets// low-to-high or high-to-low
		+ (issignmode && !isabsvalue) * ((offsetsstride + isfltpmode) / 2 - isdescsort)
		+ (isdescsort && (!issignmode || isabsvalue)) * (offsetsstride - 1)
		+ (isfltpmode && !issignmode && isabsvalue) * (1 - isdescsort * 2)};
	unsigned b;// return value, indicates if a carry-out has occurred and all inputs are valued the same
	if constexpr(isrevorder){
		size_t offset{*t};
		if constexpr(!isabsvalue && issignmode){// handle the sign bit, virtually offset the top part by half the range here
			size_t difference{t[1 - isdescsort * 2]};
			b = count < offset;// carry-out can only happen once per cycle here, so optimise that
			--offset;
			unsigned j{256 / 2 - 1 - (!ismultistage && isfltpmode && !isdescsort)};// the regular floating-point mode has one less iteration on the signed half (-0. elimination)
			*t = offset;
			t += 1 - isdescsort * 2;
			do{
				offset += difference;
				addcarryofless(b, count, difference);
				size_t difference{t[1 - isdescsort * 2]};
				*t = offset;
				t += 1 - isdescsort * 2;
			}while(--j);
			offset += difference;
			addcarryofless(b, count, difference);
			difference = t[256 * (isdescsort * 2 - 1)];
			j = 256 / 2 - 3 - (!ismultistage && isfltpmode && isdescsort);// the regular floating-point mode has one less iteration on the signed half (-0. elimination)
			*t = offset;
			t += (256 - 1) * (isdescsort * 2 - 1);// offset to the start/end of the range
			do{
				offset += difference;
				addcarryofless(b, count, difference);
				difference = *t;
				*t = offset;
				t += 1 - isdescsort * 2;
			}while(--j);
			offset += difference;
			addcarryofless(b, count, difference);
			*t = offset;
			addcarryofless(b, count, t[1 - isdescsort * 2]);
			t[1 - isdescsort * 2] = count;// the last offset always starts at the end
		}else{// unsigned or signed absolute
			if constexpr(isfltpmode && !issignmode && isabsvalue){// starts at one removed from the initial index
				// custom loop for the special mode: absolute floating-point, but negative inputs will sort just below their positive counterparts
				size_t difference{t[isdescsort * 2 - 1]};// even
				t += isdescsort * 2 - 1;// step back
				b = count < offset;// carry-out can only happen once per cycle here, so optimise that
				--offset;
				unsigned j{256 / 2 - 1};// double the number of items per loop
				do{
					t[1 - isdescsort * 2] = offset;
					offset += difference;
					addcarryofless(b, count, difference);
					difference = t[3 - isdescsort * 6];// odd
					*t = offset;
					offset += difference;
					addcarryofless(b, count, difference);
					difference = t[isdescsort * 2 - 1];// even
					t += 2 - isdescsort * 4;// step forward twice
				}while(--j);
				offset += difference;
				addcarryofless(b, count, difference);
				t[1 - isdescsort * 2] = offset;
				addcarryofless(b, count, *t);
				*t = count;// the last offset always starts at the end
			}else{// all other modes
				size_t difference{t[1 - isdescsort * 2]};
				b = count < offset;// carry-out can only happen once per cycle here, so optimise that
				--offset;
				unsigned j{256 - 2 - 127 * (isabsvalue && issignmode) - isfltpmode};
				do{
					offset += difference;
					addcarryofless(b, count, difference);
					difference = t[2 - isdescsort * 4];
					*t = offset;
					t += 1 - isdescsort * 2;
				}while(--j);
				offset += difference;
				addcarryofless(b, count, difference);
				*t = offset;
				addcarryofless(b, count, t[1 - isdescsort * 2]);
				t[1 - isdescsort * 2] = count;// the last offset always starts at the end
			}
		}
	}else{// not reverse ordered
		size_t offset{*t};
		*t = 0;// the first offset always starts at zero
		if constexpr(!isabsvalue && issignmode){// handle the sign bit, virtually offset the top part by half the range here
			t += 1 - isdescsort * 2;
			unsigned j{256 / 2 - 1 - (!ismultistage && isfltpmode && !isdescsort)};// the regular floating-point mode has one less iteration on the signed half (-0. elimination)
			b = count < offset;// carry-out can only happen once per cycle here, so optimise that
			size_t difference;
			do{
				difference = *t;
				*t = offset;
				t += 1 - isdescsort * 2;
				offset += difference;
				addcarryofless(b, count, difference);
			}while(--j);
			difference = t[256 * (isdescsort * 2 - 1)];
			t[256 * (isdescsort * 2 - 1)] = offset;
			t += (256 - 1) * (isdescsort * 2 - 1);// offset to the start/end of the range
			j = 256 / 2 - 2 - (!ismultistage && isfltpmode && isdescsort);// the regular floating-point mode has one less iteration on the signed half (-0. elimination)
			offset += difference;
			addcarryofless(b, count, difference);
			do{
				difference = *t;
				*t = offset;
				t += 1 - isdescsort * 2;
				offset += difference;
				addcarryofless(b, count, difference);
			}while(--j);
		}else{// unsigned or signed absolute
			if constexpr(isfltpmode && !issignmode && isabsvalue){// starts at one removed from the initial index
				// custom loop for the special mode: absolute floating-point, but negative inputs will sort just below their positive counterparts
				t += isdescsort * 2 - 1;// step back
				unsigned j{256 / 2 - 1};// double the number of items per loop
				b = count < offset;// carry-out can only happen once per cycle here, so optimise that
				do{
					size_t difference{*t};// even
					*t = offset;
					offset += difference;
					addcarryofless(b, count, difference);
					difference = t[3 - isdescsort * 6];// odd
					t[3 - isdescsort * 6] = offset;
					t += 2 - isdescsort * 4;// step forward twice
					offset += difference;
					addcarryofless(b, count, difference);
				}while(--j);
			}else{// all other modes
				t += 1 - isdescsort * 2;
				unsigned j{256 - 2 - 127 * (isabsvalue && issignmode) - isfltpmode};
				b = count < offset;// carry-out can only happen once per cycle here, so optimise that
				do{
					size_t difference{*t};
					*t = offset;
					t += 1 - isdescsort * 2;
					offset += difference;
					addcarryofless(b, count, difference);
				}while(--j);
			}
		}
		addcarryofless(b, count, *t);
		*t = offset;
	}
	return{b};
}

template<bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, bool ismultithreadcapable>
RSBD8_FUNC_INLINE unsigned generateoffsetssingle(size_t count, size_t offsets[], std::conditional_t<ismultithreadcapable, size_t *, std::nullptr_t> offsetscompanion, std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread)noexcept{
	// do not pass a nullptr here
	assert(offsets);
	if constexpr(ismultithreadcapable) if(usemultithread) assert(offsetscompanion);
	unsigned b;// return value, indicates if a carry-out has occurred and all inputs are valued the same
	if constexpr(ismultithreadcapable) if(usemultithread){
		b = generateoffsetssinglemain<isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode>(count, offsets, offsetscompanion);
		goto exit;
	}
	// single-threaded cases, both compile-time and run-time
	b = generateoffsetssinglemain<isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode>(count, offsets);
exit:
	return{b};
}

template<bool isdescsort, bool isabsvalue, bool issignmode, bool isfltpmode, bool ismultithreadcapable, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	128 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T),
	std::pair<unsigned, unsigned>> generateoffsetsmulti(size_t count, size_t offsets[], std::conditional_t<ismultithreadcapable, size_t *, std::nullptr_t> offsetscompanion, std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread, unsigned paritybool = 0)noexcept{
	// Determining the starting point depends on several factors here.
	static size_t constexpr typebitsize{
		(std::is_same_v<longdoubletest128, T> ||
		std::is_same_v<longdoubletest96, T> ||
		std::is_same_v<longdoubletest80, T> ||
		std::is_same_v<long double, T> &&
		64 == LDBL_MANT_DIG &&
		16384 == LDBL_MAX_EXP &&
		128 >= CHAR_BIT * sizeof(long double) &&
		64 < CHAR_BIT * sizeof(long double))? 80 : CHAR_BIT * sizeof(T)};
	// do not pass a nullptr here
	assert(offsets);
	if constexpr(ismultithreadcapable) if(usemultithread) assert(offsetscompanion);
	size_t *tbase{offsets + (typebitsize / 8 - 1) * 256};// point at the top set of the offsets
	unsigned skipsteps{};
	if constexpr(ismultithreadcapable) if(usemultithread){
		size_t *ubase{offsetscompanion + (typebitsize / 8 - 1) * 256};
		if constexpr(issignmode){// start off with signed handling on the top
			unsigned b{generateoffsetssinglemain<isdescsort, false, isabsvalue, issignmode, isfltpmode, true>(count, tbase, ubase)};
			tbase -= 256;
			ubase -= 256;
			paritybool ^= b;
			skipsteps |= b << (typebitsize / 8 - 1);
		}
		if constexpr(16 < typebitsize || !issignmode || !(isfltpmode && !issignmode && isabsvalue)){
			int k{typebitsize / 8 - 1 - issignmode - (isfltpmode && !issignmode && isabsvalue)};
			do{// handle these sets like regular unsigned
				unsigned b{generateoffsetssinglemain<isdescsort, false, false, false, false, true>(count, tbase, ubase)};
				tbase -= 256;
				ubase -= 256;
				paritybool ^= b;
				skipsteps |= b << k;
				--k;
			}while((isfltpmode && !issignmode && isabsvalue)? 0 < k : 0 <= k);
		}else{// handle this set like regular unsigned
			unsigned b{generateoffsetssinglemain<isdescsort, false, false, false, false, true>(count, tbase, ubase)};
			if constexpr(isfltpmode && !issignmode && isabsvalue){	
				tbase -= 256;
				ubase -= 256;
			}
			paritybool ^= b;
			if constexpr(issignmode) skipsteps |= b;
			else skipsteps += b * 2;
		}
		if constexpr(isfltpmode && !issignmode && isabsvalue){	// handle the least significant bit
			unsigned b{generateoffsetssinglemain<isdescsort, false, isabsvalue, issignmode, isfltpmode, true>(count, tbase, ubase)};
			paritybool ^= b;
			skipsteps |= b;
		}
		goto exit;
	}
	// single-threaded cases, both compile-time and run-time
	if constexpr(issignmode){// start off with signed handling on the top
		unsigned b{generateoffsetssinglemain<isdescsort, false, isabsvalue, issignmode, isfltpmode, true>(count, tbase)};
		tbase -= 256;
		paritybool ^= b;
		skipsteps |= b << (typebitsize / 8 - 1);
	}
	if constexpr(16 < typebitsize || !issignmode || !(isfltpmode && !issignmode && isabsvalue)){
		int k{typebitsize / 8 - 1 - issignmode};
		do{// handle these sets like regular unsigned
			unsigned b{generateoffsetssinglemain<isdescsort, false, false, false, false, true>(count, tbase)};
			tbase -= 256;
			paritybool ^= b;
			skipsteps |= b << k;
			--k;
		}while((isfltpmode && !issignmode && isabsvalue)? 0 < k : 0 <= k);
	}else{// handle this set like regular unsigned
		unsigned b{generateoffsetssinglemain<isdescsort, false, false, false, false, true>(count, tbase)};
		if constexpr(isfltpmode && !issignmode && isabsvalue){	
			tbase -= 256;
		}
		paritybool ^= b;
		if constexpr(issignmode) skipsteps |= b;
		else skipsteps += b * 2;
	}
	if constexpr(isfltpmode && !issignmode && isabsvalue){	// handle the least significant bit
		unsigned b{generateoffsetssinglemain<isdescsort, false, isabsvalue, issignmode, isfltpmode, true>(count, tbase)};
		paritybool ^= b;
		skipsteps |= b;
	}
exit:
	return{skipsteps, paritybool};// paritybool will be 1 for when the swap count is odd
}

// Function implementation templates for multi-part types

// initialisation part, multi-threading companion for the radixsortnoallocmulti() function implementation template for 80-bit-based long double types without indirection
// Platforms with a native 80-bit long double type are all little endian, hence that is the only implementation here.
// Do not use this function directly.
template<bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, bool isinputconst, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)),
	void> radixsortnoallocmultiinitmtc(size_t count, std::conditional_t<isinputconst, T const *, T *> input, T pout[], std::conditional_t<isinputconst, T *, std::nullptr_t> pdst, size_t offsetscompanion[])noexcept{
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;
	using U = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t, unsigned>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(3 <= count);// this function is not for small arrays, 4 is the minimum original array count
	// do not pass a nullptr here
	assert(input);
	assert(pout);
	if(isinputconst) assert(pdst);
	assert(offsetscompanion);
	if constexpr(isrevorder && 80 < CHAR_BIT * sizeof(T)){// also reverse the array at the same time
		// reverse ordering is applied here because the padding bytes could matter, hence the check above
		if constexpr(isinputconst){
			size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
			pout += count;
			pdst += count;
			do{
				U curelo{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(input) + 1)};
				uint_least64_t curmlo{*reinterpret_cast<uint_least64_t const *>(input)};
				U curehi{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(input + 1) + 1)};
				uint_least64_t curmhi{*reinterpret_cast<uint_least64_t const *>(input + 1)};
				input += 2;
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(
						curmlo, curelo, pout, pdst,
						curmhi, curehi, pout - 1, pdst - 1);
					pout -= 2;
					pdst -= 2;
				}
				// register pressure performance issue on several platforms: first do the low half here
				unsigned curelo0{static_cast<unsigned>(curelo & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pout) + 1) = curelo;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pdst) + 1) = curelo;
				}
				curelo >>= 8;
				unsigned curmlo0{static_cast<unsigned>(curmlo & 0xFFu)};
				unsigned curmlo1{static_cast<unsigned>(curmlo >> (8 - log2ptrs))};
				unsigned curmlo2{static_cast<unsigned>(curmlo >> (16 - log2ptrs))};
				unsigned curmlo3{static_cast<unsigned>(curmlo >> (24 - log2ptrs))};
				unsigned curmlo4{static_cast<unsigned>(curmlo >> (32 - log2ptrs))};
				unsigned curmlo5{static_cast<unsigned>(curmlo >> (40 - log2ptrs))};
				unsigned curmlo6{static_cast<unsigned>(curmlo >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(pout) = curmlo;
					*reinterpret_cast<uint_least64_t *>(pdst) = curmlo;
				}
				curmlo >>= 56;
				++offsetscompanion[8 * 256 + static_cast<size_t>(curelo0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curelo &= 0x7Fu;
				else curelo &= 0xFFu;
				++offsetscompanion[curmlo0];
				curmlo1 &= sizeof(void *) * 0xFFu;
				curmlo2 &= sizeof(void *) * 0xFFu;
				curmlo3 &= sizeof(void *) * 0xFFu;
				curmlo4 &= sizeof(void *) * 0xFFu;
				curmlo5 &= sizeof(void *) * 0xFFu;
				curmlo6 &= sizeof(void *) * 0xFFu;
				++offsetscompanion[9 * 256 + static_cast<size_t>(curelo)];
				++offsetscompanion[7 * 256 + static_cast<size_t>(curmlo)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curmlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curmlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curmlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curmlo4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curmlo5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curmlo6);
				// register pressure performance issue on several platforms: do the high half here second
				unsigned curehi0{static_cast<unsigned>(curehi & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pout - 1) + 1) = curehi;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pdst - 1) + 1) = curehi;
				}
				curehi >>= 8;
				unsigned curmhi0{static_cast<unsigned>(curmhi & 0xFFu)};
				unsigned curmhi1{static_cast<unsigned>(curmhi >> (8 - log2ptrs))};
				unsigned curmhi2{static_cast<unsigned>(curmhi >> (16 - log2ptrs))};
				unsigned curmhi3{static_cast<unsigned>(curmhi >> (24 - log2ptrs))};
				unsigned curmhi4{static_cast<unsigned>(curmhi >> (32 - log2ptrs))};
				unsigned curmhi5{static_cast<unsigned>(curmhi >> (40 - log2ptrs))};
				unsigned curmhi6{static_cast<unsigned>(curmhi >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(pout - 1) = curmhi;
					pout -= 2;
					*reinterpret_cast<uint_least64_t *>(pdst - 1) = curmhi;
					pdst -= 2;
				}
				curmhi >>= 56;
				++offsetscompanion[8 * 256 + static_cast<size_t>(curehi0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curehi &= 0x7Fu;
				else curehi &= 0xFFu;
				++offsetscompanion[curmhi0];
				curmhi1 &= sizeof(void *) * 0xFFu;
				curmhi2 &= sizeof(void *) * 0xFFu;
				curmhi3 &= sizeof(void *) * 0xFFu;
				curmhi4 &= sizeof(void *) * 0xFFu;
				curmhi5 &= sizeof(void *) * 0xFFu;
				curmhi6 &= sizeof(void *) * 0xFFu;
				++offsetscompanion[7 * 256 + static_cast<size_t>(curmhi)];
				++offsetscompanion[9 * 256 + static_cast<size_t>(curehi)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curmhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curmhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curmhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curmhi4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curmhi5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curmhi6);
				i -= 2;
			}while(0 <= i);
		}else{// !isinputconst
			size_t i{(count + 1 + 2) >> 2};// rounded up in the companion thread
			T *pinputlo{input}, *pinputhi{input + count};
			T *poutputlo{pout}, *poutputhi{pout + count};
			do{
				U curelo{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(pinputlo) + 1)};
				uint_least64_t curmlo{*reinterpret_cast<uint_least64_t const *>(pinputlo)};
				U curehi{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(pinputhi) + 1)};
				uint_least64_t curmhi{*reinterpret_cast<uint_least64_t const *>(pinputhi)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(
						curmlo, curelo, pinputhi, poutputhi,
						curmhi, curehi, pinputlo, poutputlo);
					--pinputhi;
					--poutputhi;
					++pinputlo;
					++poutputlo;
				}
				// register pressure performance issue on several platforms: first do the low half here
				unsigned curelo0{static_cast<unsigned>(curelo & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pinputhi) + 1) = curelo;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(poutputhi) + 1) = curelo;
				}
				curelo >>= 8;
				unsigned curmlo0{static_cast<unsigned>(curmlo & 0xFFu)};
				unsigned curmlo1{static_cast<unsigned>(curmlo >> (8 - log2ptrs))};
				unsigned curmlo2{static_cast<unsigned>(curmlo >> (16 - log2ptrs))};
				unsigned curmlo3{static_cast<unsigned>(curmlo >> (24 - log2ptrs))};
				unsigned curmlo4{static_cast<unsigned>(curmlo >> (32 - log2ptrs))};
				unsigned curmlo5{static_cast<unsigned>(curmlo >> (40 - log2ptrs))};
				unsigned curmlo6{static_cast<unsigned>(curmlo >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(pinputhi) = curmlo;
					--pinputhi;
					*reinterpret_cast<uint_least64_t *>(poutputhi) = curmlo;
					--poutputhi;
				}
				curmlo >>= 56;
				++offsetscompanion[8 * 256 + static_cast<size_t>(curelo0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curelo &= 0x7Fu;
				else curelo &= 0xFFu;
				++offsetscompanion[curmlo0];
				curmlo1 &= sizeof(void *) * 0xFFu;
				curmlo2 &= sizeof(void *) * 0xFFu;
				curmlo3 &= sizeof(void *) * 0xFFu;
				curmlo4 &= sizeof(void *) * 0xFFu;
				curmlo5 &= sizeof(void *) * 0xFFu;
				curmlo6 &= sizeof(void *) * 0xFFu;
				++offsetscompanion[9 * 256 + static_cast<size_t>(curelo)];
				++offsetscompanion[7 * 256 + static_cast<size_t>(curmlo)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curmlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curmlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curmlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curmlo4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curmlo5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curmlo6);
				// register pressure performance issue on several platforms: do the low half here second
				unsigned curehi0{static_cast<unsigned>(curehi & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pinputlo) + 1) = curehi;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(poutputlo) + 1) = curehi;
				}
				curehi >>= 8;
				unsigned curmhi0{static_cast<unsigned>(curmhi & 0xFFu)};
				unsigned curmhi1{static_cast<unsigned>(curmhi >> (8 - log2ptrs))};
				unsigned curmhi2{static_cast<unsigned>(curmhi >> (16 - log2ptrs))};
				unsigned curmhi3{static_cast<unsigned>(curmhi >> (24 - log2ptrs))};
				unsigned curmhi4{static_cast<unsigned>(curmhi >> (32 - log2ptrs))};
				unsigned curmhi5{static_cast<unsigned>(curmhi >> (40 - log2ptrs))};
				unsigned curmhi6{static_cast<unsigned>(curmhi >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(pinputlo) = curmhi;
					++pinputlo;
					*reinterpret_cast<uint_least64_t *>(poutputlo) = curmhi;
					++poutputlo;
				}
				curmhi >>= 56;
				++offsetscompanion[8 * 256 + static_cast<size_t>(curehi0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curehi &= 0x7Fu;
				else curehi &= 0xFFu;
				++offsetscompanion[curmhi0];
				curmhi1 &= sizeof(void *) * 0xFFu;
				curmhi2 &= sizeof(void *) * 0xFFu;
				curmhi3 &= sizeof(void *) * 0xFFu;
				curmhi4 &= sizeof(void *) * 0xFFu;
				curmhi5 &= sizeof(void *) * 0xFFu;
				curmhi6 &= sizeof(void *) * 0xFFu;
				++offsetscompanion[7 * 256 + static_cast<size_t>(curmhi)];
				++offsetscompanion[9 * 256 + static_cast<size_t>(curehi)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curmhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curmhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curmhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curmhi4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curmhi5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curmhi6);
			}while(--i);
		}
	}else{// not in reverse order
		input += count;
		pout += count;
		size_t i{(count + 1 + 2) >> 2};// rounded up in the companion thread
		do{
			U curehi{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(input) + 1)};
			uint_least64_t curmhi{*reinterpret_cast<uint_least64_t const *>(input)};
			U curelo{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(input - 1) + 1)};
			uint_least64_t curmlo{*reinterpret_cast<uint_least64_t const *>(input - 1)};
			input -= 2;
			if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
				filterinput<isabsvalue, issignmode, isfltpmode, T>(
					curmhi, curehi, pout,
					curmlo, curelo, pout - 1);
				pout -= 2;
			}
			unsigned curehi0{static_cast<unsigned>(curehi & 0xFFu)};
			if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pout) + 1) = curehi;
			}
			curehi >>= 8;
			unsigned curmhi0{static_cast<unsigned>(curmhi & 0xFFu)};
			unsigned curmhi1{static_cast<unsigned>(curmhi >> (8 - log2ptrs))};
			unsigned curmhi2{static_cast<unsigned>(curmhi >> (16 - log2ptrs))};
			unsigned curmhi3{static_cast<unsigned>(curmhi >> (24 - log2ptrs))};
			unsigned curmhi4{static_cast<unsigned>(curmhi >> (32 - log2ptrs))};
			unsigned curmhi5{static_cast<unsigned>(curmhi >> (40 - log2ptrs))};
			unsigned curmhi6{static_cast<unsigned>(curmhi >> (48 - log2ptrs))};
			if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
				*reinterpret_cast<uint_least64_t *>(pout) = curmhi;
			}
			curmhi >>= 56;
			++offsetscompanion[8 * 256 + static_cast<size_t>(curehi0)];
			curehi &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
			++offsetscompanion[curmhi0];
			curmhi1 &= sizeof(void *) * 0xFFu;
			curmhi2 &= sizeof(void *) * 0xFFu;
			curmhi3 &= sizeof(void *) * 0xFFu;
			curmhi4 &= sizeof(void *) * 0xFFu;
			curmhi5 &= sizeof(void *) * 0xFFu;
			curmhi6 &= sizeof(void *) * 0xFFu;
			++offsetscompanion[9 * 256 + static_cast<size_t>(curehi)];
			++offsetscompanion[7 * 256 + static_cast<size_t>(curmhi)];
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curmhi1);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curmhi2);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curmhi3);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curmhi4);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curmhi5);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curmhi6);
			// register pressure performance issue on several platforms: do the low half here second
			unsigned curelo0{static_cast<unsigned>(curelo & 0xFFu)};
			if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pout - 1) + 1) = curelo;
			}
			curelo >>= 8;
			unsigned curmlo0{static_cast<unsigned>(curmlo & 0xFFu)};
			unsigned curmlo1{static_cast<unsigned>(curmlo >> (8 - log2ptrs))};
			unsigned curmlo2{static_cast<unsigned>(curmlo >> (16 - log2ptrs))};
			unsigned curmlo3{static_cast<unsigned>(curmlo >> (24 - log2ptrs))};
			unsigned curmlo4{static_cast<unsigned>(curmlo >> (32 - log2ptrs))};
			unsigned curmlo5{static_cast<unsigned>(curmlo >> (40 - log2ptrs))};
			unsigned curmlo6{static_cast<unsigned>(curmlo >> (48 - log2ptrs))};
			if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
				*reinterpret_cast<uint_least64_t *>(pout - 1) = curmlo;
				pout -= 2;
			}
			curmlo >>= 56;
			++offsetscompanion[8 * 256 + static_cast<size_t>(curelo0)];
			curelo &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
			++offsetscompanion[curmlo0];
			curmlo1 &= sizeof(void *) * 0xFFu;
			curmlo2 &= sizeof(void *) * 0xFFu;
			curmlo3 &= sizeof(void *) * 0xFFu;
			curmlo4 &= sizeof(void *) * 0xFFu;
			curmlo5 &= sizeof(void *) * 0xFFu;
			curmlo6 &= sizeof(void *) * 0xFFu;
			++offsetscompanion[9 * 256 + static_cast<size_t>(curelo)];
			++offsetscompanion[7 * 256 + static_cast<size_t>(curmlo)];
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curmlo1);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curmlo2);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curmlo3);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curmlo4);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curmlo5);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curmlo6);
		}while(--i);
	}
}

// main part, multi-threading companion for the radixsortnoallocmulti() function implementation template for 80-bit-based long double types without indirection
// Platforms with a native 80-bit long double type are all little endian, hence that is the only implementation here.
// Do not use this function directly.
template<bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)),
	void> radixsortnoallocmultimainmtc(size_t count, T const input[], T pdst[], T pdstnext[], size_t offsetscompanion[], unsigned runsteps, std::atomic_uintptr_t &atomiclightbarrier)noexcept{
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;
	using U = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t, unsigned>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(3 <= count);// this function is not for small arrays, 4 is the minimum original array count
	// do not pass a nullptr here
	assert(input);
	assert(pdst);
	assert(pdstnext);
	assert(offsetscompanion);
	assert(runsteps);
	unsigned shifter{bitscanforwardportable(runsteps)};// at least 1 bit is set inside runsteps as by previous check
	T *psrchi;
	if constexpr(isrevorder && 80 < CHAR_BIT * sizeof(T)){
		psrchi = pdstnext + count;// reverse ordering is applied here because the padding bytes could matter
	}else{// no reverse ordering applied
		psrchi = const_cast<T *>(input) + count;// psrchi will never be written to
	}
	// skip a step if possible
	runsteps >>= shifter;
	size_t *poffset{offsetscompanion + static_cast<size_t>(shifter) * 256};
	if(80 / 8 - 2 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
		[[unlikely]]
#endif
		goto handletop16;// rare, but possible
	if(80 / 8 - 2 < shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
		[[unlikely]]
#endif
		goto handletop8;// rare, but possible
	shifter *= 8;
	for(;;){
		{
			size_t j{(count + 1 + 4) >> 3};// rounded up in the top part
			do{// fill the array, four at a time
				U outea{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrchi) + 1)};
				uint_least64_t outma{*reinterpret_cast<uint_least64_t const *>(psrchi)};
				U outeb{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrchi - 1) + 1)};
				uint_least64_t outmb{*reinterpret_cast<uint_least64_t const *>(psrchi - 1)};
				U outec{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrchi - 2) + 1)};
				uint_least64_t outmc{*reinterpret_cast<uint_least64_t const *>(psrchi - 2)};
				U outed{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrchi - 3) + 1)};
				uint_least64_t outmd{*reinterpret_cast<uint_least64_t const *>(psrchi - 3)};
				psrchi -= 4;
				auto[cura, curb, curc, curd]{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(outma, outea, outmb, outeb, outmc, outec, outmd, outed, shifter)};
				size_t offseta{poffset[cura]--};// the next item will be placed one lower
				size_t offsetb{poffset[curb]--};
				size_t offsetc{poffset[curc]--};
				size_t offsetd{poffset[curd]--};
				T *pwa = pdst + offseta;
				T *pwb = pdst + offsetb;
				T *pwc = pdst + offsetc;
				T *pwd = pdst + offsetd;
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwa) + 1) = static_cast<W>(outea);
				*reinterpret_cast<uint_least64_t *>(pwa) = outma;
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwb) + 1) = static_cast<W>(outeb);
				*reinterpret_cast<uint_least64_t *>(pwb) = outmb;
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwc) + 1) = static_cast<W>(outec);
				*reinterpret_cast<uint_least64_t *>(pwc) = outmc;
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwd) + 1) = static_cast<W>(outed);
				*reinterpret_cast<uint_least64_t *>(pwd) = outmd;
			}while(--j);
		}
		runsteps >>= 1;
		if(!runsteps)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
			break;
		{
			unsigned index{bitscanforwardportable(runsteps)};// at least 1 bit is set inside runsteps as by previous check
			shifter += 8;
			poffset += 256;
			// swap the pointers for the next round, data is moved on each iteration
			psrchi = pdst;
			uintptr_t old{atomiclightbarrier.fetch_add(~static_cast<uintptr_t>(0))};
			pdst = pdstnext;
			pdstnext = psrchi;
			psrchi += count;
			// skip a step if possible
			runsteps >>= index;
			shifter += index * 8;
			poffset += static_cast<size_t>(index) * 256;
			if(!old) do{
				spinpause();
			}while(atomiclightbarrier.load(std::memory_order_relaxed));
		}
		// handle the top two parts differently
		if(80 - 16 <= shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
		{
			if(80 - 16 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
				[[likely]]
#endif
			{
				{
handletop16:
					size_t j{(count + 1 + 4) >> 3};// rounded up in the top part
					do{// fill the array, four at a time
						U outea{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrchi) + 1)};
						uint_least64_t outma{*reinterpret_cast<uint_least64_t const *>(psrchi)};
						U outeb{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrchi - 1) + 1)};
						uint_least64_t outmb{*reinterpret_cast<uint_least64_t const *>(psrchi - 1)};
						U outec{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrchi - 2) + 1)};
						uint_least64_t outmc{*reinterpret_cast<uint_least64_t const *>(psrchi - 2)};
						U outed{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrchi - 3) + 1)};
						uint_least64_t outmd{*reinterpret_cast<uint_least64_t const *>(psrchi - 3)};
						psrchi -= 4;
						auto[cura, curb, curc, curd]{filterbelowtop8<isabsvalue, issignmode, isfltpmode, T, U>(outma, outea, outmb, outeb, outmc, outec, outmd, outed)};
						size_t offseta{offsetscompanion[cura + (80 - 16) * 256 / 8]--};// the next item will be placed one lower
						size_t offsetb{offsetscompanion[curb + (80 - 16) * 256 / 8]--};
						size_t offsetc{offsetscompanion[curc + (80 - 16) * 256 / 8]--};
						size_t offsetd{offsetscompanion[curd + (80 - 16) * 256 / 8]--};
						T *pwa = pdst + offseta;
						T *pwb = pdst + offsetb;
						T *pwc = pdst + offsetc;
						T *pwd = pdst + offsetd;
						*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwa) + 1) = static_cast<W>(outea);
						*reinterpret_cast<uint_least64_t *>(pwa) = outma;
						*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwb) + 1) = static_cast<W>(outeb);
						*reinterpret_cast<uint_least64_t *>(pwb) = outmb;
						*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwc) + 1) = static_cast<W>(outec);
						*reinterpret_cast<uint_least64_t *>(pwc) = outmc;
						*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwd) + 1) = static_cast<W>(outed);
						*reinterpret_cast<uint_least64_t *>(pwd) = outmd;
					}while(--j);
				}
				if(1 == runsteps)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
					[[unlikely]]
#endif
					break;
				{
					uintptr_t old{atomiclightbarrier.fetch_add(~static_cast<uintptr_t>(0))};
					// swap the pointers for the next round, data is moved on each iteration
					psrchi = pdst;
					pdst = pdstnext;
					// unused: pdstnext = psrchi;
					psrchi += count;
					if(!old) do{
						spinpause();
					}while(atomiclightbarrier.load(std::memory_order_relaxed));
				}
handletop8:
				size_t j{(count + 1 + 4) >> 3};// rounded up in the top part
				do{// fill the array, four at a time
					U outea{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrchi) + 1)};
					uint_least64_t outma{*reinterpret_cast<uint_least64_t const *>(psrchi)};
					U outeb{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrchi - 1) + 1)};
					uint_least64_t outmb{*reinterpret_cast<uint_least64_t const *>(psrchi - 1)};
					U outec{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrchi - 2) + 1)};
					uint_least64_t outmc{*reinterpret_cast<uint_least64_t const *>(psrchi - 2)};
					U outed{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrchi - 3) + 1)};
					uint_least64_t outmd{*reinterpret_cast<uint_least64_t const *>(psrchi - 3)};
					psrchi -= 4;
					auto[cura, curb, curc, curd]{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outma, outea, outmb, outeb, outmc, outec, outmd, outed)};
					size_t offseta{offsetscompanion[cura + (80 - 8) * 256 / 8]--};// the next item will be placed one lower
					size_t offsetb{offsetscompanion[curb + (80 - 8) * 256 / 8]--};
					size_t offsetc{offsetscompanion[curc + (80 - 8) * 256 / 8]--};
					size_t offsetd{offsetscompanion[curd + (80 - 8) * 256 / 8]--};
					T *pwa = pdst + offseta;
					T *pwb = pdst + offsetb;
					T *pwc = pdst + offsetc;
					T *pwd = pdst + offsetd;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwa) + 1) = static_cast<W>(outea);
					*reinterpret_cast<uint_least64_t *>(pwa) = outma;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwb) + 1) = static_cast<W>(outeb);
					*reinterpret_cast<uint_least64_t *>(pwb) = outmb;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwc) + 1) = static_cast<W>(outec);
					*reinterpret_cast<uint_least64_t *>(pwc) = outmc;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwd) + 1) = static_cast<W>(outed);
					*reinterpret_cast<uint_least64_t *>(pwd) = outmd;
				}while(--j);
				break;// no further processing beyond the top part
			}
		}
	}
}

// main part for the radixsortcopynoallocmulti() and radixsortnoallocmulti() function implementation templates for 80-bit-based long double types without indirection
// Do not use this function directly.
template<bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, bool ismultithreadcapable, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)),
	void> radixsortnoallocmultimain(size_t count, T const input[], T pdst[], T pdstnext[], size_t offsets[], unsigned runsteps, std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread, std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t &, std::nullptr_t> atomiclightbarrier)noexcept{
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;
	using U = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t, unsigned>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(count && count != MAXSIZE_T);
	// do not pass a nullptr here
	assert(input);
	assert(pdst);
	assert(pdstnext);
	assert(offsets);
	assert(runsteps);
	unsigned shifter{bitscanforwardportable(runsteps)};// at least 1 bit is set inside runsteps as by previous check
	T *psrclo;
	if constexpr(isrevorder && 80 < CHAR_BIT * sizeof(T)){
		psrclo = pdstnext;// reverse ordering is applied here because the padding bytes could matter
	}else{// no reverse ordering applied
		psrclo = const_cast<T *>(input);// psrclo will never be written to
	}
	// skip a step if possible
	runsteps >>= shifter;
	size_t *poffset{offsets + static_cast<size_t>(shifter) * 256};
	if(80 / 8 - 2 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
		[[unlikely]]
#endif
		goto handletop16;// rare, but possible
	if(80 / 8 - 2 < shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
		[[unlikely]]
#endif
		goto handletop8;// rare, but possible
	shifter *= 8;
	for(;;){
		{
			ptrdiff_t j;// rounded down in the bottom part, or no multithreading
			if constexpr(!ismultithreadcapable) j = static_cast<ptrdiff_t>((count + 1) >> 2);
			else j = static_cast<ptrdiff_t>((count + 1) >> (2 + usemultithread));
			while(0 <= --j){// fill the array, four at a time
				U outea{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo) + 1)};
				uint_least64_t outma{*reinterpret_cast<uint_least64_t const *>(psrclo)};
				U outeb{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo + 1) + 1)};
				uint_least64_t outmb{*reinterpret_cast<uint_least64_t const *>(psrclo + 1)};
				U outec{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo + 2) + 1)};
				uint_least64_t outmc{*reinterpret_cast<uint_least64_t const *>(psrclo + 2)};
				U outed{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo + 3) + 1)};
				uint_least64_t outmd{*reinterpret_cast<uint_least64_t const *>(psrclo + 3)};
				psrclo += 4;
				auto[cura, curb, curc, curd]{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(outma, outea, outmb, outeb, outmc, outec, outmd, outed, shifter)};
				size_t offseta{poffset[cura]++};// the next item will be placed one higher
				size_t offsetb{poffset[curb]++};
				size_t offsetc{poffset[curc]++};
				size_t offsetd{poffset[curd]++};
				T *pwa = pdst + offseta;
				T *pwb = pdst + offsetb;
				T *pwc = pdst + offsetc;
				T *pwd = pdst + offsetd;
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwa) + 1) = static_cast<W>(outea);
				*reinterpret_cast<uint_least64_t *>(pwa) = outma;
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwb) + 1) = static_cast<W>(outeb);
				*reinterpret_cast<uint_least64_t *>(pwb) = outmb;
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwc) + 1) = static_cast<W>(outec);
				*reinterpret_cast<uint_least64_t *>(pwc) = outmc;
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwd) + 1) = static_cast<W>(outed);
				*reinterpret_cast<uint_least64_t *>(pwd) = outmd;
			}
		}
		if(2 & count + 1){// fill in the final two items for a remainder of 2 or 3
			U outea{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo) + 1)};
			uint_least64_t outma{*reinterpret_cast<uint_least64_t const *>(psrclo)};
			U outeb{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo + 1) + 1)};
			uint_least64_t outmb{*reinterpret_cast<uint_least64_t const *>(psrclo + 1)};
			psrclo += 2;
			auto[cura, curb]{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(outma, outea, outmb, outeb, shifter)};
			size_t offseta{poffset[cura]++};// the next item will be placed one higher
			size_t offsetb{poffset[curb]++};
			T *pwa = pdst + offseta;
			T *pwb = pdst + offsetb;
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwa) + 1) = static_cast<W>(outea);
			*reinterpret_cast<uint_least64_t *>(pwa) = outma;
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwb) + 1) = static_cast<W>(outeb);
			*reinterpret_cast<uint_least64_t *>(pwb) = outmb;
		}
		if(!(1 & count)){// fill in the final item for odd counts
			U oute{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo) + 1)};
			uint_least64_t outm{*reinterpret_cast<uint_least64_t const *>(psrclo)};
			size_t cur{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(outm, oute, shifter)};
			size_t offset{poffset[cur]};
			T *pw = pdst + offset;
			*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pw) + 1) = static_cast<W>(oute);
			*reinterpret_cast<uint_least64_t *>(pw) = outm;
		}
		runsteps >>= 1;
		if(!runsteps)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
			break;
		{
			unsigned index{bitscanforwardportable(runsteps)};// at least 1 bit is set inside runsteps as by previous check
			shifter += 8;
			poffset += 256;
			// swap the pointers for the next round, data is moved on each iteration
			psrclo = pdst;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
			[[maybe_unused]]
#endif
			uintptr_t old;
			if constexpr(ismultithreadcapable) old = atomiclightbarrier.fetch_add(usemultithread);
			pdst = pdstnext;
			pdstnext = psrclo;
			// skip a step if possible
			runsteps >>= index;
			shifter += index * 8;
			poffset += static_cast<size_t>(index) * 256;
			if constexpr(ismultithreadcapable) if(old < usemultithread) do{
				spinpause();
			}while(atomiclightbarrier.load(std::memory_order_relaxed));
		}
		// handle the top two parts differently
		if(80 - 16 <= shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
		{
			if(80 - 16 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
				[[likely]]
#endif
			{
				{
handletop16:
					ptrdiff_t j;// rounded down in the bottom part, or no multithreading
					if constexpr(!ismultithreadcapable) j = static_cast<ptrdiff_t>((count + 1) >> 2);
					else j = static_cast<ptrdiff_t>((count + 1) >> (2 + usemultithread));
					while(0 <= --j){// fill the array, four at a time
						U outea{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo) + 1)};
						uint_least64_t outma{*reinterpret_cast<uint_least64_t const *>(psrclo)};
						U outeb{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo + 1) + 1)};
						uint_least64_t outmb{*reinterpret_cast<uint_least64_t const *>(psrclo + 1)};
						U outec{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo + 2) + 1)};
						uint_least64_t outmc{*reinterpret_cast<uint_least64_t const *>(psrclo + 2)};
						U outed{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo + 3) + 1)};
						uint_least64_t outmd{*reinterpret_cast<uint_least64_t const *>(psrclo + 3)};
						psrclo += 4;
						auto[cura, curb, curc, curd]{filterbelowtop8<isabsvalue, issignmode, isfltpmode, T, U>(outma, outea, outmb, outeb, outmc, outec, outmd, outed)};
						size_t offseta{offsets[cura + (80 - 16) * 256 / 8]++};// the next item will be placed one higher
						size_t offsetb{offsets[curb + (80 - 16) * 256 / 8]++};
						size_t offsetc{offsets[curc + (80 - 16) * 256 / 8]++};
						size_t offsetd{offsets[curd + (80 - 16) * 256 / 8]++};
						T *pwa = pdst + offseta;
						T *pwb = pdst + offsetb;
						T *pwc = pdst + offsetc;
						T *pwd = pdst + offsetd;
						*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwa) + 1) = static_cast<W>(outea);
						*reinterpret_cast<uint_least64_t *>(pwa) = outma;
						*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwb) + 1) = static_cast<W>(outeb);
						*reinterpret_cast<uint_least64_t *>(pwb) = outmb;
						*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwc) + 1) = static_cast<W>(outec);
						*reinterpret_cast<uint_least64_t *>(pwc) = outmc;
						*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwd) + 1) = static_cast<W>(outed);
						*reinterpret_cast<uint_least64_t *>(pwd) = outmd;
					}
				}
				if(2 & count + 1){// fill in the final two items for a remainder of 2 or 3
					U outea{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo) + 1)};
					uint_least64_t outma{*reinterpret_cast<uint_least64_t const *>(psrclo)};
					U outeb{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo + 1) + 1)};
					uint_least64_t outmb{*reinterpret_cast<uint_least64_t const *>(psrclo + 1)};
					psrclo += 2;
					auto[cura, curb]{filterbelowtop8<isabsvalue, issignmode, isfltpmode, T, U>(outma, outea, outmb, outeb)};
					size_t offseta{offsets[cura + (80 - 16) * 256 / 8]++};// the next item will be placed one higher
					size_t offsetb{offsets[curb + (80 - 16) * 256 / 8]++};
					T *pwa = pdst + offseta;
					T *pwb = pdst + offsetb;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwa) + 1) = static_cast<W>(outea);
					*reinterpret_cast<uint_least64_t *>(pwa) = outma;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwb) + 1) = static_cast<W>(outeb);
					*reinterpret_cast<uint_least64_t *>(pwb) = outmb;
				}
				if(!(1 & count)){// fill in the final item for odd counts
					U oute{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo) + 1)};
					uint_least64_t outm{*reinterpret_cast<uint_least64_t const *>(psrclo)};
					size_t cur{filterbelowtop8<isabsvalue, issignmode, isfltpmode, T, U>(outm, oute)};
					size_t offset{offsets[cur + (80 - 16) * 256 / 8]};
					T *pw = pdst + offset;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pw) + 1) = static_cast<W>(oute);
					*reinterpret_cast<uint_least64_t *>(pw) = outm;
				}
				runsteps >>= 1;
				if(!runsteps)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
					[[unlikely]]
#endif
					break;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
				[[maybe_unused]]
#endif
				uintptr_t old;
				if constexpr(ismultithreadcapable) old = atomiclightbarrier.fetch_add(usemultithread);
				// swap the pointers for the next round, data is moved on each iteration
				psrclo = pdst;
				pdst = pdstnext;
				// unused: pdstnext = psrclo;
				if constexpr(ismultithreadcapable) if(old < usemultithread) do{
					spinpause();
				}while(atomiclightbarrier.load(std::memory_order_relaxed));
			}
handletop8:
			ptrdiff_t j;// rounded down in the bottom part, or no multithreading
			if constexpr(!ismultithreadcapable) j = static_cast<ptrdiff_t>((count + 1) >> 2);
			else j = static_cast<ptrdiff_t>((count + 1) >> (2 + usemultithread));
			while(0 <= --j){// fill the array, four at a time
				U outea{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo) + 1)};
				uint_least64_t outma{*reinterpret_cast<uint_least64_t const *>(psrclo)};
				U outeb{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo + 1) + 1)};
				uint_least64_t outmb{*reinterpret_cast<uint_least64_t const *>(psrclo + 1)};
				U outec{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo + 2) + 1)};
				uint_least64_t outmc{*reinterpret_cast<uint_least64_t const *>(psrclo + 2)};
				U outed{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo + 3) + 1)};
				uint_least64_t outmd{*reinterpret_cast<uint_least64_t const *>(psrclo + 3)};
				psrclo += 4;
				auto[cura, curb, curc, curd]{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outma, outea, outmb, outeb, outmc, outec, outmd, outed)};
				size_t offseta{offsets[cura + (80 - 8) * 256 / 8]++};// the next item will be placed one higher
				size_t offsetb{offsets[curb + (80 - 8) * 256 / 8]++};
				size_t offsetc{offsets[curc + (80 - 8) * 256 / 8]++};
				size_t offsetd{offsets[curd + (80 - 8) * 256 / 8]++};
				T *pwa = pdst + offseta;
				T *pwb = pdst + offsetb;
				T *pwc = pdst + offsetc;
				T *pwd = pdst + offsetd;
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwa) + 1) = static_cast<W>(outea);
				*reinterpret_cast<uint_least64_t *>(pwa) = outma;
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwb) + 1) = static_cast<W>(outeb);
				*reinterpret_cast<uint_least64_t *>(pwb) = outmb;
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwc) + 1) = static_cast<W>(outec);
				*reinterpret_cast<uint_least64_t *>(pwc) = outmc;
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwd) + 1) = static_cast<W>(outed);
				*reinterpret_cast<uint_least64_t *>(pwd) = outmd;
			}
			if(2 & count + 1){// fill in the final two items for a remainder of 2 or 3
				U outea{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo) + 1)};
				uint_least64_t outma{*reinterpret_cast<uint_least64_t const *>(psrclo)};
				U outeb{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo + 1) + 1)};
				uint_least64_t outmb{*reinterpret_cast<uint_least64_t const *>(psrclo + 1)};
				psrclo += 2;
				auto[cura, curb]{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outma, outea, outmb, outeb)};
				size_t offseta{offsets[cura + (80 - 8) * 256 / 8]++};// the next item will be placed one higher
				size_t offsetb{offsets[curb + (80 - 8) * 256 / 8]++};
				T *pwa = pdst + offseta;
				T *pwb = pdst + offsetb;
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwa) + 1) = static_cast<W>(outea);
				*reinterpret_cast<uint_least64_t *>(pwa) = outma;
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pwb) + 1) = static_cast<W>(outeb);
				*reinterpret_cast<uint_least64_t *>(pwb) = outmb;
			}
			if(!(1 & count)){// fill in the final item for odd counts
				U oute{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(psrclo) + 1)};
				uint_least64_t outm{*reinterpret_cast<uint_least64_t const *>(psrclo)};
				size_t cur{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outm, oute)};
				size_t offset{offsets[cur + (80 - 8) * 256 / 8]};
				T *pw = pdst + offset;
				*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pw) + 1) = static_cast<W>(oute);
				*reinterpret_cast<uint_least64_t *>(pw) = outm;
			}
			break;// no further processing beyond the top part
		}
	}
}

// multi-threading companion for the radixsortcopynoallocmulti() function implementation template for 80-bit-based long double types without indirection
// Do not use this function directly.
template<bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)),
	void> radixsortcopynoallocmultimtc(size_t count, T const input[], T output[], T buffer[], std::atomic_uintptr_t &atomiclightbarrier)noexcept{
	// do not pass a nullptr here
	assert(input);
	assert(output);
	assert(buffer);
	static size_t constexpr offsetsstride{80 * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
	size_t offsetscompanion[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
	radixsortnoallocmultiinitmtc<isrevorder, isabsvalue, issignmode, isfltpmode, true, T>(count, input, output, buffer, offsetscompanion);

	size_t *offsets;
	{// barrier and pointer exchange with the main thread
		uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsetscompanion))};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed);
			}while(reinterpret_cast<uintptr_t>(offsetscompanion) == other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		offsets = reinterpret_cast<size_t *>(other);// retrieve the pointer
	}

	// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
	auto[runsteps, paritybool]{generateoffsetsmultimtc<isdescsort, isabsvalue, issignmode, isfltpmode, T>(count, offsets, offsetscompanion)};

	{// barrier and (flipped bits) runsteps, paritybool value exchange with the main thread
		}
		// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
		uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + 1};
		while(reinterpret_cast<uintptr_t>(offsetscompanion) == atomiclightbarrier.load(std::memory_order_relaxed)){
			spinpause();// catch up
		uintptr_t other{atomiclightbarrier.fetch_add(compound)};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
			}while(!other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		other += compound;// combine
		unsigned lowercarryoutbits{2 + paritybool};
		paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
		other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
		runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
	}

	// perform the bidirectional 8-bit sorting sequence
	// flip the relevant bits inside runsteps first
	if(runsteps ^= (1u << 80 / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
		[[likely]]
#endif
	{// perform the bidirectional 8-bit sorting sequence
		T *pdst{buffer}, *pdstnext{output};// for the next iteration
		if(paritybool){// swap if the count of sorting actions to do is odd
			pdst = output;
			pdstnext = buffer;
		}
		radixsortnoallocmultimainmtc<isrevorder, isabsvalue, issignmode, isfltpmode, T>(count, input, pdst, pdstnext, offsetscompanion, runsteps, atomiclightbarrier);
	}
}

// radixsortcopynoalloc() function implementation template for 80-bit-based long double types without indirection
// Platforms with a native 80-bit long double type are all little endian, hence that is the only implementation here.
template<bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_NORMAL std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)),
	void> radixsortcopynoallocmulti(size_t count, T const input[], T output[], T buffer[])noexcept{
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;
	using U = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t, unsigned>;// assume zero-extension to be basically free for U on basically all modern machines
	static bool constexpr ismultithreadcapable{
#ifdef RSBD8_DISABLE_MULTITHREADING
		false
#else
		true
#endif
	};
	// do not pass a nullptr here, even though it's safe if count is 0
	assert(input);
	assert(output);
	assert(buffer);
	// All the code in this function is adapted for count to be one below its input value here.
	--count;
	if(0 < static_cast<ptrdiff_t>(count)){// a 0 or 1 count array is legal here
		static size_t constexpr offsetsstride{80 * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
		// conditionally enable multi-threading here
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread;// filled in as a boolean 0 or 1, used as unsigned input later on
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t, std::nullptr_t> atomiclightbarrier;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::future<void>, std::nullptr_t> asynchandle;

		// count the 256 configurations, all in one go
		if constexpr(ismultithreadcapable){
			usemultithread = 0;
			// TODO: fine-tune, right now the threshold is set to the 7-bit limit (the minimum is 1 to 7, depending on the size of T)
			if(0x7Fu < count && 1 < std::thread::hardware_concurrency()){
				try{
					asynchandle = std::async(std::launch::async, radixsortcopynoallocmultimtc<isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, T>, count, input, output, buffer, std::ref(atomiclightbarrier));
					usemultithread = 1;
				}catch(...){// std::async may fail gracefully here
					assert(false);
				}
			}
		}
		size_t offsets[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
		ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
		if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
		if constexpr(isrevorder && 80 < CHAR_BIT * sizeof(T)){// also reverse the array at the same time
			// reverse ordering is applied here because the padding bytes could matter, hence the check above
			T const *pinput{input + count};
			T *poutput{output};
			T *pbuffer{buffer};
			do{
				U curehi{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(pinput) + 1)};
				uint_least64_t curmhi{*reinterpret_cast<uint_least64_t const *>(pinput)};
				U curelo{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(pinput - 1) + 1)};
				uint_least64_t curmlo{*reinterpret_cast<uint_least64_t const *>(pinput - 1)};
				pinput -= 2;
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(
						curmhi, curehi, poutput, pbuffer,
						curmlo, curelo, poutput + 1, pbuffer + 1);
					poutput += 2;
					pbuffer += 2;
				}
				// register pressure performance issue on several platforms: first do the high half here
				unsigned curehi0{static_cast<unsigned>(curehi & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(poutput) + 1) = curehi;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pbuffer) + 1) = curehi;
				}
				curehi >>= 8;
				unsigned curmhi0{static_cast<unsigned>(curmhi & 0xFFu)};
				unsigned curmhi1{static_cast<unsigned>(curmhi >> (8 - log2ptrs))};
				unsigned curmhi2{static_cast<unsigned>(curmhi >> (16 - log2ptrs))};
				unsigned curmhi3{static_cast<unsigned>(curmhi >> (24 - log2ptrs))};
				unsigned curmhi4{static_cast<unsigned>(curmhi >> (32 - log2ptrs))};
				unsigned curmhi5{static_cast<unsigned>(curmhi >> (40 - log2ptrs))};
				unsigned curmhi6{static_cast<unsigned>(curmhi >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(poutput) = curmhi;
					*reinterpret_cast<uint_least64_t *>(pbuffer) = curmhi;
				}
				curmhi >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(curehi0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curehi &= 0x7Fu;
				else curehi &= 0xFFu;
				++offsets[curmhi0];
				curmhi1 &= sizeof(void *) * 0xFFu;
				curmhi2 &= sizeof(void *) * 0xFFu;
				curmhi3 &= sizeof(void *) * 0xFFu;
				curmhi4 &= sizeof(void *) * 0xFFu;
				curmhi5 &= sizeof(void *) * 0xFFu;
				curmhi6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(curehi)];
				++offsets[7 * 256 + static_cast<size_t>(curmhi)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curmhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curmhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curmhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curmhi4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curmhi5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curmhi6);
				// register pressure performance issue on several platforms: do the low half here second
				unsigned curelo0{static_cast<unsigned>(curelo & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(poutput + 1) + 1) = curelo;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pbuffer + 1) + 1) = curelo;
				}
				curelo >>= 8;
				unsigned curmlo0{static_cast<unsigned>(curmlo & 0xFFu)};
				unsigned curmlo1{static_cast<unsigned>(curmlo >> (8 - log2ptrs))};
				unsigned curmlo2{static_cast<unsigned>(curmlo >> (16 - log2ptrs))};
				unsigned curmlo3{static_cast<unsigned>(curmlo >> (24 - log2ptrs))};
				unsigned curmlo4{static_cast<unsigned>(curmlo >> (32 - log2ptrs))};
				unsigned curmlo5{static_cast<unsigned>(curmlo >> (40 - log2ptrs))};
				unsigned curmlo6{static_cast<unsigned>(curmlo >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(poutput + 1) = curmlo;
					poutput += 2;
					*reinterpret_cast<uint_least64_t *>(pbuffer + 1) = curmlo;
					pbuffer += 2;
				}
				curmlo >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(curelo0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curelo &= 0x7Fu;
				else curelo &= 0xFFu;
				++offsets[curmlo0];
				curmlo1 &= sizeof(void *) * 0xFFu;
				curmlo2 &= sizeof(void *) * 0xFFu;
				curmlo3 &= sizeof(void *) * 0xFFu;
				curmlo4 &= sizeof(void *) * 0xFFu;
				curmlo5 &= sizeof(void *) * 0xFFu;
				curmlo6 &= sizeof(void *) * 0xFFu;
				++offsets[7 * 256 + static_cast<size_t>(curmlo)];
				++offsets[9 * 256 + static_cast<size_t>(curelo)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curmlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curmlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curmlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curmlo4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curmlo5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curmlo6);
				i -= 2;
			}while(0 < i);
			if(!(1 & i)){// fill in the final item for odd counts
				U cure{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(pinput) + 1)};
				uint_least64_t curm{*reinterpret_cast<uint_least64_t const *>(pinput)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curm, cure, poutput, pbuffer);
				}
				unsigned cure0{static_cast<unsigned>(cure & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(poutput) + 1) = cure;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pbuffer) + 1) = cure;
				}
				cure >>= 8;
				unsigned curm0{static_cast<unsigned>(curm & 0xFFu)};
				unsigned curm1{static_cast<unsigned>(curm >> (8 - log2ptrs))};
				unsigned curm2{static_cast<unsigned>(curm >> (16 - log2ptrs))};
				unsigned curm3{static_cast<unsigned>(curm >> (24 - log2ptrs))};
				unsigned curm4{static_cast<unsigned>(curm >> (32 - log2ptrs))};
				unsigned curm5{static_cast<unsigned>(curm >> (40 - log2ptrs))};
				unsigned curm6{static_cast<unsigned>(curm >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(poutput) = curm;
					*reinterpret_cast<uint_least64_t *>(pbuffer) = curm;
				}
				curm >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(cure0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) cure &= 0x7Fu;
				else cure &= 0xFFu;
				++offsets[curm0];
				curm1 &= sizeof(void *) * 0xFFu;
				curm2 &= sizeof(void *) * 0xFFu;
				curm3 &= sizeof(void *) * 0xFFu;
				curm4 &= sizeof(void *) * 0xFFu;
				curm5 &= sizeof(void *) * 0xFFu;
				curm6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(cure)];
				++offsets[7 * 256 + static_cast<size_t>(curm)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curm1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curm2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curm3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curm4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curm5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curm6);
			}
		}else{// not in reverse order
			T const *pinput{input};
			T *poutput{output};
			do{
				U curelo{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(pinput) + 1)};
				uint_least64_t curmlo{*reinterpret_cast<uint_least64_t const *>(pinput)};
				U curehi{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(pinput + 1) + 1)};
				uint_least64_t curmhi{*reinterpret_cast<uint_least64_t const *>(pinput + 1)};
				pinput += 2;
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(
						curmlo, curelo, poutput,
						curmhi, curehi, poutput + 1);
					poutput += 2;
				}
				unsigned curelo0{static_cast<unsigned>(curelo & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(poutput) + 1) = curelo;
				}
				curelo >>= 8;
				unsigned curmlo0{static_cast<unsigned>(curmlo & 0xFFu)};
				unsigned curmlo1{static_cast<unsigned>(curmlo >> (8 - log2ptrs))};
				unsigned curmlo2{static_cast<unsigned>(curmlo >> (16 - log2ptrs))};
				unsigned curmlo3{static_cast<unsigned>(curmlo >> (24 - log2ptrs))};
				unsigned curmlo4{static_cast<unsigned>(curmlo >> (32 - log2ptrs))};
				unsigned curmlo5{static_cast<unsigned>(curmlo >> (40 - log2ptrs))};
				unsigned curmlo6{static_cast<unsigned>(curmlo >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(poutput) = curmlo;
				}
				curmlo >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(curelo0)];
				curelo &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
				++offsets[curmlo0];
				curmlo1 &= sizeof(void *) * 0xFFu;
				curmlo2 &= sizeof(void *) * 0xFFu;
				curmlo3 &= sizeof(void *) * 0xFFu;
				curmlo4 &= sizeof(void *) * 0xFFu;
				curmlo5 &= sizeof(void *) * 0xFFu;
				curmlo6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(curelo)];
				++offsets[7 * 256 + static_cast<size_t>(curmlo)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curmlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curmlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curmlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curmlo4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curmlo5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curmlo6);
				// register pressure performance issue on several platforms: do the low half here second
				unsigned curehi0{static_cast<unsigned>(curehi & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(poutput + 1) + 1) = curehi;
				}
				curehi >>= 8;
				unsigned curmhi0{static_cast<unsigned>(curmhi & 0xFFu)};
				unsigned curmhi1{static_cast<unsigned>(curmhi >> (8 - log2ptrs))};
				unsigned curmhi2{static_cast<unsigned>(curmhi >> (16 - log2ptrs))};
				unsigned curmhi3{static_cast<unsigned>(curmhi >> (24 - log2ptrs))};
				unsigned curmhi4{static_cast<unsigned>(curmhi >> (32 - log2ptrs))};
				unsigned curmhi5{static_cast<unsigned>(curmhi >> (40 - log2ptrs))};
				unsigned curmhi6{static_cast<unsigned>(curmhi >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(poutput + 1) = curmhi;
					poutput += 2;
				}
				curmhi >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(curehi0)];
				curehi &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
				++offsets[curmhi0];
				curmhi1 &= sizeof(void *) * 0xFFu;
				curmhi2 &= sizeof(void *) * 0xFFu;
				curmhi3 &= sizeof(void *) * 0xFFu;
				curmhi4 &= sizeof(void *) * 0xFFu;
				curmhi5 &= sizeof(void *) * 0xFFu;
				curmhi6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(curehi)];
				++offsets[7 * 256 + static_cast<size_t>(curmhi)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curmhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curmhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curmhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curmhi4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curmhi5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curmhi6);
				i -= 2;
			}while(0 < i);
			if(!(1 & i)){// fill in the final item for odd counts
				U cure{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(pinput) + 1)};
				uint_least64_t curm{*reinterpret_cast<uint_least64_t const *>(pinput)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curm, cure, poutput);
				}
				unsigned cure0{static_cast<unsigned>(cure & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(poutput) + 1) = static_cast<W>(cure);
				}
				cure >>= 8;
				unsigned curm0{static_cast<unsigned>(curm & 0xFFu)};
				unsigned curm1{static_cast<unsigned>(curm >> (8 - log2ptrs))};
				unsigned curm2{static_cast<unsigned>(curm >> (16 - log2ptrs))};
				unsigned curm3{static_cast<unsigned>(curm >> (24 - log2ptrs))};
				unsigned curm4{static_cast<unsigned>(curm >> (32 - log2ptrs))};
				unsigned curm5{static_cast<unsigned>(curm >> (40 - log2ptrs))};
				unsigned curm6{static_cast<unsigned>(curm >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(poutput) = curm;
				}
				curm >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(cure0)];
				cure &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
				++offsets[curm0];
				curm1 &= sizeof(void *) * 0xFFu;
				curm2 &= sizeof(void *) * 0xFFu;
				curm3 &= sizeof(void *) * 0xFFu;
				curm4 &= sizeof(void *) * 0xFFu;
				curm5 &= sizeof(void *) * 0xFFu;
				curm6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(cure)];
				++offsets[7 * 256 + static_cast<size_t>(curm)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curm1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curm2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curm3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curm4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curm5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curm6);
			}
		}

		// barrier and pointer exchange with the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, size_t *, std::nullptr_t> offsetscompanion;
		if constexpr(ismultithreadcapable){
			uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsets) & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed);
				}while(reinterpret_cast<uintptr_t>(offsets) == other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			// this will just be zero if usemultithread is zero
			offsetscompanion = reinterpret_cast<size_t *>(other);// retrieve the pointer
		}

		// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
		auto[runsteps, paritybool]{generateoffsetsmulti<isdescsort, isabsvalue, issignmode, isfltpmode, ismultithreadcapable, T>(count, offsets, offsetscompanion, usemultithread)};

		// barrier and (flipped bits) runsteps, paritybool value exchange with the companion thread
		if constexpr(ismultithreadcapable){
			// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
			uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + static_cast<uintptr_t>(usemultithread)};
			while(reinterpret_cast<uintptr_t>(offsets) == atomiclightbarrier.load(std::memory_order_relaxed)){
				spinpause();// catch up
			}
			uintptr_t other{atomiclightbarrier.fetch_add(compound & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
				}while(!other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			other += compound;// combine
			unsigned lowercarryoutbits{2 * usemultithread + paritybool};
			paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
			other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
			runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
		}

		// perform the bidirectional 8-bit sorting sequence
		// flip the relevant bits inside runsteps first
		if(runsteps ^= (1u << 80 / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
		{
			T *pdst{buffer}, *pdstnext{output};// for the next iteration
			if(paritybool){// swap if the count of sorting actions to do is odd
				pdst = output;
				pdstnext = buffer;
			}
			radixsortnoallocmultimain<isrevorder, isabsvalue, issignmode, isfltpmode, ismultithreadcapable, T>(count, input, pdst, pdstnext, offsets, runsteps, usemultithread, atomiclightbarrier);
		}
	}
}

// multi-threading companion for the radixsortnoallocmulti() function implementation template for 80-bit-based long double types without indirection
// Do not use this function directly.
template<bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)),
	void> radixsortnoallocmultimtc(size_t count, T input[], T buffer[], std::atomic_uintptr_t &atomiclightbarrier)noexcept{
	// do not pass a nullptr here
	assert(input);
	assert(buffer);
	static size_t constexpr offsetsstride{80 * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
	size_t offsetscompanion[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
	radixsortnoallocmultiinitmtc<isrevorder, isabsvalue, issignmode, isfltpmode, false, T>(count, input, buffer, nullptr, offsetscompanion);

	size_t *offsets;
	{// barrier and pointer exchange with the main thread
		uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsetscompanion))};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed);
			}while(reinterpret_cast<uintptr_t>(offsetscompanion) == other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		offsets = reinterpret_cast<size_t *>(other);// retrieve the pointer
	}

	// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
	auto[runsteps, paritybool]{generateoffsetsmultimtc<isdescsort, isabsvalue, issignmode, isfltpmode, T>(count, offsets, offsetscompanion)};

	{// barrier and (flipped bits) runsteps, paritybool value exchange with the main thread
		// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
		uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + 1};
		while(reinterpret_cast<uintptr_t>(offsetscompanion) == atomiclightbarrier.load(std::memory_order_relaxed)){
			spinpause();// catch up
		}
		uintptr_t other{atomiclightbarrier.fetch_add(compound)};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
			}while(!other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		other += compound;// combine
		unsigned lowercarryoutbits{2 + paritybool};
		paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
		other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
		runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
	}

	// perform the bidirectional 8-bit sorting sequence
	// flip the relevant bits inside runsteps first
	if(runsteps ^= (1u << 80 / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
		[[likely]]
#endif
	{// perform the bidirectional 8-bit sorting sequence
		T *psrclo{input}, *pdst{buffer};
		if(paritybool){// swap if the count of sorting actions to do is odd
			psrclo = buffer;
			pdst = input;
		}
		radixsortnoallocmultimainmtc<isrevorder, isabsvalue, issignmode, isfltpmode, T>(count, psrclo, pdst, psrclo, offsetscompanion, runsteps, atomiclightbarrier);
	}
}

// radixsortnoalloc() function implementation template for 80-bit-based long double types without indirection
// Platforms with a native 80-bit long double type are all little endian, hence that is the only implementation here.
template<bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_NORMAL std::enable_if_t<
	(std::is_same_v<longdoubletest128, T> ||
	std::is_same_v<longdoubletest96, T> ||
	std::is_same_v<longdoubletest80, T> ||
	std::is_same_v<long double, T> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)),
	void> radixsortnoallocmulti(size_t count, T input[], T buffer[], bool movetobuffer = false)noexcept{
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;
	using U = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t, unsigned>;// assume zero-extension to be basically free for U on basically all modern machines
	static bool constexpr ismultithreadcapable{
#ifdef RSBD8_DISABLE_MULTITHREADING
		false
#else
		true
#endif
	};
	// do not pass a nullptr here, even though it's safe if count is 0
	assert(input);
	assert(buffer);
	// All the code in this function is adapted for count to be one below its input value here.
	--count;
	if(0 < static_cast<ptrdiff_t>(count)){// a 0 or 1 count array is legal here
		static size_t constexpr offsetsstride{80 * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
		// conditionally enable multi-threading here
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread;// filled in as a boolean 0 or 1, used as unsigned input later on
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t, std::nullptr_t> atomiclightbarrier;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::future<void>, std::nullptr_t> asynchandle;

		// count the 256 configurations, all in one go
		if constexpr(ismultithreadcapable){
			usemultithread = 0;
			// TODO: fine-tune, right now the threshold is set to the 7-bit limit (the minimum is 1 to 7, depending on the size of T)
			if(0x7Fu < count && 1 < std::thread::hardware_concurrency()){
				try{
					asynchandle = std::async(std::launch::async, radixsortnoallocmultimtc<isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, T>, count, input, buffer, std::ref(atomiclightbarrier));
					usemultithread = 1;
				}catch(...){// std::async may fail gracefully here
					assert(false);
				}
			}
		}
		size_t offsets[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
		if constexpr(isrevorder && 80 < CHAR_BIT * sizeof(T)){// also reverse the array at the same time
			// reverse ordering is applied here because the padding bytes could matter, hence the check above
			T *pinputlo, *pinputhi, *pbufferlo, *pbufferhi;
			if constexpr(!ismultithreadcapable){
				pinputlo = input;
				pinputhi = input + count;
				pbufferlo = buffer;
				pbufferhi = buffer + count;
			}else{// if mulitithreaded, the half count will be rounded up in the companion thread
				ptrdiff_t stride{-static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 8) >> 4 * 8)};
				pinputlo = input + stride;
				pinputhi = input + (count - stride);
				pbufferlo = buffer + stride;
				pbufferhi = buffer + (count - stride);
			}
			do{
				U curelo{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(pinputlo) + 1)};
				uint_least64_t curmlo{*reinterpret_cast<uint_least64_t const *>(pinputlo)};
				U curehi{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(pinputhi) + 1)};
				uint_least64_t curmhi{*reinterpret_cast<uint_least64_t const *>(pinputhi)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(
						curmlo, curelo, pinputhi, pbufferhi,
						curmhi, curehi, pinputlo, pbufferlo);
					--pinputhi;
					--pbufferhi;
					++pinputlo;
					++pbufferlo;
				}
				// register pressure performance issue on several platforms: first do the low half here
				unsigned curelo0{static_cast<unsigned>(curelo & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pinputhi) + 1) = curelo;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pbufferhi) + 1) = curelo;
				}
				curelo >>= 8;
				unsigned curmlo0{static_cast<unsigned>(curmlo & 0xFFu)};
				unsigned curmlo1{static_cast<unsigned>(curmlo >> (8 - log2ptrs))};
				unsigned curmlo2{static_cast<unsigned>(curmlo >> (16 - log2ptrs))};
				unsigned curmlo3{static_cast<unsigned>(curmlo >> (24 - log2ptrs))};
				unsigned curmlo4{static_cast<unsigned>(curmlo >> (32 - log2ptrs))};
				unsigned curmlo5{static_cast<unsigned>(curmlo >> (40 - log2ptrs))};
				unsigned curmlo6{static_cast<unsigned>(curmlo >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(pinputhi) = curmlo;
					--pinputhi;
					*reinterpret_cast<uint_least64_t *>(pbufferhi) = curmlo;
					--pbufferhi;
				}
				curmlo >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(curelo0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curelo &= 0x7Fu;
				else curelo &= 0xFFu;
				++offsets[curmlo0];
				curmlo1 &= sizeof(void *) * 0xFFu;
				curmlo2 &= sizeof(void *) * 0xFFu;
				curmlo3 &= sizeof(void *) * 0xFFu;
				curmlo4 &= sizeof(void *) * 0xFFu;
				curmlo5 &= sizeof(void *) * 0xFFu;
				curmlo6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(curelo)];
				++offsets[7 * 256 + static_cast<size_t>(curmlo)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curmlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curmlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curmlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curmlo4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curmlo5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curmlo6);
				// register pressure performance issue on several platforms: do the high half here second
				unsigned curehi0{static_cast<unsigned>(curehi & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pinputlo) + 1) = curehi;
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pbufferlo) + 1) = curehi;
				}
				curehi >>= 8;
				unsigned curmhi0{static_cast<unsigned>(curmhi & 0xFFu)};
				unsigned curmhi1{static_cast<unsigned>(curmhi >> (8 - log2ptrs))};
				unsigned curmhi2{static_cast<unsigned>(curmhi >> (16 - log2ptrs))};
				unsigned curmhi3{static_cast<unsigned>(curmhi >> (24 - log2ptrs))};
				unsigned curmhi4{static_cast<unsigned>(curmhi >> (32 - log2ptrs))};
				unsigned curmhi5{static_cast<unsigned>(curmhi >> (40 - log2ptrs))};
				unsigned curmhi6{static_cast<unsigned>(curmhi >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(pinputlo) = curmhi;
					++pinputlo;
					*reinterpret_cast<uint_least64_t *>(pbufferlo) = curmhi;
					++pbufferlo;
				}
				curmhi >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(curehi0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curehi &= 0x7Fu;
				else curehi &= 0xFFu;
				++offsets[curmhi0];
				curmhi1 &= sizeof(void *) * 0xFFu;
				curmhi2 &= sizeof(void *) * 0xFFu;
				curmhi3 &= sizeof(void *) * 0xFFu;
				curmhi4 &= sizeof(void *) * 0xFFu;
				curmhi5 &= sizeof(void *) * 0xFFu;
				curmhi6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(curehi)];
				++offsets[7 * 256 + static_cast<size_t>(curmhi)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curmhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curmhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curmhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curmhi4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curmhi5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curmhi6);
			}while(pinputlo < pinputhi);
			if(pinputlo == pinputhi){// fill in the final item for odd counts
				U cure{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(pinputlo) + 1)};
				uint_least64_t curm{*reinterpret_cast<uint_least64_t const *>(pinputlo)};
				// no write to input, as this is the midpoint
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curm, cure, pbufferhi);
				}
				unsigned cure0{static_cast<unsigned>(cure & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pbufferhi) + 1) = static_cast<W>(cure);
				}
				cure >>= 8;
				unsigned curm0{static_cast<unsigned>(curm & 0xFFu)};
				unsigned curm1{static_cast<unsigned>(curm >> (8 - log2ptrs))};
				unsigned curm2{static_cast<unsigned>(curm >> (16 - log2ptrs))};
				unsigned curm3{static_cast<unsigned>(curm >> (24 - log2ptrs))};
				unsigned curm4{static_cast<unsigned>(curm >> (32 - log2ptrs))};
				unsigned curm5{static_cast<unsigned>(curm >> (40 - log2ptrs))};
				unsigned curm6{static_cast<unsigned>(curm >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(pbufferhi) = curm;
				}
				curm >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(cure0)];
				cure &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
				++offsets[curm0];
				curm1 &= sizeof(void *) * 0xFFu;
				curm2 &= sizeof(void *) * 0xFFu;
				curm3 &= sizeof(void *) * 0xFFu;
				curm4 &= sizeof(void *) * 0xFFu;
				curm5 &= sizeof(void *) * 0xFFu;
				curm6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(cure)];
				++offsets[7 * 256 + static_cast<size_t>(curm)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curm1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curm2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curm3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curm4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curm5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curm6);
			}
		}else{// not in reverse order
			T *pinput{input};
			T *pbuffer{buffer};
			ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
			if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
			do{
				U curelo{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(pinput) + 1)};
				uint_least64_t curmlo{*reinterpret_cast<uint_least64_t const *>(pinput)};
				U curehi{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(pinput + 1) + 1)};
				uint_least64_t curmhi{*reinterpret_cast<uint_least64_t const *>(pinput + 1)};
				pinput += 2;
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(
						curmlo, curelo, pbuffer,
						curmhi, curehi, pbuffer + 1);
					pbuffer += 2;
				}
				// register pressure performance issue on several platforms: first do the low half here
				unsigned curelo0{static_cast<unsigned>(curelo & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pbuffer) + 1) = static_cast<W>(curelo);
				}
				curelo >>= 8;
				unsigned curmlo0{static_cast<unsigned>(curmlo & 0xFFu)};
				unsigned curmlo1{static_cast<unsigned>(curmlo >> (8 - log2ptrs))};
				unsigned curmlo2{static_cast<unsigned>(curmlo >> (16 - log2ptrs))};
				unsigned curmlo3{static_cast<unsigned>(curmlo >> (24 - log2ptrs))};
				unsigned curmlo4{static_cast<unsigned>(curmlo >> (32 - log2ptrs))};
				unsigned curmlo5{static_cast<unsigned>(curmlo >> (40 - log2ptrs))};
				unsigned curmlo6{static_cast<unsigned>(curmlo >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(pbuffer) = curmlo;
				}
				curmlo >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(curelo0)];
				curelo &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
				++offsets[curmlo0];
				curmlo1 &= sizeof(void *) * 0xFFu;
				curmlo2 &= sizeof(void *) * 0xFFu;
				curmlo3 &= sizeof(void *) * 0xFFu;
				curmlo4 &= sizeof(void *) * 0xFFu;
				curmlo5 &= sizeof(void *) * 0xFFu;
				curmlo6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(curelo)];
				++offsets[7 * 256 + static_cast<size_t>(curmlo)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curmlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curmlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curmlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curmlo4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curmlo5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curmlo6);
				// register pressure performance issue on several platforms: do the high half here second
				unsigned curehi0{static_cast<unsigned>(curehi & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pbuffer + 1) + 1) = static_cast<W>(curehi);
				}
				curehi >>= 8;
				unsigned curmhi0{static_cast<unsigned>(curmhi & 0xFFu)};
				unsigned curmhi1{static_cast<unsigned>(curmhi >> (8 - log2ptrs))};
				unsigned curmhi2{static_cast<unsigned>(curmhi >> (16 - log2ptrs))};
				unsigned curmhi3{static_cast<unsigned>(curmhi >> (24 - log2ptrs))};
				unsigned curmhi4{static_cast<unsigned>(curmhi >> (32 - log2ptrs))};
				unsigned curmhi5{static_cast<unsigned>(curmhi >> (40 - log2ptrs))};
				unsigned curmhi6{static_cast<unsigned>(curmhi >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(pbuffer + 1) = curmhi;
					pbuffer += 2;
				}
				curmhi >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(curehi0)];
				curehi &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
				++offsets[curmhi0];
				curmhi1 &= sizeof(void *) * 0xFFu;
				curmhi2 &= sizeof(void *) * 0xFFu;
				curmhi3 &= sizeof(void *) * 0xFFu;
				curmhi4 &= sizeof(void *) * 0xFFu;
				curmhi5 &= sizeof(void *) * 0xFFu;
				curmhi6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(curehi)];
				++offsets[7 * 256 + static_cast<size_t>(curmhi)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curmhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curmhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curmhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curmhi4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curmhi5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curmhi6);
				i -= 2;
			}while(0 < i);
			if(!(1 & i)){// fill in the final item for odd counts
				U cure{*reinterpret_cast<W const *>(reinterpret_cast<uint_least64_t const *>(pinput) + 1)};
				uint_least64_t curm{*reinterpret_cast<uint_least64_t const *>(pinput)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curm, cure, pbuffer);
				}
				unsigned cure0{static_cast<unsigned>(cure & 0xFFu)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<W *>(reinterpret_cast<uint_least64_t *>(pbuffer) + 1) = static_cast<W>(cure);
				}
				cure >>= 8;
				unsigned curm0{static_cast<unsigned>(curm & 0xFFu)};
				unsigned curm1{static_cast<unsigned>(curm >> (8 - log2ptrs))};
				unsigned curm2{static_cast<unsigned>(curm >> (16 - log2ptrs))};
				unsigned curm3{static_cast<unsigned>(curm >> (24 - log2ptrs))};
				unsigned curm4{static_cast<unsigned>(curm >> (32 - log2ptrs))};
				unsigned curm5{static_cast<unsigned>(curm >> (40 - log2ptrs))};
				unsigned curm6{static_cast<unsigned>(curm >> (48 - log2ptrs))};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)){
					*reinterpret_cast<uint_least64_t *>(pbuffer) = curm;
					++pbuffer;
				}
				curm >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(cure0)];
				cure &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
				++offsets[curm0];
				curm1 &= sizeof(void *) * 0xFFu;
				curm2 &= sizeof(void *) * 0xFFu;
				curm3 &= sizeof(void *) * 0xFFu;
				curm4 &= sizeof(void *) * 0xFFu;
				curm5 &= sizeof(void *) * 0xFFu;
				curm6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(cure)];
				++offsets[7 * 256 + static_cast<size_t>(curm)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curm1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curm2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curm3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curm4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curm5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curm6);
			}
		}

		// barrier and pointer exchange with the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, size_t *, std::nullptr_t> offsetscompanion;
		if constexpr(ismultithreadcapable){
			uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsets) & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed);
				}while(reinterpret_cast<uintptr_t>(offsets) == other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			// this will just be zero if usemultithread is zero
			offsetscompanion = reinterpret_cast<size_t *>(other);// retrieve the pointer
		}

		// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
		auto[runsteps, paritybool]{generateoffsetsmulti<isdescsort, isabsvalue, issignmode, isfltpmode, ismultithreadcapable, T>(count, offsets, offsetscompanion, usemultithread, movetobuffer)};

		// barrier and (flipped bits) runsteps, paritybool value exchange with the companion thread
		if constexpr(ismultithreadcapable){
			// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
			uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + static_cast<uintptr_t>(usemultithread)};
			while(reinterpret_cast<uintptr_t>(offsets) == atomiclightbarrier.load(std::memory_order_relaxed)){
				spinpause();// catch up
			}
			uintptr_t other{atomiclightbarrier.fetch_add(compound & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
				}while(!other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			other += compound;// combine
			unsigned lowercarryoutbits{2 * usemultithread + paritybool};
			paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
			other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
			runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
		}

		// perform the bidirectional 8-bit sorting sequence
		// flip the relevant bits inside runsteps first
		if(runsteps ^= (1u << 80 / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
		{
			T *psrclo{input}, *pdst{buffer};
			if(paritybool){// swap if the count of sorting actions to do is odd
				psrclo = buffer;
				pdst = input;
			}
			radixsortnoallocmultimain<isrevorder, isabsvalue, issignmode, isfltpmode, ismultithreadcapable, T>(count, psrclo, pdst, psrclo, offsets, runsteps, usemultithread, atomiclightbarrier);
		}
	}
}

// initialisation part, multi-threading companion for the radixsortnoallocmulti() function implementation template for multi-part types without indirection
// Do not use this function directly.
template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	!std::is_same_v<bool, T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T),
	void> radixsortnoallocmultiinitmtc(size_t count, T const input[], T pout[], size_t offsetscompanion[])noexcept{
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(7 <= count);// this function is not for small arrays, 8 is the minimum original array count for 16-bit inputs
	// do not pass a nullptr here
	assert(input);
	assert(pout);
	assert(offsetscompanion);
	if constexpr(64 == CHAR_BIT * sizeof(T)){
		if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
		}else{// 64-bit, not in reverse order
			// unsigned counter, not zero inclusive inside the loop
			size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
			input += count - i;
			pout += count - i;
			do{
				U curhi{input[i]};
				U curlo{input[i - 1]};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(
						curhi, pout + i,
						curlo, pout + i - 1);
				}
				// register pressure performance issue on several platforms: first do the high half here
				U curhi0{curhi & 0xFFu};
				U curhi1{curhi >> (8 - log2ptrs)};
				U curhi2{curhi >> (16 - log2ptrs)};
				U curhi3{curhi >> (24 - log2ptrs)};
				U curhi4{curhi >> (32 - log2ptrs)};
				U curhi5{curhi >> (40 - log2ptrs)};
				U curhi6{curhi >> (48 - log2ptrs)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i] = static_cast<T>(curhi);
				curhi >>= 56;
				++offsetscompanion[curhi0];
				curhi1 &= sizeof(void *) * 0xFFu;
				curhi2 &= sizeof(void *) * 0xFFu;
				curhi3 &= sizeof(void *) * 0xFFu;
				curhi4 &= sizeof(void *) * 0xFFu;
				curhi5 &= sizeof(void *) * 0xFFu;
				curhi6 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curhi4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curhi5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curhi6);
				++offsetscompanion[7 * 256 + static_cast<size_t>(curhi)];
				// register pressure performance issue on several platforms: do the low half here second
				U curlo0{curlo & 0xFFu};
				U curlo1{curlo >> (8 - log2ptrs)};
				U curlo2{curlo >> (16 - log2ptrs)};
				U curlo3{curlo >> (24 - log2ptrs)};
				U curlo4{curlo >> (32 - log2ptrs)};
				U curlo5{curlo >> (40 - log2ptrs)};
				U curlo6{curlo >> (48 - log2ptrs)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i - 1] = static_cast<T>(curlo);
				curlo >>= 56;
				++offsetscompanion[curlo0];
				curlo1 &= sizeof(void *) * 0xFFu;
				curlo2 &= sizeof(void *) * 0xFFu;
				curlo3 &= sizeof(void *) * 0xFFu;
				curlo4 &= sizeof(void *) * 0xFFu;
				curlo5 &= sizeof(void *) * 0xFFu;
				curlo6 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curlo4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curlo5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curlo6);
				++offsetscompanion[7 * 256 + static_cast<size_t>(curlo)];
			}while(i -= 2);
		}
	}else if constexpr(56 == CHAR_BIT * sizeof(T)){
		if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
		}else{// 56-bit, not in reverse order
			// unsigned counter, not zero inclusive inside the loop
			size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
			input += count - i;
			pout += count - i;
			do{
				U curhi{input[i]};
				U curlo{input[i - 1]};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(
						curhi, pout + i,
						curlo, pout + i - 1);
				}
				// register pressure performance issue on several platforms: first do the high half here
				U curhi0{curhi & 0xFFu};
				U curhi1{curhi >> (8 - log2ptrs)};
				U curhi2{curhi >> (16 - log2ptrs)};
				U curhi3{curhi >> (24 - log2ptrs)};
				U curhi4{curhi >> (32 - log2ptrs)};
				U curhi5{curhi >> (40 - log2ptrs)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i] = static_cast<T>(curhi);
				curhi >>= 48;
				++offsetscompanion[curhi0];
				curhi1 &= sizeof(void *) * 0xFFu;
				curhi2 &= sizeof(void *) * 0xFFu;
				curhi3 &= sizeof(void *) * 0xFFu;
				curhi4 &= sizeof(void *) * 0xFFu;
				curhi5 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curhi4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curhi5);
				++offsetscompanion[6 * 256 + static_cast<size_t>(curhi)];
				// register pressure performance issue on several platforms: do the low half here second
				U curlo0{curlo & 0xFFu};
				U curlo1{curlo >> (8 - log2ptrs)};
				U curlo2{curlo >> (16 - log2ptrs)};
				U curlo3{curlo >> (24 - log2ptrs)};
				U curlo4{curlo >> (32 - log2ptrs)};
				U curlo5{curlo >> (40 - log2ptrs)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i - 1] = static_cast<T>(curlo);
				curlo >>= 48;
				++offsetscompanion[curlo0];
				curlo1 &= sizeof(void *) * 0xFFu;
				curlo2 &= sizeof(void *) * 0xFFu;
				curlo3 &= sizeof(void *) * 0xFFu;
				curlo4 &= sizeof(void *) * 0xFFu;
				curlo5 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curlo4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curlo5);
				++offsetscompanion[6 * 256 + static_cast<size_t>(curlo)];
			}while(i -= 2);
		}
	}else if constexpr(48 == CHAR_BIT * sizeof(T)){
		if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
		}else{// 48-bit, not in reverse order
			// unsigned counter, not zero inclusive inside the loop
			size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
			input += count - i;
			pout += count - i;
			do{
				U curhi{input[i]};
				U curlo{input[i - 1]};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(
						curhi, pout + i,
						curlo, pout + i - 1);
				}
				// register pressure performance issue on several platforms: first do the high half here
				U curhi0{curhi & 0xFFu};
				U curhi1{curhi >> (8 - log2ptrs)};
				U curhi2{curhi >> (16 - log2ptrs)};
				U curhi3{curhi >> (24 - log2ptrs)};
				U curhi4{curhi >> (32 - log2ptrs)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i] = static_cast<T>(curhi);
				curhi >>= 40;
				++offsetscompanion[curhi0];
				curhi1 &= sizeof(void *) * 0xFFu;
				curhi2 &= sizeof(void *) * 0xFFu;
				curhi3 &= sizeof(void *) * 0xFFu;
				curhi4 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curhi4);
				++offsetscompanion[5 * 256 + static_cast<size_t>(curhi)];
				// register pressure performance issue on several platforms: do the low half here second
				U curlo0{curlo & 0xFFu};
				U curlo1{curlo >> (8 - log2ptrs)};
				U curlo2{curlo >> (16 - log2ptrs)};
				U curlo3{curlo >> (24 - log2ptrs)};
				U curlo4{curlo >> (32 - log2ptrs)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i - 1] = static_cast<T>(curlo);
				curlo >>= 40;
				++offsetscompanion[curlo0];
				curlo1 &= sizeof(void *) * 0xFFu;
				curlo2 &= sizeof(void *) * 0xFFu;
				curlo3 &= sizeof(void *) * 0xFFu;
				curlo4 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curlo4);
				++offsetscompanion[5 * 256 + static_cast<size_t>(curlo)];
			}while(i -= 2);
		}
	}else if constexpr(40 == CHAR_BIT * sizeof(T)){
		if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
		}else{// 40-bit, not in reverse order
			// unsigned counter, not zero inclusive inside the loop
			size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
			input += count - i;
			pout += count - i;
			do{
				U curhi{input[i]};
				U curlo{input[i - 1]};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(
						curhi, pout + i,
						curlo, pout + i - 1);
				}
				U curhi0{curhi & 0xFFu};
				U curhi1{curhi >> (8 - log2ptrs)};
				U curhi2{curhi >> (16 - log2ptrs)};
				U curhi3{curhi >> (24 - log2ptrs)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i] = static_cast<T>(curhi);
				curhi >>= 32;
				U curlo0{curlo & 0xFFu};
				U curlo1{curlo >> (8 - log2ptrs)};
				U curlo2{curlo >> (16 - log2ptrs)};
				U curlo3{curlo >> (24 - log2ptrs)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i - 1] = static_cast<T>(curhi);
				curlo >>= 32;
				++offsetscompanion[curhi0];
				curhi1 &= sizeof(void *) * 0xFFu;
				curhi2 &= sizeof(void *) * 0xFFu;
				curhi3 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
				++offsetscompanion[curlo0];
				curlo1 &= sizeof(void *) * 0xFFu;
				curlo2 &= sizeof(void *) * 0xFFu;
				curlo3 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
				++offsetscompanion[4 * 256 + static_cast<size_t>(curhi)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
				++offsetscompanion[4 * 256 + static_cast<size_t>(curlo)];
			}while(i -= 2);
		}
	}else if constexpr(32 == CHAR_BIT * sizeof(T)){
		if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
		}else{// 32-bit, not in reverse order
			// unsigned counter, not zero inclusive inside the loop
			size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
			input += count - i;
			pout += count - i;
			do{
				U cura{input[i]};
				U curb{input[i - 1]};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(
						cura, pout + i,
						curb, pout + i - 1);
				}
				U cur0a{cura & 0xFFu};
				U cur1a{cura >> (8 - log2ptrs)};
				U cur2a{cura >> (16 - log2ptrs)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i] = static_cast<T>(cura);
				cura >>= 24;
				U cur0b{curb & 0xFFu};
				U cur1b{curb >> (8 - log2ptrs)};
				U cur2b{curb >> (16 - log2ptrs)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i - 1] = static_cast<T>(curb);
				curb >>= 24;
				++offsetscompanion[cur0a];
				cur1a &= sizeof(void *) * 0xFFu;
				cur2a &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
				++offsetscompanion[cur0b];
				cur1b &= sizeof(void *) * 0xFFu;
				cur2b &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1a);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + cur2a);
				++offsetscompanion[3 * 256 + static_cast<size_t>(cura)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1b);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + cur2b);
				++offsetscompanion[3 * 256 + static_cast<size_t>(curb)];
			}while(i -= 2);
		}
	}else if constexpr(24 == CHAR_BIT * sizeof(T)){
		if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
		}else{// 24-bit, not in reverse order
			// unsigned counter, not zero inclusive inside the loop
			size_t i{((count + 1 + 3) / 6) * 3};// rounded up in the companion thread
			input += count - i;
			pout += count - i;
			do{
				U cura{input[i]};
				U curb{input[i - 1]};
				U curc{input[i - 2]};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(
						cura, pout + i,
						curb, pout + i - 1,
						curc, pout + i - 2);
				}
				U cur0a{cura & 0xFFu};
				U cur1a{cura >> (8 - log2ptrs)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i] = static_cast<T>(cura);
				cura >>= 16;
				U cur0b{curb & 0xFFu};
				U cur1b{curb >> (8 - log2ptrs)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i - 1] = static_cast<T>(curb);
				curb >>= 16;
				U cur0c{curc & 0xFFu};
				U cur1c{curc >> (8 - log2ptrs)};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i - 2] = static_cast<T>(curc);
				curc >>= 16;
				++offsetscompanion[cur0a];
				cur1a &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
				++offsetscompanion[cur0b];
				cur1b &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
				++offsetscompanion[cur0c];
				cur1c &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1a);
				++offsetscompanion[2 * 256 + static_cast<size_t>(cura)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1b);
				++offsetscompanion[2 * 256 + static_cast<size_t>(curb)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1c);
				++offsetscompanion[2 * 256 + static_cast<size_t>(curc)];
			}while(i -= 3);
		}
	}else if constexpr(16 == CHAR_BIT * sizeof(T)){
		if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
		}else{// 16-bit, not in reverse order
			// unsigned counter, not zero inclusive inside the loop
			size_t i{((count + 1 + 4) >> 3) * 4};// rounded up in the companion thread
			input += count - i;
			pout += count - i;
			do{
				U cura{input[i]};
				U curb{input[i - 1]};
				U curc{input[i - 2]};
				U curd{input[i - 3]};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(
						cura, pout + i,
						curb, pout + i - 1,
						curc, pout + i - 2,
						curd, pout + i - 3);
				}
				U cur0a{cura & 0xFFu};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i] = static_cast<T>(cura);
				cura >>= 8;
				U cur0b{curb & 0xFFu};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i - 1] = static_cast<T>(curb);
				curb >>= 8;
				U cur0c{curc & 0xFFu};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i - 2] = static_cast<T>(curc);
				curc >>= 8;
				U cur0d{curd & 0xFFu};
				if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) pout[i - 3] = static_cast<T>(curd);
				curd >>= 8;
				++offsetscompanion[cur0a];
				if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
				++offsetscompanion[cur0b];
				if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
				++offsetscompanion[cur0c];
				if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
				++offsetscompanion[cur0d];
				if constexpr(isabsvalue && issignmode && isfltpmode) curd &= 0x7Fu;
				++offsetscompanion[256 + static_cast<size_t>(cura)];
				++offsetscompanion[256 + static_cast<size_t>(curb)];
				++offsetscompanion[256 + static_cast<size_t>(curc)];
				++offsetscompanion[256 + static_cast<size_t>(curd)];
			}while(i -= 4);
		}
	}else static_assert(false, "Implementing larger types will require additional work and optimisation for this library.");
}

// main part, multi-threading companion for the radixsortnoallocmulti() function implementation template for multi-part types without indirection
// Do not use this function directly.
template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	!std::is_same_v<bool, T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T),
	void> radixsortnoallocmultimainmtc(size_t count, T const input[], T pdst[], T pdstnext[], size_t offsetscompanion[], unsigned runsteps, std::atomic_uintptr_t &atomiclightbarrier)noexcept{
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(7 <= count);// this function is not for small arrays, 8 is the minimum original array count for 16-bit inputs
	// do not pass a nullptr here
	assert(input);
	assert(pdst);
	assert(pdstnext);
	assert(offsetscompanion);
	assert(runsteps);
	unsigned shifter{bitscanforwardportable(runsteps)};// at least 1 bit is set inside runsteps as by previous check
	T *psrchi;
	if constexpr(false){// useless when not handling indirection: isrevorder){
		psrchi = pdstnext + count;
	}else{
		psrchi = const_cast<T *>(input) + count;// psrchi will never be written to
	}
	// skip a step if possible
	runsteps >>= shifter;
	size_t *poffset{offsetscompanion + static_cast<size_t>(shifter) * 256};
	if constexpr(!isabsvalue && isfltpmode) if(CHAR_BIT * sizeof(T) / 8 - 1 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
		[[unlikely]]
#endif
		goto handletop8;// rare, but possible
	shifter *= 8;
	for(;;){
		{
			size_t j{(count + 1 + 4) >> 3};// rounded up in the top part
			do{// fill the array, four at a time
				U outa{psrchi[0]};
				U outb{psrchi[-1]};
				U outc{psrchi[-2]};
				U outd{psrchi[-3]};
				psrchi -= 4;
				auto[cura, curb, curc, curd]{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd, shifter)};
				size_t offseta{poffset[cura]--};// the next item will be placed one lower
				size_t offsetb{poffset[curb]--};
				size_t offsetc{poffset[curc]--};
				size_t offsetd{poffset[curd]--};
				pdst[offseta] = static_cast<T>(outa);
				pdst[offsetb] = static_cast<T>(outb);
				pdst[offsetc] = static_cast<T>(outc);
				pdst[offsetd] = static_cast<T>(outd);
			}while(--j);
		}
		runsteps >>= 1;
		if(!runsteps)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
			break;
		{
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
			[[maybe_unused]]
#endif
			unsigned index;
			if constexpr(16 < CHAR_BIT * sizeof(T)) index = bitscanforwardportable(runsteps);// at least 1 bit is set inside runsteps as by previous check
			shifter += 8;
			poffset += 256;
			// swap the pointers for the next round, data is moved on each iteration
			psrchi = pdst;
			uintptr_t old{atomiclightbarrier.fetch_add(~static_cast<uintptr_t>(0))};
			pdst = pdstnext;
			pdstnext = psrchi;
			psrchi += count;
			// skip a step if possible
			if constexpr(16 < CHAR_BIT * sizeof(T)){
				runsteps >>= index;
				shifter += index * 8;
				poffset += static_cast<size_t>(index) * 256;
			}
			if(!old) do{
				spinpause();
			}while(atomiclightbarrier.load(std::memory_order_relaxed));
		}
		// handle the top part for floating-point differently
		if(!isabsvalue && isfltpmode && CHAR_BIT * sizeof(T) - 8 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
		{
handletop8:// this prevents "!isabsvalue && isfltpmode" to be made constexpr here, but that's fine
			size_t j{(count + 1 + 4) >> 3};// rounded up in the top part
			do{// fill the array, four at a time
				U outa{psrchi[0]};
				U outb{psrchi[-1]};
				U outc{psrchi[-2]};
				U outd{psrchi[-3]};
				psrchi -= 4;
				auto[cura, curb, curc, curd]{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd)};
				size_t offseta{offsetscompanion[cura + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]--};// the next item will be placed one lower
				size_t offsetb{offsetscompanion[curb + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]--};
				size_t offsetc{offsetscompanion[curc + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]--};
				size_t offsetd{offsetscompanion[curd + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]--};
				pdst[offseta] = static_cast<T>(outa);
				pdst[offsetb] = static_cast<T>(outb);
				pdst[offsetc] = static_cast<T>(outc);
				pdst[offsetd] = static_cast<T>(outd);
			}while(--j);
			break;// no further processing beyond the top part
		}
	}
}

// main part for the radixsortcopynoallocmulti() and radixsortnoallocmulti() function implementation templates for multi-part types without indirection
// Do not use this function directly.
template<bool isabsvalue, bool issignmode, bool isfltpmode, bool ismultithreadcapable, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	!std::is_same_v<bool, T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T),
	void> radixsortnoallocmultimain(size_t count, T const input[], T pdst[], T pdstnext[], size_t offsets[], unsigned runsteps, std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread, std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t &, std::nullptr_t> atomiclightbarrier)noexcept{
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(count && count != MAXSIZE_T);
	// do not pass a nullptr here
	assert(input);
	assert(pdst);
	assert(pdstnext);
	assert(offsets);
	assert(runsteps);
	unsigned shifter{bitscanforwardportable(runsteps)};// at least 1 bit is set inside runsteps as by previous check
	T *psrclo;
	if constexpr(false){// useless when not handling indirection: isrevorder){
		psrclo = pdstnext;
	}else{
		psrclo = const_cast<T *>(input);// psrclo will never be written to
	}
	// skip a step if possible
	runsteps >>= shifter;
	size_t *poffset{offsets + static_cast<size_t>(shifter) * 256};
	if constexpr(!isabsvalue && isfltpmode) if(CHAR_BIT * sizeof(T) / 8 - 1 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
		[[unlikely]]
#endif
		goto handletop8;// rare, but possible
	shifter *= 8;
	for(;;){
		{
			ptrdiff_t j;// rounded down in the bottom part, or no multithreading
			if constexpr(!ismultithreadcapable) j = static_cast<ptrdiff_t>((count + 1) >> 2);
			else j = static_cast<ptrdiff_t>((count + 1) >> (2 + usemultithread));
			while(0 <= --j){// fill the array, four at a time
				U outa{psrclo[0]};
				U outb{psrclo[1]};
				U outc{psrclo[2]};
				U outd{psrclo[3]};
				psrclo += 4;
				auto[cura, curb, curc, curd]{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd, shifter)};
				size_t offseta{poffset[cura]++};// the next item will be placed one higher
				size_t offsetb{poffset[curb]++};
				size_t offsetc{poffset[curc]++};
				size_t offsetd{poffset[curd]++};
				pdst[offseta] = static_cast<T>(outa);
				pdst[offsetb] = static_cast<T>(outb);
				pdst[offsetc] = static_cast<T>(outc);
				pdst[offsetd] = static_cast<T>(outd);
			}
		}
		if(2 & count + 1){// fill in the final two items for a remainder of 2 or 3
			U outa{psrclo[0]};
			U outb{psrclo[1]};
			psrclo += 2;
			auto[cura, curb]{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, shifter)};
			size_t offseta{poffset[cura]++};// the next item will be placed one higher
			size_t offsetb{poffset[curb]++};
			pdst[offseta] = static_cast<T>(outa);
			pdst[offsetb] = static_cast<T>(outb);
		}
		if(!(1 & count)){// fill in the final item for odd counts
			U out{psrclo[0]};
			size_t cur{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(out, shifter)};
			size_t offset{poffset[cur]};
			pdst[offset] = static_cast<T>(out);
		}
		runsteps >>= 1;
		if(!runsteps)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
			break;
		{
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
			[[maybe_unused]]
#endif
			unsigned index;
			if constexpr(16 < CHAR_BIT * sizeof(T)) index = bitscanforwardportable(runsteps);// at least 1 bit is set inside runsteps as by previous check
			shifter += 8;
			poffset += 256;
			// swap the pointers for the next round, data is moved on each iteration
			psrclo = pdst;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
			[[maybe_unused]]
#endif
			uintptr_t old;
			if constexpr(ismultithreadcapable) old = atomiclightbarrier.fetch_add(usemultithread);
			pdst = pdstnext;
			pdstnext = psrclo;
			// skip a step if possible
			if constexpr(16 < CHAR_BIT * sizeof(T)){
				runsteps >>= index;
				shifter += index * 8;
				poffset += static_cast<size_t>(index) * 256;
			}
			if constexpr(ismultithreadcapable) if(old < usemultithread) do{
				spinpause();
			}while(atomiclightbarrier.load(std::memory_order_relaxed));
		}
		// handle the top part for floating-point differently
		if(!isabsvalue && isfltpmode && CHAR_BIT * sizeof(T) - 8 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
		{
handletop8:// this prevents "!isabsvalue && isfltpmode" to be made constexpr here, but that's fine
			ptrdiff_t j;// rounded down in the bottom part, or no multithreading
			if constexpr(!ismultithreadcapable) j = static_cast<ptrdiff_t>((count + 1) >> 2);
			else j = static_cast<ptrdiff_t>((count + 1) >> (2 + usemultithread));
			while(0 <= --j){// fill the array, four at a time
				U outa{psrclo[0]};
				U outb{psrclo[1]};
				U outc{psrclo[2]};
				U outd{psrclo[3]};
				psrclo += 4;
				auto[cura, curb, curc, curd]{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd)};
				size_t offseta{offsets[cura + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]++};// the next item will be placed one higher
				size_t offsetb{offsets[curb + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]++};
				size_t offsetc{offsets[curc + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]++};
				size_t offsetd{offsets[curd + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]++};
				pdst[offseta] = static_cast<T>(outa);
				pdst[offsetb] = static_cast<T>(outb);
				pdst[offsetc] = static_cast<T>(outc);
				pdst[offsetd] = static_cast<T>(outd);
			}
			if(2 & count + 1){// fill in the final two items for a remainder of 2 or 3
				U outa{psrclo[0]};
				U outb{psrclo[1]};
				psrclo += 2;
				auto[cura, curb]{filtertop8<isabsvalue, issignmode, isfltpmode,T, U>(outa, outb)};
				size_t offseta{offsets[cura + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]++};// the next item will be placed one higher
				size_t offsetb{offsets[curb + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]++};
				pdst[offseta] = static_cast<T>(outa);
				pdst[offsetb] = static_cast<T>(outb);
			}
			if(!(1 & count)){// fill in the final item for odd counts
				U out{psrclo[0]};
				size_t cur{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(out)};
				size_t offset{offsets[cur + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]};
				pdst[offset] = static_cast<T>(out);
			}
			break;// no further processing beyond the top part
		}
	}
}

// multi-threading companion for the radixsortcopynoallocmulti() function implementation template for 80-bit-based long double types without indirection
// Do not use this function directly.
template<bool isdescsort, bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	!std::is_same_v<bool, T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T),
	void> radixsortcopynoallocmultimtc(size_t count, T const input[], T output[], T buffer[], std::atomic_uintptr_t &atomiclightbarrier)noexcept{
	// do not pass a nullptr here
	assert(input);
	assert(output);
	assert(buffer);
	static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
	size_t offsetscompanion[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
	radixsortnoallocmultiinitmtc<isabsvalue, issignmode, isfltpmode, T>(count, input, output, offsetscompanion);

	size_t *offsets;
	{// barrier and pointer exchange with the main thread
		uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsetscompanion))};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed);
			}while(reinterpret_cast<uintptr_t>(offsetscompanion) == other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		offsets = reinterpret_cast<size_t *>(other);// retrieve the pointer
	}

	// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
	auto[runsteps, paritybool]{generateoffsetsmultimtc<isdescsort, isabsvalue, issignmode, isfltpmode, T>(count, offsets, offsetscompanion)};

	{// barrier and (flipped bits) runsteps, paritybool value exchange with the main thread
		// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
		uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + 1};
		while(reinterpret_cast<uintptr_t>(offsetscompanion) == atomiclightbarrier.load(std::memory_order_relaxed)){
			spinpause();// catch up
		}
		uintptr_t other{atomiclightbarrier.fetch_add(compound)};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
			}while(!other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		other += compound;// combine
		unsigned lowercarryoutbits{2 + paritybool};
		paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
		other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
		runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
	}

	// perform the bidirectional 8-bit sorting sequence
	// flip the relevant bits inside runsteps first
	if(runsteps ^= (1u << CHAR_BIT * sizeof(T) / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
		[[likely]]
#endif
	{// perform the bidirectional 8-bit sorting sequence
		T *pdst{buffer}, *pdstnext{output};// for the next iteration
		if(paritybool){// swap if the count of sorting actions to do is odd
			pdst = output;
			pdstnext = buffer;
		}
		radixsortnoallocmultimainmtc<isabsvalue, issignmode, isfltpmode, T>(count, input, pdst, pdstnext, offsetscompanion, runsteps, atomiclightbarrier);
	}
}

// radixsortcopynoalloc() function implementation template for multi-part types without indirection
template<bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_NORMAL std::enable_if_t<
	std::is_unsigned_v<T> &&
	!std::is_same_v<bool, T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T),
	void> radixsortcopynoallocmulti(size_t count, T const input[], T output[], T buffer[])noexcept{
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	static bool constexpr ismultithreadcapable{
#ifdef RSBD8_DISABLE_MULTITHREADING
		false
#else
		true
#endif
	};
	// do not pass a nullptr here, even though it's safe if count is 0
	assert(input);
	assert(output);
	assert(buffer);
	// All the code in this function is adapted for count to be one below its input value here.
	--count;
	if(0 < static_cast<ptrdiff_t>(count)){// a 0 or 1 count array is legal here
		static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
		// conditionally enable multi-threading here
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread;// filled in as a boolean 0 or 1, used as unsigned input later on
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t, std::nullptr_t> atomiclightbarrier;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::future<void>, std::nullptr_t> asynchandle;

		// count the 256 configurations, all in one go
		if constexpr(ismultithreadcapable){
			usemultithread = 0;
			// TODO: fine-tune, right now the threshold is set to the 7-bit limit (the minimum is 1 to 7, depending on the size of T)
			if(0x7Fu < count && 1 < std::thread::hardware_concurrency()){
				try{
					asynchandle = std::async(std::launch::async, radixsortcopynoallocmultimtc<isdescsort, isabsvalue, issignmode, isfltpmode, T>, count, input, output, buffer, std::ref(atomiclightbarrier));
					usemultithread = 1;
				}catch(...){// std::async may fail gracefully here
					assert(false);
				}
			}
		}
		size_t offsets[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
		if constexpr(64 == CHAR_BIT * sizeof(T)){
			if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
			}else{// 64-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				do{
					U curhi{input[i]};
					U curlo{input[i - 1]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							curhi, output + i,
							curlo, output + i - 1);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					U curhi6{curhi >> (48 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i] = static_cast<T>(curhi);
					curhi >>= 56;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					curhi6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curhi5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curhi6);
					++offsets[7 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					U curlo6{curlo >> (48 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i - 1] = static_cast<T>(curlo);
					curlo >>= 56;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					curlo6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curlo5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curlo6);
					++offsets[7 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					U cur{input[0]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, output);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					U cur5{cur >> (40 - log2ptrs)};
					U cur6{cur >> (48 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[0] = static_cast<T>(cur);
					cur >>= 56;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					cur5 &= sizeof(void *) * 0xFFu;
					cur6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + cur5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + cur6);
					++offsets[7 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(56 == CHAR_BIT * sizeof(T)){
			if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
			}else{// 56-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				do{
					U curhi{input[i]};
					U curlo{input[i - 1]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							curhi, output + i,
							curlo, output + i - 1);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i] = static_cast<T>(curhi);
					curhi >>= 48;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curhi5);
					++offsets[6 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i - 1] = static_cast<T>(curlo);
					curlo >>= 48;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curlo5);
					++offsets[6 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					U cur{input[0]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, output);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					U cur5{cur >> (40 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[0] = static_cast<T>(cur);
					cur >>= 48;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					cur5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + cur5);
					++offsets[6 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(48 == CHAR_BIT * sizeof(T)){
			if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
			}else{// 48-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				do{
					U curhi{input[i]};
					U curlo{input[i - 1]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							curhi, output + i,
							curlo, output + i - 1);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i] = static_cast<T>(curhi);
					curhi >>= 40;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++offsets[5 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i - 1] = static_cast<T>(curlo);
					curlo >>= 40;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++offsets[5 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					U cur{input[0]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, output);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[0] = static_cast<T>(cur);
					cur >>= 40;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++offsets[5 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(40 == CHAR_BIT * sizeof(T)){
			if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
			}else{// 40-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				do{
					U curhi{input[i]};
					U curlo{input[i - 1]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							curhi, output + i,
							curlo, output + i - 1);
					}
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i] = static_cast<T>(curhi);
					curhi >>= 32;
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i - 1] = static_cast<T>(curlo);
					curlo >>= 32;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++offsets[4 * 256 + static_cast<size_t>(curhi)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++offsets[4 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					U cur{input[0]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, output);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[0] = static_cast<T>(cur);
					cur >>= 32;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++offsets[4 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(32 == CHAR_BIT * sizeof(T)){
			if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
			}else{// 32-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				do{
					U cura{input[i]};
					U curb{input[i - 1]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							cura, output + i,
							curb, output + i - 1);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					U cur2a{cura >> (16 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i] = static_cast<T>(cura);
					cura >>= 24;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					U cur2b{curb >> (16 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i - 1] = static_cast<T>(curb);
					curb >>= 24;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					cur2a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					cur2b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2a);
					++offsets[3 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2b);
					++offsets[3 * 256 + static_cast<size_t>(curb)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					U cur{input[0]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, output);
					}
					U cur0{cur & 0xFFu};
					U cur1{static_cast<unsigned>(cur) >> (8 - log2ptrs)};
					U cur2{static_cast<unsigned>(cur) >> (16 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[0] = static_cast<T>(cur);
					cur >>= 24;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++offsets[3 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(24 == CHAR_BIT * sizeof(T)){
			if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
			}else{// 24-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>(count + 1 + 3) / 6 * 3;
				i -= 2;
				if(0 <= i)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
					[[likely]]
#endif
					do{
					U cura{input[i + 2]};
					U curb{input[i + 1]};
					U curc{input[i]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							cura, output + i + 2,
							curb, output + i + 1,
							curc, output + i);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i + 2] = static_cast<T>(cura);
					cura >>= 16;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i + 1] = static_cast<T>(curb);
					curb >>= 16;
					U cur0c{curc & 0xFFu};
					U cur1c{curc >> (8 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i] = static_cast<T>(curc);
					curc >>= 16;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[cur0c];
					cur1c &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++offsets[2 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++offsets[2 * 256 + static_cast<size_t>(curb)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1c);
					++offsets[2 * 256 + static_cast<size_t>(curc)];
					i -= 3;
				}while(0 <= i);
				if(2 & i){// fill in the final two items for a remainder of 2 or 3
					U cura{input[i + 2]};
					U curb{input[i + 1]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							cura, output + i + 2,
							curb, output + i + 1);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i + 2] = static_cast<T>(cura);
					cura >>= 16;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i + 1] = static_cast<T>(curb);
					curb >>= 16;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++offsets[2 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++offsets[2 * 256 + static_cast<size_t>(curb)];
				}else if(1 & i){// fill in the final item for odd counts
					U cur{input[0]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, output);
					}
					U cur0{cur & 0xFFu};
					U cur1{static_cast<unsigned>(cur) >> (8 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[0] = static_cast<T>(cur);
					cur >>= 16;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++offsets[2 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(16 == CHAR_BIT * sizeof(T)){
			if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
			}else{// 16-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 4) >> 3) * 4;
				i -= 3;
				if(0 <= i)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
					[[likely]]
#endif
					do{
					U cura{input[i + 3]};
					U curb{input[i + 2]};
					U curc{input[i + 1]};
					U curd{input[i]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							cura, output + i + 3,
							curb, output + i + 2,
							curc, output + i + 1,
							curd, output + i);
					}
					U cur0a{cura & 0xFFu};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i + 3] = static_cast<T>(cura);
					cura >>= 8;
					U cur0b{curb & 0xFFu};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i + 2] = static_cast<T>(curb);
					curb >>= 8;
					U cur0c{curc & 0xFFu};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i + 1] = static_cast<T>(curc);
					curc >>= 8;
					U cur0d{curd & 0xFFu};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i] = static_cast<T>(curd);
					curd >>= 8;
					++offsets[cur0a];
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[cur0c];
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++offsets[cur0d];
					if constexpr(isabsvalue && issignmode && isfltpmode) curd &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cura)];
					++offsets[256 + static_cast<size_t>(curb)];
					++offsets[256 + static_cast<size_t>(curc)];
					++offsets[256 + static_cast<size_t>(curd)];
					i -= 4;
				}while(0 <= i);
				if(2 & i){// fill in the final two items for a remainder of 2 or 3
					U cura{input[i + 3]};
					U curb{input[i + 2]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							cura, output + i + 3,
							curb, output + i + 2);
					}
					U cur0a{cura & 0xFFu};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i + 3] = static_cast<T>(cura);
					cura >>= 8;
					U cur0b{curb & 0xFFu};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[i + 2] = static_cast<T>(curb);
					curb >>= 8;
					++offsets[cur0a];
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cura)];
					++offsets[256 + static_cast<size_t>(curb)];
				}
				if(1 & i){// fill in the final item for odd counts
					U cur{input[0]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, output);
					}
					U cur0{cur & 0xFFu};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) output[0] = static_cast<T>(cur);
					cur >>= 8;
					++offsets[cur0];
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cur)];
				}
			}
		}else static_assert(false, "Implementing larger types will require additional work and optimisation for this library.");

		// barrier and pointer exchange with the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, size_t *, std::nullptr_t> offsetscompanion;
		if constexpr(ismultithreadcapable){
			uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsets) & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed);
				}while(reinterpret_cast<uintptr_t>(offsets) == other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			// this will just be zero if usemultithread is zero
			offsetscompanion = reinterpret_cast<size_t *>(other);// retrieve the pointer
		}

		// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
		auto[runsteps, paritybool]{generateoffsetsmulti<isdescsort, isabsvalue, issignmode, isfltpmode, ismultithreadcapable, T>(count, offsets, offsetscompanion, usemultithread)};

		// barrier and (flipped bits) runsteps, paritybool value exchange with the companion thread
		if constexpr(ismultithreadcapable){
			// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
			uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + static_cast<uintptr_t>(usemultithread)};
			while(reinterpret_cast<uintptr_t>(offsets) == atomiclightbarrier.load(std::memory_order_relaxed)){
				spinpause();// catch up
			}
			uintptr_t other{atomiclightbarrier.fetch_add(compound & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
				}while(!other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			other += compound;// combine
			unsigned lowercarryoutbits{2 * usemultithread + paritybool};
			paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
			other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
			runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
		}

		// perform the bidirectional 8-bit sorting sequence
		// flip the relevant bits inside runsteps first
		if(runsteps ^= (1u << CHAR_BIT * sizeof(T) / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
		{
			T *pdst{buffer}, *pdstnext{output};// for the next iteration
			if(paritybool){// swap if the count of sorting actions to do is odd
				pdst = output;
				pdstnext = buffer;
			}
			radixsortnoallocmultimain<isabsvalue, issignmode, isfltpmode, ismultithreadcapable, T>(count, input, pdst, pdstnext, offsets, runsteps, usemultithread, atomiclightbarrier);
		}
	}
}

// multi-threading companion for the radixsortnoallocmulti() function implementation template for multi-part types without indirection
// Do not use this function directly.
template<bool isdescsort, bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	!std::is_same_v<bool, T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T),
	void> radixsortnoallocmultimtc(size_t count, T input[], T buffer[], std::atomic_uintptr_t &atomiclightbarrier)noexcept{
	// do not pass a nullptr here
	assert(input);
	assert(buffer);
	static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
	size_t offsetscompanion[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
	radixsortnoallocmultiinitmtc<isabsvalue, issignmode, isfltpmode, T>(count, input, buffer, offsetscompanion);

	size_t *offsets;
	{// barrier and pointer exchange with the main thread
		uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsetscompanion))};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed);
			}while(reinterpret_cast<uintptr_t>(offsetscompanion) == other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		offsets = reinterpret_cast<size_t *>(other);// retrieve the pointer
	}

	// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
	auto[runsteps, paritybool]{generateoffsetsmultimtc<isdescsort, isabsvalue, issignmode, isfltpmode, T>(count, offsets, offsetscompanion)};

	{// barrier and (flipped bits) runsteps, paritybool value exchange with the main thread
		// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
		uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + 1};
		while(reinterpret_cast<uintptr_t>(offsetscompanion) == atomiclightbarrier.load(std::memory_order_relaxed)){
			spinpause();// catch up
		}
		uintptr_t other{atomiclightbarrier.fetch_add(compound)};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
			}while(!other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		other += compound;// combine
		unsigned lowercarryoutbits{2 + paritybool};
		paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
		other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
		runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
	}

	// perform the bidirectional 8-bit sorting sequence
	// flip the relevant bits inside runsteps first
	if(runsteps ^= (1u << CHAR_BIT * sizeof(T) / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
		[[likely]]
#endif
	{// perform the bidirectional 8-bit sorting sequence
		T *psrclo{input}, *pdst{buffer};
		if(paritybool){// swap if the count of sorting actions to do is odd
			psrclo = buffer;
			pdst = input;
		}
		radixsortnoallocmultimainmtc<isabsvalue, issignmode, isfltpmode, T>(count, psrclo, pdst, psrclo, offsetscompanion, runsteps, atomiclightbarrier);
	}
}

// radixsortnoalloc() function implementation template for multi-part types without indirection
template<bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_NORMAL std::enable_if_t<
	std::is_unsigned_v<T> &&
	!std::is_same_v<bool, T> &&
	64 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T),
	void> radixsortnoallocmulti(size_t count, T input[], T buffer[], bool movetobuffer = false)noexcept{
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	static bool constexpr ismultithreadcapable{
#ifdef RSBD8_DISABLE_MULTITHREADING
		false
#else
		true
#endif
	};
	// do not pass a nullptr here, even though it's safe if count is 0
	assert(input);
	assert(buffer);
	// All the code in this function is adapted for count to be one below its input value here.
	--count;
	if(0 < static_cast<ptrdiff_t>(count)){// a 0 or 1 count array is legal here
		static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
		// conditionally enable multi-threading here
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread;// filled in as a boolean 0 or 1, used as unsigned input later on
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t, std::nullptr_t> atomiclightbarrier;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::future<void>, std::nullptr_t> asynchandle;

		// count the 256 configurations, all in one go
		if constexpr(ismultithreadcapable){
			usemultithread = 0;
			// TODO: fine-tune, right now the threshold is set to the 7-bit limit (the minimum is 1 to 7, depending on the size of T)
			if(0x7Fu < count && 1 < std::thread::hardware_concurrency()){
				try{
					asynchandle = std::async(std::launch::async, radixsortnoallocmultimtc<isdescsort, isabsvalue, issignmode, isfltpmode, T>, count, input, buffer, std::ref(atomiclightbarrier));
					usemultithread = 1;
				}catch(...){// std::async may fail gracefully here
					assert(false);
				}
			}
		}
		size_t offsets[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
		if constexpr(64 == CHAR_BIT * sizeof(T)){
			if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
			}else{// 64-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				do{
					U curhi{input[i]};
					U curlo{input[i - 1]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							curhi, buffer + i,
							curlo, buffer + i - 1);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					U curhi6{curhi >> (48 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i] = static_cast<T>(curhi);
					curhi >>= 56;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					curhi6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curhi5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curhi6);
					++offsets[7 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					U curlo6{curlo >> (48 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i - 1] = static_cast<T>(curlo);
					curlo >>= 56;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					curlo6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curlo5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curlo6);
					++offsets[7 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					U cur{input[0]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, buffer);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					U cur5{cur >> (40 - log2ptrs)};
					U cur6{cur >> (48 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[0] = static_cast<T>(cur);
					cur >>= 56;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					cur5 &= sizeof(void *) * 0xFFu;
					cur6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + cur5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + cur6);
					++offsets[7 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(56 == CHAR_BIT * sizeof(T)){
			if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
			}else{// 56-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				do{
					U curhi{input[i]};
					U curlo{input[i - 1]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							curhi, buffer + i,
							curlo, buffer + i - 1);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i] = static_cast<T>(curhi);
					curhi >>= 48;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curhi5);
					++offsets[6 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i - 1] = static_cast<T>(curlo);
					curlo >>= 48;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curlo5);
					++offsets[6 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					U cur{input[0]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, buffer);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					U cur5{cur >> (40 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[0] = static_cast<T>(cur);
					cur >>= 48;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					cur5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + cur5);
					++offsets[6 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(48 == CHAR_BIT * sizeof(T)){
			if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
			}else{// 48-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				do{
					U curhi{input[i]};
					U curlo{input[i - 1]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							curhi, buffer + i,
							curlo, buffer + i - 1);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i] = static_cast<T>(curhi);
					curhi >>= 40;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++offsets[5 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i - 1] = static_cast<T>(curlo);
					curlo >>= 40;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++offsets[5 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					U cur{input[0]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, buffer);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[0] = static_cast<T>(cur);
					cur >>= 40;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++offsets[5 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(40 == CHAR_BIT * sizeof(T)){
			if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
			}else{// 40-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				do{
					U curhi{input[i]};
					U curlo{input[i - 1]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							curhi, buffer + i,
							curlo, buffer + i - 1);
					}
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i] = static_cast<T>(curhi);
					curhi >>= 32;
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i - 1] = static_cast<T>(curlo);
					curlo >>= 32;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++offsets[4 * 256 + static_cast<size_t>(curhi)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++offsets[4 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					U cur{input[0]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, buffer);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[0] = static_cast<T>(cur);
					cur >>= 32;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++offsets[4 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(32 == CHAR_BIT * sizeof(T)){
			if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
			}else{// 32-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				do{
					U cura{input[i]};
					U curb{input[i - 1]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							cura, buffer + i,
							curb, buffer + i - 1);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					U cur2a{cura >> (16 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i] = static_cast<T>(cura);
					cura >>= 24;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					U cur2b{curb >> (16 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i - 1] = static_cast<T>(curb);
					curb >>= 24;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					cur2a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					cur2b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2a);
					++offsets[3 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2b);
					++offsets[3 * 256 + static_cast<size_t>(curb)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					U cur{input[0]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, buffer);
					}
					U cur0{cur & 0xFFu};
					U cur1{static_cast<unsigned>(cur) >> (8 - log2ptrs)};
					U cur2{static_cast<unsigned>(cur) >> (16 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[0] = static_cast<T>(cur);
					cur >>= 24;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++offsets[3 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(24 == CHAR_BIT * sizeof(T)){
			if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
			}else{// 24-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 3) / 6) * 3;
				i -= 2;
				if(0 <= i)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
					[[likely]]
#endif
					do{
					U cura{input[i + 2]};
					U curb{input[i + 1]};
					U curc{input[i]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							cura, buffer + i + 2,
							curb, buffer + i + 1,
							curc, buffer + i);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i + 3] = static_cast<T>(cura);
					cura >>= 16;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i + 2] = static_cast<T>(curb);
					curb >>= 16;
					U cur0c{curc & 0xFFu};
					U cur1c{curc >> (8 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i + 1] = static_cast<T>(curc);
					curc >>= 16;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[cur0c];
					cur1c &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++offsets[2 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++offsets[2 * 256 + static_cast<size_t>(curb)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1c);
					++offsets[2 * 256 + static_cast<size_t>(curc)];
					i -= 3;
				}while(0 <= i);
				if(2 & i){// fill in the final two items for a remainder of 2 or 3
					U cura{input[i + 2]};
					U curb{input[i + 1]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							cura, buffer + i + 2,
							curb, buffer + i + 1);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i + 2] = static_cast<T>(cura);
					cura >>= 16;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i + 1] = static_cast<T>(curb);
					curb >>= 16;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++offsets[2 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++offsets[2 * 256 + static_cast<size_t>(curb)];
				}
				if(1 & i){// fill in the final item for odd counts
					U cur{input[0]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, buffer);
					}
					U cur0{cur & 0xFFu};
					U cur1{static_cast<unsigned>(cur) >> (8 - log2ptrs)};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[0] = static_cast<T>(cur);
					cur >>= 16;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++offsets[2 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(16 == CHAR_BIT * sizeof(T)){
			if constexpr(false){// useless when not handling indirection: isrevorder){// also reverse the array at the same time
			}else{// 16-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 4) >> 3) * 4;
				i -= 3;
				if(0 <= i)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
					[[likely]]
#endif
					do{
					U cura{input[i + 3]};
					U curb{input[i + 2]};
					U curc{input[i + 1]};
					U curd{input[i]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							cura, buffer + i + 3,
							curb, buffer + i + 2,
							curc, buffer + i + 1,
							curd, buffer + i);
					}
					U cur0a{cura & 0xFFu};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i + 3] = static_cast<T>(cura);
					cura >>= 8;
					U cur0b{curb & 0xFFu};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i + 2] = static_cast<T>(curb);
					curb >>= 8;
					U cur0c{curc & 0xFFu};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i + 1] = static_cast<T>(curc);
					curc >>= 8;
					U cur0d{curd & 0xFFu};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i] = static_cast<T>(curd);
					curd >>= 8;
					++offsets[cur0a];
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[cur0c];
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++offsets[cur0d];
					if constexpr(isabsvalue && issignmode && isfltpmode) curd &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cura)];
					++offsets[256 + static_cast<size_t>(curb)];
					++offsets[256 + static_cast<size_t>(curc)];
					++offsets[256 + static_cast<size_t>(curd)];
					i -= 4;
				}while(0 <= i);
				if(2 & i){// fill in the final two items for a remainder of 2 or 3
					U cura{input[i + 3]};
					U curb{input[i + 2]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(
							cura, buffer + i + 3,
							curb, buffer + i + 2);
					}
					U cur0a{cura & 0xFFu};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i + 3] = static_cast<T>(cura);
					cura >>= 8;
					U cur0b{curb & 0xFFu};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[i + 2] = static_cast<T>(curb);
					curb >>= 8;
					++offsets[cur0a];
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cura)];
					++offsets[256 + static_cast<size_t>(curb)];
				}
				if(1 & i){// fill in the final item for odd counts
					U cur{input[0]};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, buffer);
					}
					U cur0{cur & 0xFFu};
					if constexpr(isfltpmode == isabsvalue && !(isabsvalue && !issignmode)) buffer[0] = static_cast<T>(cur);
					cur >>= 8;
					++offsets[cur0];
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cur)];
				}
			}
		}else static_assert(false, "Implementing larger types will require additional work and optimisation for this library.");

		// barrier and pointer exchange with the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, size_t *, std::nullptr_t> offsetscompanion;
		if constexpr(ismultithreadcapable){
			uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsets) & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed);
				}while(reinterpret_cast<uintptr_t>(offsets) == other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			// this will just be zero if usemultithread is zero
			offsetscompanion = reinterpret_cast<size_t *>(other);// retrieve the pointer
		}

		// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
		auto[runsteps, paritybool]{generateoffsetsmulti<isdescsort, isabsvalue, issignmode, isfltpmode, ismultithreadcapable, T>(count, offsets, offsetscompanion, usemultithread, movetobuffer)};

		// barrier and (flipped bits) runsteps, paritybool value exchange with the companion thread
		if constexpr(ismultithreadcapable){
			// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
			uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + static_cast<uintptr_t>(usemultithread)};
			while(reinterpret_cast<uintptr_t>(offsets) == atomiclightbarrier.load(std::memory_order_relaxed)){
				spinpause();// catch up
			}
			uintptr_t other{atomiclightbarrier.fetch_add(compound & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
				}while(!other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			other += compound;// combine
			unsigned lowercarryoutbits{2 * usemultithread + paritybool};
			paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
			other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
			runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
		}

		// perform the bidirectional 8-bit sorting sequence
		// flip the relevant bits inside runsteps first
		if(runsteps ^= (1u << CHAR_BIT * sizeof(T) / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
		{
			T *psrclo{input}, *pdst{buffer};
			if(paritybool){// swap if the count of sorting actions to do is odd
				psrclo = buffer;
				pdst = input;
			}
			radixsortnoallocmultimain<isabsvalue, issignmode, isfltpmode, ismultithreadcapable, T>(count, psrclo, pdst, psrclo, offsets, runsteps, usemultithread, atomiclightbarrier);
		}
	}
}

// initialisation part, multi-threading companion for the radixsortcopynoallocmulti() and radixsortnoallocmulti() function implementation templates for 80-bit-based long double types with indirection
// Platforms with a native 80-bit long double type are all little endian, hence that is the only implementation here.
// Do not use this function directly.
template<auto indirection1, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, bool isinputconst, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	(std::is_same_v<longdoubletest128, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<longdoubletest96, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<longdoubletest80, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<long double, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)),
	void> radixsortnoallocmultiinitmtc(size_t count, std::conditional_t<isinputconst, V *const *, V **> input, V *pout[], std::conditional_t<isinputconst, V **, std::nullptr_t> pdst, size_t offsetscompanion[], vararguments... varparameters)noexcept{
	using T = std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>;
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;
	using U = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t, unsigned>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(3 <= count);// this function is not for small arrays, 4 is the minimum original array count
	// do not pass a nullptr here
	assert(input);
	assert(pout);
	if(isinputconst) assert(pdst);
	assert(offsetscompanion);
	if constexpr(isrevorder){// also reverse the array at the same time
		if constexpr(isinputconst){
			// unsigned counter, not zero inclusive inside the loop
			size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
			pout += count - i;
			pdst += count - i;
			do{
				V *plo{input[0]};
				V *phi{input[1]};
				input += 2;
				pout[i] = plo;
				pdst[i] = plo;
				auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
				pout[i - 1] = phi;
				pdst[i - 1] = phi;
				auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
				auto[curmlo, curelo]{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
				auto[curmhi, curehi]{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curmlo, curelo, curmhi, curehi);
				}
				// register pressure performance issue on several platforms: first do the low half here
				unsigned curelo0{static_cast<unsigned>(curelo & 0xFFu)};
				curelo >>= 8;
				unsigned curmlo0{static_cast<unsigned>(curmlo & 0xFFu)};
				unsigned curmlo1{static_cast<unsigned>(curmlo >> (8 - log2ptrs))};
				unsigned curmlo2{static_cast<unsigned>(curmlo >> (16 - log2ptrs))};
				unsigned curmlo3{static_cast<unsigned>(curmlo >> (24 - log2ptrs))};
				unsigned curmlo4{static_cast<unsigned>(curmlo >> (32 - log2ptrs))};
				unsigned curmlo5{static_cast<unsigned>(curmlo >> (40 - log2ptrs))};
				unsigned curmlo6{static_cast<unsigned>(curmlo >> (48 - log2ptrs))};
				curmlo >>= 56;
				++offsetscompanion[8 * 256 + static_cast<size_t>(curelo0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curelo &= 0x7Fu;
				else curelo &= 0xFFu;
				++offsetscompanion[curmlo0];
				curmlo1 &= sizeof(void *) * 0xFFu;
				curmlo2 &= sizeof(void *) * 0xFFu;
				curmlo3 &= sizeof(void *) * 0xFFu;
				curmlo4 &= sizeof(void *) * 0xFFu;
				curmlo5 &= sizeof(void *) * 0xFFu;
				curmlo6 &= sizeof(void *) * 0xFFu;
				++offsetscompanion[9 * 256 + static_cast<size_t>(curelo)];
				++offsetscompanion[7 * 256 + static_cast<size_t>(curmlo)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curmlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curmlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curmlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curmlo4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curmlo5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curmlo6);
				// register pressure performance issue on several platforms: do the high half here second
				unsigned curehi0{static_cast<unsigned>(curehi & 0xFFu)};
				curehi >>= 8;
				unsigned curmhi0{static_cast<unsigned>(curmhi & 0xFFu)};
				unsigned curmhi1{static_cast<unsigned>(curmhi >> (8 - log2ptrs))};
				unsigned curmhi2{static_cast<unsigned>(curmhi >> (16 - log2ptrs))};
				unsigned curmhi3{static_cast<unsigned>(curmhi >> (24 - log2ptrs))};
				unsigned curmhi4{static_cast<unsigned>(curmhi >> (32 - log2ptrs))};
				unsigned curmhi5{static_cast<unsigned>(curmhi >> (40 - log2ptrs))};
				unsigned curmhi6{static_cast<unsigned>(curmhi >> (48 - log2ptrs))};
				curmhi >>= 56;
				++offsetscompanion[8 * 256 + static_cast<size_t>(curehi0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curehi &= 0x7Fu;
				else curehi &= 0xFFu;
				++offsetscompanion[curmhi0];
				curmhi1 &= sizeof(void *) * 0xFFu;
				curmhi2 &= sizeof(void *) * 0xFFu;
				curmhi3 &= sizeof(void *) * 0xFFu;
				curmhi4 &= sizeof(void *) * 0xFFu;
				curmhi5 &= sizeof(void *) * 0xFFu;
				curmhi6 &= sizeof(void *) * 0xFFu;
				++offsetscompanion[7 * 256 + static_cast<size_t>(curmhi)];
				++offsetscompanion[9 * 256 + static_cast<size_t>(curehi)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curmhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curmhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curmhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curmhi4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curmhi5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curmhi6);
				i -= 2;
			}while(0 <= i);
		}else{// !isinputconst
			V **pinputlo{input}, **pinputhi{input + count};
			V **poutputlo{pout}, **poutputhi{pout + count};
			size_t i{(count + 1 + 2) >> 2};// rounded up in the companion thread
			do{
				V *plo{pinputlo[0]};
				V *phi{pinputhi[0]};
				*pinputhi-- = plo;
				*poutputhi-- = plo;
				auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
				*pinputlo++ = phi;
				*poutputlo++ = phi;
				auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
				auto[curmlo, curelo]{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
				auto[curmhi, curehi]{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curmlo, curelo, curmhi, curehi);
				}
				// register pressure performance issue on several platforms: first do the low half here
				unsigned curelo0{static_cast<unsigned>(curelo & 0xFFu)};
				curelo >>= 8;
				unsigned curmlo0{static_cast<unsigned>(curmlo & 0xFFu)};
				unsigned curmlo1{static_cast<unsigned>(curmlo >> (8 - log2ptrs))};
				unsigned curmlo2{static_cast<unsigned>(curmlo >> (16 - log2ptrs))};
				unsigned curmlo3{static_cast<unsigned>(curmlo >> (24 - log2ptrs))};
				unsigned curmlo4{static_cast<unsigned>(curmlo >> (32 - log2ptrs))};
				unsigned curmlo5{static_cast<unsigned>(curmlo >> (40 - log2ptrs))};
				unsigned curmlo6{static_cast<unsigned>(curmlo >> (48 - log2ptrs))};
				curmlo >>= 56;
				++offsetscompanion[8 * 256 + static_cast<size_t>(curelo0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curelo &= 0x7Fu;
				else curelo &= 0xFFu;
				++offsetscompanion[curmlo0];
				curmlo1 &= sizeof(void *) * 0xFFu;
				curmlo2 &= sizeof(void *) * 0xFFu;
				curmlo3 &= sizeof(void *) * 0xFFu;
				curmlo4 &= sizeof(void *) * 0xFFu;
				curmlo5 &= sizeof(void *) * 0xFFu;
				curmlo6 &= sizeof(void *) * 0xFFu;
				++offsetscompanion[9 * 256 + static_cast<size_t>(curelo)];
				++offsetscompanion[7 * 256 + static_cast<size_t>(curmlo)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curmlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curmlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curmlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curmlo4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curmlo5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curmlo6);
				// register pressure performance issue on several platforms: do the high half here second
				unsigned curehi0{static_cast<unsigned>(curehi & 0xFFu)};
				curehi >>= 8;
				unsigned curmhi0{static_cast<unsigned>(curmhi & 0xFFu)};
				unsigned curmhi1{static_cast<unsigned>(curmhi >> (8 - log2ptrs))};
				unsigned curmhi2{static_cast<unsigned>(curmhi >> (16 - log2ptrs))};
				unsigned curmhi3{static_cast<unsigned>(curmhi >> (24 - log2ptrs))};
				unsigned curmhi4{static_cast<unsigned>(curmhi >> (32 - log2ptrs))};
				unsigned curmhi5{static_cast<unsigned>(curmhi >> (40 - log2ptrs))};
				unsigned curmhi6{static_cast<unsigned>(curmhi >> (48 - log2ptrs))};
				curmhi >>= 56;
				++offsetscompanion[8 * 256 + static_cast<size_t>(curehi0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curehi &= 0x7Fu;
				else curehi &= 0xFFu;
				++offsetscompanion[curmhi0];
				curmhi1 &= sizeof(void *) * 0xFFu;
				curmhi2 &= sizeof(void *) * 0xFFu;
				curmhi3 &= sizeof(void *) * 0xFFu;
				curmhi4 &= sizeof(void *) * 0xFFu;
				curmhi5 &= sizeof(void *) * 0xFFu;
				curmhi6 &= sizeof(void *) * 0xFFu;
				++offsetscompanion[7 * 256 + static_cast<size_t>(curmhi)];
				++offsetscompanion[9 * 256 + static_cast<size_t>(curehi)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curmhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curmhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curmhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curmhi4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curmhi5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curmhi6);
			}while(--i);
		}
	}else{// not in reverse order
		// unsigned counter, not zero inclusive inside the loop
		size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
		input += count - i;
		pout += count - i;
		do{
			V *phi{input[i]};
			V *plo{input[i - 1]};
			pout[i] = phi;
			pout[i - 1] = plo;
			auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
			auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
			auto[curmhi, curehi]{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
			auto[curmlo, curelo]{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
			if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
				filterinput<isabsvalue, issignmode, isfltpmode, T>(curmhi, curehi, curmlo, curelo);
			}
			// register pressure performance issue on several platforms: first do the high half here
			unsigned curehi0{static_cast<unsigned>(curehi & 0xFFu)};
			curehi >>= 8;
			unsigned curmhi0{static_cast<unsigned>(curmhi & 0xFFu)};
			unsigned curmhi1{static_cast<unsigned>(curmhi >> (8 - log2ptrs))};
			unsigned curmhi2{static_cast<unsigned>(curmhi >> (16 - log2ptrs))};
			unsigned curmhi3{static_cast<unsigned>(curmhi >> (24 - log2ptrs))};
			unsigned curmhi4{static_cast<unsigned>(curmhi >> (32 - log2ptrs))};
			unsigned curmhi5{static_cast<unsigned>(curmhi >> (40 - log2ptrs))};
			unsigned curmhi6{static_cast<unsigned>(curmhi >> (48 - log2ptrs))};
			curmhi >>= 56;
			++offsetscompanion[8 * 256 + static_cast<size_t>(curehi0)];
			curehi &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
			++offsetscompanion[curmhi0];
			curmhi1 &= sizeof(void *) * 0xFFu;
			curmhi2 &= sizeof(void *) * 0xFFu;
			curmhi3 &= sizeof(void *) * 0xFFu;
			curmhi4 &= sizeof(void *) * 0xFFu;
			curmhi5 &= sizeof(void *) * 0xFFu;
			curmhi6 &= sizeof(void *) * 0xFFu;
			++offsetscompanion[9 * 256 + static_cast<size_t>(curehi)];
			++offsetscompanion[7 * 256 + static_cast<size_t>(curmhi)];
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curmhi1);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curmhi2);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curmhi3);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curmhi4);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curmhi5);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curmhi6);
			// register pressure performance issue on several platforms: do the low half here second
			unsigned curelo0{static_cast<unsigned>(curelo & 0xFFu)};
			curelo >>= 8;
			unsigned curmlo0{static_cast<unsigned>(curmlo & 0xFFu)};
			unsigned curmlo1{static_cast<unsigned>(curmlo >> (8 - log2ptrs))};
			unsigned curmlo2{static_cast<unsigned>(curmlo >> (16 - log2ptrs))};
			unsigned curmlo3{static_cast<unsigned>(curmlo >> (24 - log2ptrs))};
			unsigned curmlo4{static_cast<unsigned>(curmlo >> (32 - log2ptrs))};
			unsigned curmlo5{static_cast<unsigned>(curmlo >> (40 - log2ptrs))};
			unsigned curmlo6{static_cast<unsigned>(curmlo >> (48 - log2ptrs))};
			curmlo >>= 56;
			++offsetscompanion[8 * 256 + static_cast<size_t>(curelo0)];
			curelo &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
			++offsetscompanion[curmlo0];
			curmlo1 &= sizeof(void *) * 0xFFu;
			curmlo2 &= sizeof(void *) * 0xFFu;
			curmlo3 &= sizeof(void *) * 0xFFu;
			curmlo4 &= sizeof(void *) * 0xFFu;
			curmlo5 &= sizeof(void *) * 0xFFu;
			curmlo6 &= sizeof(void *) * 0xFFu;
			++offsetscompanion[9 * 256 + static_cast<size_t>(curelo)];
			++offsetscompanion[7 * 256 + static_cast<size_t>(curmlo)];
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curmlo1);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curmlo2);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curmlo3);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curmlo4);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curmlo5);
			++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curmlo6);
		}while(i -= 2);
	}
}

// main part, multi-threading companion for the radixsortcopynoallocmulti() and radixsortnoallocmulti() function implementation templates for 80-bit-based long double types with indirection
// Platforms with a native 80-bit long double type are all little endian, hence that is the only implementation here.
// Do not use this function directly.
template<auto indirection1, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	(std::is_same_v<longdoubletest128, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<longdoubletest96, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<longdoubletest80, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<long double, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)),
	void> radixsortnoallocmultimainmtc(size_t count, V *const input[], V *pdst[], V *pdstnext[], size_t offsetscompanion[], unsigned runsteps, std::atomic_uintptr_t &atomiclightbarrier, vararguments... varparameters)noexcept{
	using T = std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>;
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;
	using U = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t, unsigned>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(3 <= count);// this function is not for small arrays, 4 is the minimum original array count
	// do not pass a nullptr here
	assert(input);
	assert(pdst);
	assert(pdstnext);
	assert(offsetscompanion);
	assert(runsteps);
	unsigned shifter{bitscanforwardportable(runsteps)};// at least 1 bit is set inside runsteps as by previous check
	V **psrchi;
	if constexpr(isrevorder){
		psrchi = pdstnext + count;
	}else{
		psrchi = const_cast<V **>(input) + count;// psrchi will never be written to
	}
	// skip a step if possible
	runsteps >>= shifter;
	size_t *poffset{offsetscompanion + static_cast<size_t>(shifter) * 256};
	if(80 / 8 - 2 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
		[[unlikely]]
#endif
		goto handletop16;// rare, but possible
	if(80 / 8 - 2 < shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
		[[unlikely]]
#endif
		goto handletop8;// rare, but possible
	shifter *= 8;
	for(;;){
		{
			size_t j{(count + 1 + 4) >> 3};// rounded up in the top part
			do{// fill the array, four at a time
				V *pa{psrchi[0]};
				V *pb{psrchi[-1]};
				V *pc{psrchi[-2]};
				V *pd{psrchi[-3]};
				psrchi -= 4;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
				auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
				auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				auto outc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
				auto outd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
				auto[cura, curb, curc, curd]{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd, shifter)};
				size_t offseta{poffset[cura]--};// the next item will be placed one lower
				size_t offsetb{poffset[curb]--};
				size_t offsetc{poffset[curc]--};
				size_t offsetd{poffset[curd]--};
				pdst[offseta] = pa;
				pdst[offsetb] = pb;
				pdst[offsetc] = pc;
				pdst[offsetd] = pd;
			}while(--j);
		}
		runsteps >>= 1;
		if(!runsteps)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
			break;
		{
			unsigned index{bitscanforwardportable(runsteps)};// at least 1 bit is set inside runsteps as by previous check
			shifter += 8;
			poffset += 256;
			// swap the pointers for the next round, data is moved on each iteration
			psrchi = pdst;
			uintptr_t old{atomiclightbarrier.fetch_add(~static_cast<uintptr_t>(0))};
			pdst = pdstnext;
			pdstnext = psrchi;
			psrchi += count;
			// skip a step if possible
			runsteps >>= index;
			shifter += index * 8;
			poffset += static_cast<size_t>(index) * 256;
			if(!old) do{
				spinpause();
			}while(atomiclightbarrier.load(std::memory_order_relaxed));
		}
		// handle the top two parts differently
		if(80 - 16 <= shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
		{
			if(80 - 16 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
				[[likely]]
#endif
			{
				{
handletop16:
					size_t j{(count + 1 + 4) >> 3};// rounded up in the top part
					do{// fill the array, four at a time
						V *pa{psrchi[0]};
						V *pb{psrchi[-1]};
						V *pc{psrchi[-2]};
						V *pd{psrchi[-3]};
						psrchi -= 4;
						auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
						auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
						auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
						auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
						auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
						auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
						auto outc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
						auto outd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
						auto[cura, curb, curc, curd]{filterbelowtop8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd)};
						size_t offseta{offsetscompanion[cura + (80 - 16) * 256 / 8]--};// the next item will be placed one lower
						size_t offsetb{offsetscompanion[curb + (80 - 16) * 256 / 8]--};
						size_t offsetc{offsetscompanion[curc + (80 - 16) * 256 / 8]--};
						size_t offsetd{offsetscompanion[curd + (80 - 16) * 256 / 8]--};
						pdst[offseta] = pa;
						pdst[offsetb] = pb;
						pdst[offsetc] = pc;
						pdst[offsetd] = pd;
					}while(--j);
				}
				if(1 == runsteps)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
					[[unlikely]]
#endif
					break;
				{
					uintptr_t old{atomiclightbarrier.fetch_add(~static_cast<uintptr_t>(0))};
					// swap the pointers for the next round, data is moved on each iteration
					psrchi = pdst;
					pdst = pdstnext;
					// unused: pdstnext = psrchi;
					psrchi += count;
					if(!old) do{
						spinpause();
					}while(atomiclightbarrier.load(std::memory_order_relaxed));
				}
handletop8:
				size_t j{(count + 1 + 4) >> 3};// rounded up in the top part
				do{// fill the array, four at a time
					V *pa{psrchi[0]};
					V *pb{psrchi[-1]};
					V *pc{psrchi[-2]};
					V *pd{psrchi[-3]};
					psrchi -= 4;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
					auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
					auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					auto outc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
					auto outd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
					auto[cura, curb, curc, curd]{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd)};
					size_t offseta{offsetscompanion[cura + (80 - 8) * 256 / 8]--};// the next item will be placed one lower
					size_t offsetb{offsetscompanion[curb + (80 - 8) * 256 / 8]--};
					size_t offsetc{offsetscompanion[curc + (80 - 8) * 256 / 8]--};
					size_t offsetd{offsetscompanion[curd + (80 - 8) * 256 / 8]--};
					pdst[offseta] = pa;
					pdst[offsetb] = pb;
					pdst[offsetc] = pc;
					pdst[offsetd] = pd;
				}while(--j);
				break;// no further processing beyond the top part
			}
		}
	}
}

// main part for the radixsortcopynoallocmulti() and radixsortnoallocmulti() function implementation templates for 80-bit-based long double types with indirection
// Do not use this function directly.
template<auto indirection1, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, bool ismultithreadcapable, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	(std::is_same_v<longdoubletest128, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<longdoubletest96, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<longdoubletest80, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<long double, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)),
	void> radixsortnoallocmultimain(size_t count, V *const input[], V *pdst[], V *pdstnext[], size_t offsets[], unsigned runsteps, std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread, std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t &, std::nullptr_t> atomiclightbarrier, vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	using T = tounifunsigned<std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>>;
	using U = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t, unsigned>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(count && count != MAXSIZE_T);
	// do not pass a nullptr here
	assert(input);
	assert(pdst);
	assert(pdstnext);
	assert(offsets);
	assert(runsteps);
	unsigned shifter{bitscanforwardportable(runsteps)};// at least 1 bit is set inside runsteps as by previous check
	V **psrclo;
	if constexpr(isrevorder){
		psrclo = pdstnext;
	}else{
		psrclo = const_cast<V **>(input);// psrclo will never be written to
	}
	// skip a step if possible
	runsteps >>= shifter;
	size_t *poffset{offsets + static_cast<size_t>(shifter) * 256};
	if(80 / 8 - 2 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
		[[unlikely]]
#endif
	goto handletop16;// rare, but possible
	if(80 / 8 - 2 < shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
		[[unlikely]]
#endif
	goto handletop8;// rare, but possible
	shifter *= 8;
	for(;;){
		{
			ptrdiff_t j;// rounded down in the bottom part, or no multithreading
			if constexpr(!ismultithreadcapable) j = static_cast<ptrdiff_t>((count + 1) >> 2);
			else j = static_cast<ptrdiff_t>((count + 1) >> (2 + usemultithread));
			while(0 <= --j){// fill the array, four at a time
				V *pa{psrclo[0]};
				V *pb{psrclo[1]};
				V *pc{psrclo[2]};
				V *pd{psrclo[3]};
				psrclo += 4;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
				auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
				auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				auto outc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
				auto outd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
				auto[cura, curb, curc, curd]{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd, shifter)};
				size_t offseta{poffset[cura]++};// the next item will be placed one higher
				size_t offsetb{poffset[curb]++};
				size_t offsetc{poffset[curc]++};
				size_t offsetd{poffset[curd]++};
				pdst[offseta] = pa;
				pdst[offsetb] = pb;
				pdst[offsetc] = pc;
				pdst[offsetd] = pd;
			}
		}
		if(2 & count + 1){// fill in the final two items for a remainder of 2 or 3
			V *pa{psrclo[0]};
			V *pb{psrclo[1]};
			psrclo += 2;
			auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
			auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
			auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
			auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
			auto[cura, curb]{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, shifter)};
			size_t offseta{poffset[cura]++};// the next item will be placed one higher
			size_t offsetb{poffset[curb]++};
			pdst[offseta] = pa;
			pdst[offsetb] = pb;
		}
		if(!(1 & count)){// fill in the final item for odd counts
			V *p{psrclo[0]};
			auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
			auto out{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
			size_t cur{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(out, shifter)};
			size_t offset{poffset[cur]};
			pdst[offset] = p;
		}
		runsteps >>= 1;
		if(!runsteps)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
			break;
		{
			unsigned index{bitscanforwardportable(runsteps)};// at least 1 bit is set inside runsteps as by previous check
			shifter += 8;
			poffset += 256;
			// swap the pointers for the next round, data is moved on each iteration
			psrclo = pdst;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
			[[maybe_unused]]
#endif
			uintptr_t old;
			if constexpr(ismultithreadcapable) old = atomiclightbarrier.fetch_add(usemultithread);
			pdst = pdstnext;
			pdstnext = psrclo;
			// skip a step if possible
			runsteps >>= index;
			shifter += index * 8;
			poffset += static_cast<size_t>(index) * 256;
			if constexpr(ismultithreadcapable) if(old < usemultithread) do{
				spinpause();
			}while(atomiclightbarrier.load(std::memory_order_relaxed));
		}
		// handle the top two parts differently
		if(80 - 16 <= shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
		{
			if(80 - 16 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
				[[likely]]
#endif
			{
				{
handletop16:
					ptrdiff_t j;// rounded down in the bottom part, or no multithreading
					if constexpr(!ismultithreadcapable) j = static_cast<ptrdiff_t>((count + 1) >> 2);
					else j = static_cast<ptrdiff_t>((count + 1) >> (2 + usemultithread));
					while(0 <= --j){// fill the array, four at a time
						V *pa{psrclo[0]};
						V *pb{psrclo[1]};
						V *pc{psrclo[2]};
						V *pd{psrclo[3]};
						psrclo += 4;
						auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
						auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
						auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
						auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
						auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
						auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
						auto outc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
						auto outd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
						auto[cura, curb, curc, curd]{filterbelowtop8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd)};
						size_t offseta{offsets[cura + (80 - 16) * 256 / 8]++};// the next item will be placed one higher
						size_t offsetb{offsets[curb + (80 - 16) * 256 / 8]++};
						size_t offsetc{offsets[curc + (80 - 16) * 256 / 8]++};
						size_t offsetd{offsets[curd + (80 - 16) * 256 / 8]++};
						pdst[offseta] = pa;
						pdst[offsetb] = pb;
						pdst[offsetc] = pc;
						pdst[offsetd] = pd;
					}
				}
			}
			if(2 & count + 1){// fill in the final two items for a remainder of 2 or 3
				V *pa{psrclo[0]};
				V *pb{psrclo[1]};
				psrclo += 2;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				auto[cura, curb]{filterbelowtop8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb)};
				size_t offseta{offsets[cura + (80 - 16) * 256 / 8]++};// the next item will be placed one higher
				size_t offsetb{offsets[curb + (80 - 16) * 256 / 8]++};
				pdst[offseta] = pa;
				pdst[offsetb] = pb;
			}
			if(!(1 & count)){// fill in the final item for odd counts
				V *p{psrclo[0]};
				auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
				auto out{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
				size_t cur{filterbelowtop8<isabsvalue, issignmode, isfltpmode, T, U>(out)};
				size_t offset{offsets[cur + (80 - 16) * 256 / 8]};
				pdst[offset] = p;
			}
			if(1 == runsteps)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
				[[unlikely]]
#endif
				break;
			{
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
				[[maybe_unused]]
#endif
				uintptr_t old;
				if constexpr(ismultithreadcapable) old = atomiclightbarrier.fetch_add(usemultithread);
				// swap the pointers for the next round, data is moved on each iteration
				psrclo = pdst;
				pdst = pdstnext;
				// unused: pdstnext = psrclo;
				if constexpr(ismultithreadcapable) if(old < usemultithread) do{
					spinpause();
				}while(atomiclightbarrier.load(std::memory_order_relaxed));
			}
handletop8:
			ptrdiff_t j;// rounded down in the bottom part, or no multithreading
			if constexpr(!ismultithreadcapable) j = static_cast<ptrdiff_t>((count + 1) >> 2);
			else j = static_cast<ptrdiff_t>((count + 1) >> (2 + usemultithread));
			while(0 <= --j){// fill the array, four at a time
				V *pa{psrclo[0]};
				V *pb{psrclo[1]};
				V *pc{psrclo[2]};
				V *pd{psrclo[3]};
				psrclo += 4;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
				auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
				auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				auto outc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
				auto outd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
				auto[cura, curb, curc, curd]{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd)};
				size_t offseta{offsets[cura + (80 - 8) * 256 / 8]++};// the next item will be placed one higher
				size_t offsetb{offsets[curb + (80 - 8) * 256 / 8]++};
				size_t offsetc{offsets[curc + (80 - 8) * 256 / 8]++};
				size_t offsetd{offsets[curd + (80 - 8) * 256 / 8]++};
				pdst[offseta] = pa;
				pdst[offsetb] = pb;
				pdst[offsetc] = pc;
				pdst[offsetd] = pd;
			}
			if(2 & count + 1){// fill in the final two items for a remainder of 2 or 3
				V *pa{psrclo[0]};
				V *pb{psrclo[1]};
				psrclo += 2;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				auto[cura, curb]{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb)};
				size_t offseta{offsets[cura + (80 - 8) * 256 / 8]++};// the next item will be placed one higher
				size_t offsetb{offsets[curb + (80 - 8) * 256 / 8]++};
				pdst[offseta] = pa;
				pdst[offsetb] = pb;
			}
			if(!(1 & count)){// fill in the final item for odd counts
				V *p{psrclo[0]};
				auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
				auto out{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
				size_t cur{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(out)};
				size_t offset{offsets[cur + (80 - 8) * 256 / 8]};
				pdst[offset] = p;
			}
			break;// no further processing beyond the top part
		}
	}
}

// multi-threading companion for the radixsortcopynoallocmulti() function implementation template for 80-bit-based long double types with indirection
// Do not use this function directly.
template<auto indirection1, bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	(std::is_same_v<longdoubletest128, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<longdoubletest96, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<longdoubletest80, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<long double, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)),
	void> radixsortcopynoallocmultimtc(size_t count, V *const input[], V *output[], V *buffer[], std::atomic_uintptr_t &atomiclightbarrier, vararguments... varparameters)noexcept{
	using T = std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>;
	// do not pass a nullptr here
	assert(input);
	assert(output);
	assert(buffer);
	static size_t constexpr offsetsstride{80 * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
	size_t offsetscompanion[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
	radixsortnoallocmultiinitmtc<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, true, V>(count, input, output, buffer, offsetscompanion, varparameters...);

	size_t *offsets;
	{// barrier and pointer exchange with the main thread
		uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsetscompanion))};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed);
			}while(reinterpret_cast<uintptr_t>(offsetscompanion) == other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		offsets = reinterpret_cast<size_t *>(other);// retrieve the pointer
	}

	// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
	auto[runsteps, paritybool]{generateoffsetsmultimtc<isdescsort, isabsvalue, issignmode, isfltpmode, T>(count, offsets, offsetscompanion)};

	{// barrier and (flipped bits) runsteps, paritybool value exchange with the main thread
		// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
		uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + 1};
		while(reinterpret_cast<uintptr_t>(offsetscompanion) == atomiclightbarrier.load(std::memory_order_relaxed)){
			spinpause();// catch up
		}
		uintptr_t other{atomiclightbarrier.fetch_add(compound)};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
			}while(!other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		other += compound;// combine
		unsigned lowercarryoutbits{2 + paritybool};
		paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
		other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
		runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
	}

	// perform the bidirectional 8-bit sorting sequence
	// flip the relevant bits inside runsteps first
	if(runsteps ^= (1u << 80 / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
		[[likely]]
#endif
	{// perform the bidirectional 8-bit sorting sequence
		V **pdst{buffer}, **pdstnext{output};// for the next iteration
		if(paritybool){// swap if the count of sorting actions to do is odd
			pdst = output;
			pdstnext = buffer;
		}
		radixsortnoallocmultimainmtc<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V>(count, input, pdst, pdstnext, offsetscompanion, runsteps, atomiclightbarrier, varparameters...);
	}
}

// radixsortcopynoalloc() function implementation template for 80-bit-based long double types with indirection
// Platforms with a native 80-bit long double type are all little endian, hence that is the only implementation here.
template<auto indirection1, bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_NORMAL std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	(std::is_same_v<longdoubletest128, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<longdoubletest96, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<longdoubletest80, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<long double, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)),
	void> radixsortcopynoallocmulti(size_t count, V *const input[], V *output[], V *buffer[], vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	using T = std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>;
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;
	using U = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t, unsigned>;// assume zero-extension to be basically free for U on basically all modern machines
	static bool constexpr ismultithreadcapable{
#ifdef RSBD8_DISABLE_MULTITHREADING
		false
#else
		std::is_nothrow_invocable_v<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>
#endif
	};
	// do not pass a nullptr here, even though it's safe if count is 0
	assert(input);
	assert(output);
	assert(buffer);
	// All the code in this function is adapted for count to be one below its input value here.
	--count;
	if(0 < static_cast<ptrdiff_t>(count)){// a 0 or 1 count array is legal here
		static size_t constexpr offsetsstride{80 * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
		// conditionally enable multi-threading here
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread;// filled in as a boolean 0 or 1, used as unsigned input later on
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t, std::nullptr_t> atomiclightbarrier;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::future<void>, std::nullptr_t> asynchandle;

		// count the 256 configurations, all in one go
		if constexpr(ismultithreadcapable){
			usemultithread = 0;
			// TODO: fine-tune, right now the threshold is set to the 7-bit limit (the minimum is 15)
			if(0x7Fu < count && 1 < std::thread::hardware_concurrency()){
				try{
					asynchandle = std::async(std::launch::async, radixsortcopynoallocmultimtc<indirection1, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V, vararguments...>, count, input, output, buffer, std::ref(atomiclightbarrier), varparameters...);
					usemultithread = 1;
				}catch(...){// std::async may fail gracefully here
					assert(false);
				}
			}
		}
		size_t offsets[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
		ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, ptrdiff_t, std::nullptr_t> stride;
		if constexpr(ismultithreadcapable){
			stride = -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
			i -= stride;
		}
		if constexpr(isrevorder){// also reverse the array at the same time
			V *const *pinput{input};
			if constexpr(ismultithreadcapable) pinput += stride;
			do{
				V *plo{pinput[0]};
				V *phi{pinput[1]};
				pinput += 2;
				output[i] = plo;
				buffer[i] = plo;
				auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
				output[i - 1] = phi;
				buffer[i - 1] = phi;
				auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
				auto[curmlo, curelo]{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
				auto[curmhi, curehi]{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curmlo, curelo, curmhi, curehi);
				}
				// register pressure performance issue on several platforms: first do the low half here
				unsigned curelo0{static_cast<unsigned>(curelo & 0xFFu)};
				curelo >>= 8;
				unsigned curmlo0{static_cast<unsigned>(curmlo & 0xFFu)};
				unsigned curmlo1{static_cast<unsigned>(curmlo >> (8 - log2ptrs))};
				unsigned curmlo2{static_cast<unsigned>(curmlo >> (16 - log2ptrs))};
				unsigned curmlo3{static_cast<unsigned>(curmlo >> (24 - log2ptrs))};
				unsigned curmlo4{static_cast<unsigned>(curmlo >> (32 - log2ptrs))};
				unsigned curmlo5{static_cast<unsigned>(curmlo >> (40 - log2ptrs))};
				unsigned curmlo6{static_cast<unsigned>(curmlo >> (48 - log2ptrs))};
				curmlo >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(curelo0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curelo &= 0x7Fu;
				else curelo &= 0xFFu;
				++offsets[curmlo0];
				curmlo1 &= sizeof(void *) * 0xFFu;
				curmlo2 &= sizeof(void *) * 0xFFu;
				curmlo3 &= sizeof(void *) * 0xFFu;
				curmlo4 &= sizeof(void *) * 0xFFu;
				curmlo5 &= sizeof(void *) * 0xFFu;
				curmlo6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(curelo)];
				++offsets[7 * 256 + static_cast<size_t>(curmlo)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curmlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curmlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curmlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curmlo4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curmlo5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curmlo6);
				// register pressure performance issue on several platforms: do the high half here second
				unsigned curehi0{static_cast<unsigned>(curehi & 0xFFu)};
				curehi >>= 8;
				unsigned curmhi0{static_cast<unsigned>(curmhi & 0xFFu)};
				unsigned curmhi1{static_cast<unsigned>(curmhi >> (8 - log2ptrs))};
				unsigned curmhi2{static_cast<unsigned>(curmhi >> (16 - log2ptrs))};
				unsigned curmhi3{static_cast<unsigned>(curmhi >> (24 - log2ptrs))};
				unsigned curmhi4{static_cast<unsigned>(curmhi >> (32 - log2ptrs))};
				unsigned curmhi5{static_cast<unsigned>(curmhi >> (40 - log2ptrs))};
				unsigned curmhi6{static_cast<unsigned>(curmhi >> (48 - log2ptrs))};
				curmhi >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(curehi0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curehi &= 0x7Fu;
				else curehi &= 0xFFu;
				++offsets[curmhi0];
				curmhi1 &= sizeof(void *) * 0xFFu;
				curmhi2 &= sizeof(void *) * 0xFFu;
				curmhi3 &= sizeof(void *) * 0xFFu;
				curmhi4 &= sizeof(void *) * 0xFFu;
				curmhi5 &= sizeof(void *) * 0xFFu;
				curmhi6 &= sizeof(void *) * 0xFFu;
				++offsets[7 * 256 + static_cast<size_t>(curmhi)];
				++offsets[9 * 256 + static_cast<size_t>(curehi)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curmhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curmhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curmhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curmhi4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curmhi5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curmhi6);
				i -= 2;
			}while(0 < i);
			if(!(1 & i)){// fill in the final item for odd counts
				V *p{pinput[0]};
				output[0] = p;
				buffer[0] = p;
				auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
				auto[curm, cure]{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curm, cure);
				}
				unsigned cure0{static_cast<unsigned>(cure & 0xFFu)};
				cure >>= 8;
				unsigned curm0{static_cast<unsigned>(curm & 0xFFu)};
				unsigned curm1{static_cast<unsigned>(curm >> (8 - log2ptrs))};
				unsigned curm2{static_cast<unsigned>(curm >> (16 - log2ptrs))};
				unsigned curm3{static_cast<unsigned>(curm >> (24 - log2ptrs))};
				unsigned curm4{static_cast<unsigned>(curm >> (32 - log2ptrs))};
				unsigned curm5{static_cast<unsigned>(curm >> (40 - log2ptrs))};
				unsigned curm6{static_cast<unsigned>(curm >> (48 - log2ptrs))};
				curm >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(cure0)];
				cure &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
				++offsets[curm0];
				curm1 &= sizeof(void *) * 0xFFu;
				curm2 &= sizeof(void *) * 0xFFu;
				curm3 &= sizeof(void *) * 0xFFu;
				curm4 &= sizeof(void *) * 0xFFu;
				curm5 &= sizeof(void *) * 0xFFu;
				curm6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(cure)];
				++offsets[7 * 256 + static_cast<size_t>(curm)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curm1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curm2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curm3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curm4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curm5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curm6);
			}
		}else{// not in reverse order
			do{
				V *plo{input[i]};
				V *phi{input[i - 1]};
				output[i] = plo;
				auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
				output[i - 1] = phi;
				auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
				auto[curmlo, curelo]{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
				auto[curmhi, curehi]{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curmlo, curelo, curmhi, curehi);
				}
				// register pressure performance issue on several platforms: first do the low half here
				unsigned curelo0{static_cast<unsigned>(curelo & 0xFFu)};
				curelo >>= 8;
				unsigned curmlo0{static_cast<unsigned>(curmlo & 0xFFu)};
				unsigned curmlo1{static_cast<unsigned>(curmlo >> (8 - log2ptrs))};
				unsigned curmlo2{static_cast<unsigned>(curmlo >> (16 - log2ptrs))};
				unsigned curmlo3{static_cast<unsigned>(curmlo >> (24 - log2ptrs))};
				unsigned curmlo4{static_cast<unsigned>(curmlo >> (32 - log2ptrs))};
				unsigned curmlo5{static_cast<unsigned>(curmlo >> (40 - log2ptrs))};
				unsigned curmlo6{static_cast<unsigned>(curmlo >> (48 - log2ptrs))};
				curmlo >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(curelo0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curelo &= 0x7Fu;
				else curelo &= 0xFFu;
				++offsets[curmlo0];
				curmlo1 &= sizeof(void *) * 0xFFu;
				curmlo2 &= sizeof(void *) * 0xFFu;
				curmlo3 &= sizeof(void *) * 0xFFu;
				curmlo4 &= sizeof(void *) * 0xFFu;
				curmlo5 &= sizeof(void *) * 0xFFu;
				curmlo6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(curelo)];
				++offsets[7 * 256 + static_cast<size_t>(curmlo)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curmlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curmlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curmlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curmlo4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curmlo5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curmlo6);
				// register pressure performance issue on several platforms: do the high half here second
				unsigned curehi0{static_cast<unsigned>(curehi & 0xFFu)};
				curehi >>= 8;
				unsigned curmhi0{static_cast<unsigned>(curmhi & 0xFFu)};
				unsigned curmhi1{static_cast<unsigned>(curmhi >> (8 - log2ptrs))};
				unsigned curmhi2{static_cast<unsigned>(curmhi >> (16 - log2ptrs))};
				unsigned curmhi3{static_cast<unsigned>(curmhi >> (24 - log2ptrs))};
				unsigned curmhi4{static_cast<unsigned>(curmhi >> (32 - log2ptrs))};
				unsigned curmhi5{static_cast<unsigned>(curmhi >> (40 - log2ptrs))};
				unsigned curmhi6{static_cast<unsigned>(curmhi >> (48 - log2ptrs))};
				curmhi >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(curehi0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curehi &= 0x7Fu;
				else curehi &= 0xFFu;
				++offsets[curmhi0];
				curmhi1 &= sizeof(void *) * 0xFFu;
				curmhi2 &= sizeof(void *) * 0xFFu;
				curmhi3 &= sizeof(void *) * 0xFFu;
				curmhi4 &= sizeof(void *) * 0xFFu;
				curmhi5 &= sizeof(void *) * 0xFFu;
				curmhi6 &= sizeof(void *) * 0xFFu;
				++offsets[7 * 256 + static_cast<size_t>(curmhi)];
				++offsets[9 * 256 + static_cast<size_t>(curehi)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curmhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curmhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curmhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curmhi4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curmhi5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curmhi6);
				i -= 2;
			}while(0 < i);
			if(!(1 & i)){// fill in the final item for odd counts
				V *p{input[0]};
				output[0] = p;
				auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
				auto[curm, cure]{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curm, cure);
				}
				unsigned cure0{static_cast<unsigned>(cure & 0xFFu)};
				cure >>= 8;
				unsigned curm0{static_cast<unsigned>(curm & 0xFFu)};
				unsigned curm1{static_cast<unsigned>(curm >> (8 - log2ptrs))};
				unsigned curm2{static_cast<unsigned>(curm >> (16 - log2ptrs))};
				unsigned curm3{static_cast<unsigned>(curm >> (24 - log2ptrs))};
				unsigned curm4{static_cast<unsigned>(curm >> (32 - log2ptrs))};
				unsigned curm5{static_cast<unsigned>(curm >> (40 - log2ptrs))};
				unsigned curm6{static_cast<unsigned>(curm >> (48 - log2ptrs))};
				curm >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(cure0)];
				cure &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
				++offsets[curm0];
				curm1 &= sizeof(void *) * 0xFFu;
				curm2 &= sizeof(void *) * 0xFFu;
				curm3 &= sizeof(void *) * 0xFFu;
				curm4 &= sizeof(void *) * 0xFFu;
				curm5 &= sizeof(void *) * 0xFFu;
				curm6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(cure)];
				++offsets[7 * 256 + static_cast<size_t>(curm)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curm1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curm2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curm3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curm4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curm5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curm6);
			}
		}

		// barrier and pointer exchange with the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, size_t *, std::nullptr_t> offsetscompanion;
		if constexpr(ismultithreadcapable){
			uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsets) & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed);
				}while(reinterpret_cast<uintptr_t>(offsets) == other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			// this will just be zero if usemultithread is zero
			offsetscompanion = reinterpret_cast<size_t *>(other);// retrieve the pointer
		}

		// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
		auto[runsteps, paritybool]{generateoffsetsmulti<isdescsort, isabsvalue, issignmode, isfltpmode, ismultithreadcapable, T>(count, offsets, offsetscompanion, usemultithread)};

		// barrier and (flipped bits) runsteps, paritybool value exchange with the companion thread
		if constexpr(ismultithreadcapable){
			// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
			uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + static_cast<uintptr_t>(usemultithread)};
			while(reinterpret_cast<uintptr_t>(offsets) == atomiclightbarrier.load(std::memory_order_relaxed)){
				spinpause();// catch up
			}
			uintptr_t other{atomiclightbarrier.fetch_add(compound & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
				}while(!other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			other += compound;// combine
			unsigned lowercarryoutbits{2 * usemultithread + paritybool};
			paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
			other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
			runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
		}

		// perform the bidirectional 8-bit sorting sequence
		// flip the relevant bits inside runsteps first
		if(runsteps ^= (1u << 80 / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
		{
			V **pdst{buffer}, **pdstnext{output};// for the next iteration
			if(paritybool){// swap if the count of sorting actions to do is odd
				pdst = output;
				pdstnext = buffer;
			}
			radixsortnoallocmultimain<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, ismultithreadcapable, V>(count, input, pdst, pdstnext, offsets, runsteps, usemultithread, atomiclightbarrier, varparameters...);
		}
	}
}

// multi-threading companion for the radixsortnoallocmulti() function implementation template for 80-bit-based long double types with indirection
// Do not use this function directly.
template<auto indirection1, bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	(std::is_same_v<longdoubletest128, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<longdoubletest96, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<longdoubletest80, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<long double, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)),
	void> radixsortnoallocmultimtc(size_t count, V *input[], V *buffer[], std::atomic_uintptr_t &atomiclightbarrier, vararguments... varparameters)noexcept{
	using T = std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>;
	// do not pass a nullptr here
	assert(input);
	assert(buffer);
	static size_t constexpr offsetsstride{80 * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
	size_t offsetscompanion[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
	radixsortnoallocmultiinitmtc<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, false, V>(count, input, buffer, nullptr, offsetscompanion, varparameters...);

	size_t *offsets;
	{// barrier and pointer exchange with the main thread
		uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsetscompanion))};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed);
			}while(reinterpret_cast<uintptr_t>(offsetscompanion) == other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		offsets = reinterpret_cast<size_t *>(other);// retrieve the pointer
	}

	// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
	auto[runsteps, paritybool]{generateoffsetsmultimtc<isdescsort, isabsvalue, issignmode, isfltpmode, T>(count, offsets, offsetscompanion)};

	{// barrier and (flipped bits) runsteps, paritybool value exchange with the main thread
		// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
		uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + 1};
		while(reinterpret_cast<uintptr_t>(offsetscompanion) == atomiclightbarrier.load(std::memory_order_relaxed)){
			spinpause();// catch up
		}
		uintptr_t other{atomiclightbarrier.fetch_add(compound)};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
			}while(!other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		other += compound;// combine
		unsigned lowercarryoutbits{2 + paritybool};
		paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
		other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
		runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
	}

	// perform the bidirectional 8-bit sorting sequence
	// flip the relevant bits inside runsteps first
	if(runsteps ^= (1u << 80 / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
		[[likely]]
#endif
	{// perform the bidirectional 8-bit sorting sequence
		V **psrclo{input}, **pdst{buffer};
		if(paritybool){// swap if the count of sorting actions to do is odd
			psrclo = buffer;
			pdst = input;
		}
		radixsortnoallocmultimainmtc<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V>(count, psrclo, pdst, psrclo, offsetscompanion, runsteps, atomiclightbarrier, varparameters...);
	}
}

// radixsortnoalloc() function implementation template for 80-bit-based long double types with indirection
// Platforms with a native 80-bit long double type are all little endian, hence that is the only implementation here.
template<auto indirection1, bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_NORMAL std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	(std::is_same_v<longdoubletest128, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<longdoubletest96, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<longdoubletest80, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> ||
	std::is_same_v<long double, std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>> &&
	64 == LDBL_MANT_DIG &&
	16384 == LDBL_MAX_EXP &&
	128 >= CHAR_BIT * sizeof(long double) &&
	64 < CHAR_BIT * sizeof(long double)),
	void> radixsortnoallocmulti(size_t count, V *input[], V *buffer[], bool movetobuffer = false, vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	using T = std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>;
	using W = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t,
		std::conditional_t<96 == CHAR_BIT * sizeof(T), uint_least32_t,
		std::conditional_t<80 == CHAR_BIT * sizeof(T), uint_least16_t, void>>>;
	using U = std::conditional_t<128 == CHAR_BIT * sizeof(T), uint_least64_t, unsigned>;// assume zero-extension to be basically free for U on basically all modern machines
	static bool constexpr ismultithreadcapable{
#ifdef RSBD8_DISABLE_MULTITHREADING
		false
#else
		std::is_nothrow_invocable_v<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>
#endif
	};
	// do not pass a nullptr here, even though it's safe if count is 0
	assert(input);
	assert(buffer);
	// All the code in this function is adapted for count to be one below its input value here.
	--count;
	if(0 < static_cast<ptrdiff_t>(count)){// a 0 or 1 count array is legal here
		static size_t constexpr offsetsstride{80 * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
		// conditionally enable multi-threading here
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread;// filled in as a boolean 0 or 1, used as unsigned input later on
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t, std::nullptr_t> atomiclightbarrier;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::future<void>, std::nullptr_t> asynchandle;

		// count the 256 configurations, all in one go
		// conditionally enable multi-threading here if indirection1 allows noexcept
		if constexpr(ismultithreadcapable){
			usemultithread = 0;
			// TODO: fine-tune, right now the threshold is set to the 7-bit limit (the minimum is 1 to 7, depending on the size of T)
			if(0x7Fu < count && 1 < std::thread::hardware_concurrency()){
				try{
					asynchandle = std::async(std::launch::async, radixsortnoallocmultimtc<indirection1, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V, vararguments...>, count, input, buffer, std::ref(atomiclightbarrier), varparameters...);
					usemultithread = 1;
				}catch(...){// std::async may fail gracefully here
					assert(false);
				}
			}
		}
		size_t offsets[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
		if constexpr(isrevorder){// also reverse the array at the same time
			V **pinputlo, **pinputhi, **pbufferlo, **pbufferhi;
			if constexpr(!ismultithreadcapable){
				pinputlo = input;
				pinputhi = input + count;
				pbufferlo = buffer;
				pbufferhi = buffer + count;
			}else{// if mulitithreaded, the half count will be rounded up in the companion thread
				ptrdiff_t stride{-static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2)};
				pinputlo = input + stride;
				pinputhi = input + (count - stride);
				pbufferlo = buffer + stride;
				pbufferhi = buffer + (count - stride);
			}
			do{
				V *plo{pinputlo[0]};
				V *phi{pinputhi[0]};
				*pinputhi-- = plo;
				*pbufferhi-- = plo;
				auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
				*pinputlo++ = phi;
				*pbufferlo++ = phi;
				auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
				auto[curmlo, curelo]{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
				auto[curmhi, curehi]{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curmlo, curelo, curmhi, curehi);
				}
				// register pressure performance issue on several platforms: first do the low half here
				unsigned curelo0{static_cast<unsigned>(curelo & 0xFFu)};
				curelo >>= 8;
				unsigned curmlo0{static_cast<unsigned>(curmlo & 0xFFu)};
				unsigned curmlo1{static_cast<unsigned>(curmlo >> (8 - log2ptrs))};
				unsigned curmlo2{static_cast<unsigned>(curmlo >> (16 - log2ptrs))};
				unsigned curmlo3{static_cast<unsigned>(curmlo >> (24 - log2ptrs))};
				unsigned curmlo4{static_cast<unsigned>(curmlo >> (32 - log2ptrs))};
				unsigned curmlo5{static_cast<unsigned>(curmlo >> (40 - log2ptrs))};
				unsigned curmlo6{static_cast<unsigned>(curmlo >> (48 - log2ptrs))};
				curmlo >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(curelo0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curelo &= 0x7Fu;
				else curelo &= 0xFFu;
				++offsets[curmlo0];
				curmlo1 &= sizeof(void *) * 0xFFu;
				curmlo2 &= sizeof(void *) * 0xFFu;
				curmlo3 &= sizeof(void *) * 0xFFu;
				curmlo4 &= sizeof(void *) * 0xFFu;
				curmlo5 &= sizeof(void *) * 0xFFu;
				curmlo6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(curelo)];
				++offsets[7 * 256 + static_cast<size_t>(curmlo)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curmlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curmlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curmlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curmlo4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curmlo5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curmlo6);
				// register pressure performance issue on several platforms: do the high half here second
				unsigned curehi0{static_cast<unsigned>(curehi & 0xFFu)};
				curehi >>= 8;
				unsigned curmhi0{static_cast<unsigned>(curmhi & 0xFFu)};
				unsigned curmhi1{static_cast<unsigned>(curmhi >> (8 - log2ptrs))};
				unsigned curmhi2{static_cast<unsigned>(curmhi >> (16 - log2ptrs))};
				unsigned curmhi3{static_cast<unsigned>(curmhi >> (24 - log2ptrs))};
				unsigned curmhi4{static_cast<unsigned>(curmhi >> (32 - log2ptrs))};
				unsigned curmhi5{static_cast<unsigned>(curmhi >> (40 - log2ptrs))};
				unsigned curmhi6{static_cast<unsigned>(curmhi >> (48 - log2ptrs))};
				curmhi >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(curehi0)];
				if constexpr(isabsvalue && issignmode && isfltpmode) curehi &= 0x7Fu;
				else curehi &= 0xFFu;
				++offsets[curmhi0];
				curmhi1 &= sizeof(void *) * 0xFFu;
				curmhi2 &= sizeof(void *) * 0xFFu;
				curmhi3 &= sizeof(void *) * 0xFFu;
				curmhi4 &= sizeof(void *) * 0xFFu;
				curmhi5 &= sizeof(void *) * 0xFFu;
				curmhi6 &= sizeof(void *) * 0xFFu;
				++offsets[7 * 256 + static_cast<size_t>(curmhi)];
				++offsets[9 * 256 + static_cast<size_t>(curehi)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curmhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curmhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curmhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curmhi4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curmhi5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curmhi6);
			}while(pinputlo < pinputhi);
			if(pinputlo == pinputhi){// fill in the final item for odd counts
				V *p{pinputlo[0]};
				// no write to input, as this is the midpoint
				*pbufferhi = p;
				auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
				auto[curm, cure]{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curm, cure);
				}
				unsigned cure0{static_cast<unsigned>(cure & 0xFFu)};
				cure >>= 8;
				unsigned curm0{static_cast<unsigned>(curm & 0xFFu)};
				unsigned curm1{static_cast<unsigned>(curm >> (8 - log2ptrs))};
				unsigned curm2{static_cast<unsigned>(curm >> (16 - log2ptrs))};
				unsigned curm3{static_cast<unsigned>(curm >> (24 - log2ptrs))};
				unsigned curm4{static_cast<unsigned>(curm >> (32 - log2ptrs))};
				unsigned curm5{static_cast<unsigned>(curm >> (40 - log2ptrs))};
				unsigned curm6{static_cast<unsigned>(curm >> (48 - log2ptrs))};
				curm >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(cure0)];
				cure &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
				++offsets[curm0];
				curm1 &= sizeof(void *) * 0xFFu;
				curm2 &= sizeof(void *) * 0xFFu;
				curm3 &= sizeof(void *) * 0xFFu;
				curm4 &= sizeof(void *) * 0xFFu;
				curm5 &= sizeof(void *) * 0xFFu;
				curm6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(cure)];
				++offsets[7 * 256 + static_cast<size_t>(curm)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curm1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curm2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curm3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curm4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curm5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curm6);
			}
		}else{// not in reverse order
			ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
			if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
			do{
				V *p{input[i]};
				buffer[i] = p;
				auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
				auto[curm, cure]{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curm, cure);
				}
				// register pressure performance issue on several platforms: first do the low half here
				unsigned cure0{static_cast<unsigned>(cure & 0xFFu)};
				cure >>= 8;
				unsigned curm0{static_cast<unsigned>(curm & 0xFFu)};
				unsigned curm1{static_cast<unsigned>(curm >> (8 - log2ptrs))};
				unsigned curm2{static_cast<unsigned>(curm >> (16 - log2ptrs))};
				unsigned curm3{static_cast<unsigned>(curm >> (24 - log2ptrs))};
				unsigned curm4{static_cast<unsigned>(curm >> (32 - log2ptrs))};
				unsigned curm5{static_cast<unsigned>(curm >> (40 - log2ptrs))};
				unsigned curm6{static_cast<unsigned>(curm >> (48 - log2ptrs))};
				curm >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(cure0)];
				cure &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
				++offsets[curm0];
				curm1 &= sizeof(void *) * 0xFFu;
				curm2 &= sizeof(void *) * 0xFFu;
				curm3 &= sizeof(void *) * 0xFFu;
				curm4 &= sizeof(void *) * 0xFFu;
				curm5 &= sizeof(void *) * 0xFFu;
				curm6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(cure)];
				++offsets[7 * 256 + static_cast<size_t>(curm)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curm1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curm2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curm3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curm4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curm5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curm6);
				// register pressure performance issue on several platforms: do the high half here second
				unsigned cure0{static_cast<unsigned>(cure & 0xFFu)};
				cure >>= 8;
				unsigned curm0{static_cast<unsigned>(curm & 0xFFu)};
				unsigned curm1{static_cast<unsigned>(curm >> (8 - log2ptrs))};
				unsigned curm2{static_cast<unsigned>(curm >> (16 - log2ptrs))};
				unsigned curm3{static_cast<unsigned>(curm >> (24 - log2ptrs))};
				unsigned curm4{static_cast<unsigned>(curm >> (32 - log2ptrs))};
				unsigned curm5{static_cast<unsigned>(curm >> (40 - log2ptrs))};
				unsigned curm6{static_cast<unsigned>(curm >> (48 - log2ptrs))};
				curm >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(cure0)];
				cure &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
				++offsets[curm0];
				curm1 &= sizeof(void *) * 0xFFu;
				curm2 &= sizeof(void *) * 0xFFu;
				curm3 &= sizeof(void *) * 0xFFu;
				curm4 &= sizeof(void *) * 0xFFu;
				curm5 &= sizeof(void *) * 0xFFu;
				curm6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(cure)];
				++offsets[7 * 256 + static_cast<size_t>(curm)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curm1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curm2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curm3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curm4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curm5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curm6);
				i -= 2;
			}while(0 < i);
			if(!(1 & i)){// fill in the final item for odd counts
				V *p{input[0]};
				buffer[0] = p;
				auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
				auto[curm, cure]{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curm, cure);
				}
				unsigned cure0{static_cast<unsigned>(cure & 0xFFu)};
				cure >>= 8;
				unsigned curm0{static_cast<unsigned>(curm & 0xFFu)};
				unsigned curm1{static_cast<unsigned>(curm >> (8 - log2ptrs))};
				unsigned curm2{static_cast<unsigned>(curm >> (16 - log2ptrs))};
				unsigned curm3{static_cast<unsigned>(curm >> (24 - log2ptrs))};
				unsigned curm4{static_cast<unsigned>(curm >> (32 - log2ptrs))};
				unsigned curm5{static_cast<unsigned>(curm >> (40 - log2ptrs))};
				unsigned curm6{static_cast<unsigned>(curm >> (48 - log2ptrs))};
				curm >>= 56;
				++offsets[8 * 256 + static_cast<size_t>(cure0)];
				cure &= 0xFFu >> static_cast<unsigned>(isabsvalue && issignmode && isfltpmode);
				++offsets[curm0];
				curm1 &= sizeof(void *) * 0xFFu;
				curm2 &= sizeof(void *) * 0xFFu;
				curm3 &= sizeof(void *) * 0xFFu;
				curm4 &= sizeof(void *) * 0xFFu;
				curm5 &= sizeof(void *) * 0xFFu;
				curm6 &= sizeof(void *) * 0xFFu;
				++offsets[9 * 256 + static_cast<size_t>(cure)];
				++offsets[7 * 256 + static_cast<size_t>(curm)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curm1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curm2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curm3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curm4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curm5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curm6);
			}
		}

		// barrier and pointer exchange with the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, size_t *, std::nullptr_t> offsetscompanion;
		if constexpr(ismultithreadcapable){
			uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsets) & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed);
				}while(reinterpret_cast<uintptr_t>(offsets) == other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			// this will just be zero if usemultithread is zero
			offsetscompanion = reinterpret_cast<size_t *>(other);// retrieve the pointer
		}

		// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
		auto[runsteps, paritybool]{generateoffsetsmulti<isdescsort, isabsvalue, issignmode, isfltpmode, ismultithreadcapable, T>(count, offsets, offsetscompanion, usemultithread, movetobuffer)};

		// barrier and (flipped bits) runsteps, paritybool value exchange with the companion thread
		if constexpr(ismultithreadcapable){
			// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
			uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + static_cast<uintptr_t>(usemultithread)};
			while(reinterpret_cast<uintptr_t>(offsets) == atomiclightbarrier.load(std::memory_order_relaxed)){
				spinpause();// catch up
			}
			uintptr_t other{atomiclightbarrier.fetch_add(compound & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
				}while(!other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			other += compound;// combine
			unsigned lowercarryoutbits{2 * usemultithread + paritybool};
			paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
			other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
			runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
		}

		// perform the bidirectional 8-bit sorting sequence
		// flip the relevant bits inside runsteps first
		if(runsteps ^= (1u << 80 / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
		{
			V **psrclo{input}, **pdst{buffer};
			if(paritybool){// swap if the count of sorting actions to do is odd
				psrclo = buffer;
				pdst = input;
			}
			radixsortnoallocmultimain<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, ismultithreadcapable, V>(count, psrclo, pdst, psrclo, offsets, runsteps, usemultithread, atomiclightbarrier, varparameters...);
		}
	}
}

// initialisation part, multi-threading companion for the radixsortnoallocmulti() function implementation template for multi-part types with indirection
// Do not use this function directly.
template<auto indirection1, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, bool isinputconst, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	64 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>) &&
	8 < CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortnoallocmultiinitmtc(size_t count, std::conditional_t<isinputconst, V *const *, V **> input, V *pout[], std::conditional_t<isinputconst, V **, std::nullptr_t> pdst, size_t offsetscompanion[], vararguments... varparameters)noexcept{
	using T = tounifunsigned<std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>>;
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(7 <= count);// this function is not for small arrays, 8 is the minimum original array count for 16-bit inputs
	// do not pass a nullptr here
	assert(input);
	assert(pout);
	if constexpr(isinputconst) assert(pdst);
	assert(offsetscompanion);
	if constexpr(64 == CHAR_BIT * sizeof(T)){
		if constexpr(isrevorder){// also reverse the array at the same time
			if constexpr(isinputconst){
				// unsigned counter, not zero inclusive inside the loop
				size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
				pout += count - i;
				pdst += count - i;
				do{
					V *phi{input[0]};
					V *plo{input[1]};
					input += 2;
					pout[i] = phi;
					pdst[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					pout[i - 1] = plo;
					pdst[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					U curhi6{curhi >> (48 - log2ptrs)};
					curhi >>= 56;
					++offsetscompanion[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					curhi6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curhi5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curhi6);
					++offsetscompanion[7 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					U curlo6{curlo >> (48 - log2ptrs)};
					curlo >>= 56;
					++offsetscompanion[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					curlo6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curlo5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curlo6);
					++offsetscompanion[7 * 256 + static_cast<size_t>(curlo)];
				}while(i -= 2);
			}else{// !isinputconst
				size_t i{(count + 1 + 2) >> 2};// rounded up in the companion thread
				V **pinputlo{input}, **pinputhi{input + count};
				V **poutputlo{pout}, **poutputhi{pout + count};
				do{
					V *plo{pinputlo[0]};
					V *phi{pinputhi[0]};
					*pinputhi-- = plo;
					*poutputhi-- = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					*pinputlo++ = phi;
					*poutputlo++ = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curlo, curhi);
					}
					// register pressure performance issue on several platforms: first do the low half here
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					U curlo6{curlo >> (48 - log2ptrs)};
					curlo >>= 56;
					++offsetscompanion[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					curlo6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curlo5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curlo6);
					++offsetscompanion[7 * 256 + static_cast<size_t>(curlo)];
					// register pressure performance issue on several platforms: do the high half here second
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					U curhi6{curhi >> (48 - log2ptrs)};
					curhi >>= 56;
					++offsetscompanion[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					curhi6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curhi5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curhi6);
					++offsetscompanion[7 * 256 + static_cast<size_t>(curhi)];
				}while(--i);
			}
		}else{// 64-bit, not in reverse order
			// unsigned counter, not zero inclusive inside the loop
			size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
			input += count - i;
			pout += count - i;
			do{
				V *phi{input[i]};
				V *plo{input[i - 1]};
				pout[i] = phi;
				auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
				pout[i - 1] = plo;
				auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
				U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
				U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
				}
				// register pressure performance issue on several platforms: first do the high half here
				U curhi0{curhi & 0xFFu};
				U curhi1{curhi >> (8 - log2ptrs)};
				U curhi2{curhi >> (16 - log2ptrs)};
				U curhi3{curhi >> (24 - log2ptrs)};
				U curhi4{curhi >> (32 - log2ptrs)};
				U curhi5{curhi >> (40 - log2ptrs)};
				U curhi6{curhi >> (48 - log2ptrs)};
				curhi >>= 56;
				++offsetscompanion[curhi0];
				curhi1 &= sizeof(void *) * 0xFFu;
				curhi2 &= sizeof(void *) * 0xFFu;
				curhi3 &= sizeof(void *) * 0xFFu;
				curhi4 &= sizeof(void *) * 0xFFu;
				curhi5 &= sizeof(void *) * 0xFFu;
				curhi6 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curhi4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curhi5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curhi6);
				++offsetscompanion[7 * 256 + static_cast<size_t>(curhi)];
				// register pressure performance issue on several platforms: do the low half here second
				U curlo0{curlo & 0xFFu};
				U curlo1{curlo >> (8 - log2ptrs)};
				U curlo2{curlo >> (16 - log2ptrs)};
				U curlo3{curlo >> (24 - log2ptrs)};
				U curlo4{curlo >> (32 - log2ptrs)};
				U curlo5{curlo >> (40 - log2ptrs)};
				U curlo6{curlo >> (48 - log2ptrs)};
				curlo >>= 56;
				++offsetscompanion[curlo0];
				curlo1 &= sizeof(void *) * 0xFFu;
				curlo2 &= sizeof(void *) * 0xFFu;
				curlo3 &= sizeof(void *) * 0xFFu;
				curlo4 &= sizeof(void *) * 0xFFu;
				curlo5 &= sizeof(void *) * 0xFFu;
				curlo6 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curlo4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curlo5);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 6 * 256) + curlo6);
				++offsetscompanion[7 * 256 + static_cast<size_t>(curlo)];
			}while(i -= 2);
		}
	}else if constexpr(56 == CHAR_BIT * sizeof(T)){
		if constexpr(isrevorder){// also reverse the array at the same time
			if constexpr(isinputconst){
				// unsigned counter, not zero inclusive inside the loop
				size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
				pout += count - i;
				pdst += count - i;
				do{
					V *phi{input[0]};
					V *plo{input[1]};
					input += 2;
					pout[i] = phi;
					pdst[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					pout[i - 1] = plo;
					pdst[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					curhi >>= 48;
					++offsetscompanion[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curhi5);
					++offsetscompanion[6 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					curlo >>= 48;
					++offsetscompanion[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curlo5);
					++offsetscompanion[6 * 256 + static_cast<size_t>(curlo)];
				}while(i -= 2);
			}else{// !isinputconst
				V **pinputlo{input}, **pinputhi{input + count};
				V **poutputlo{pout}, **poutputhi{pout + count};
				size_t i{(count + 1 + 2) >> 2};// rounded up in the companion thread
				do{
					V *plo{pinputlo[0]};
					V *phi{pinputhi[0]};
					*pinputhi-- = plo;
					*poutputhi-- = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					*pinputlo++ = phi;
					*poutputlo++ = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curlo, curhi);
					}
					// register pressure performance issue on several platforms: first do the low half here
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					curlo >>= 48;
					++offsetscompanion[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curlo5);
					++offsetscompanion[6 * 256 + static_cast<size_t>(curlo)];
					// register pressure performance issue on several platforms: do the high half here second
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					curhi >>= 48;
					++offsetscompanion[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curhi5);
					++offsetscompanion[6 * 256 + static_cast<size_t>(curhi)];
				}while(--i);
			}
		}else{// 56-bit, not in reverse order
			// unsigned counter, not zero inclusive inside the loop
			size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
			input += count - i;
			pout += count - i;
			do{
				V *phi{input[i]};
				V *plo{input[i - 1]};
				pout[i] = phi;
				auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
				pout[i - 1] = plo;
				auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
				U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
				U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
				}
				// register pressure performance issue on several platforms: first do the high half here
				U curhi0{curhi & 0xFFu};
				U curhi1{curhi >> (8 - log2ptrs)};
				U curhi2{curhi >> (16 - log2ptrs)};
				U curhi3{curhi >> (24 - log2ptrs)};
				U curhi4{curhi >> (32 - log2ptrs)};
				U curhi5{curhi >> (40 - log2ptrs)};
				curhi >>= 48;
				++offsetscompanion[curhi0];
				curhi1 &= sizeof(void *) * 0xFFu;
				curhi2 &= sizeof(void *) * 0xFFu;
				curhi3 &= sizeof(void *) * 0xFFu;
				curhi4 &= sizeof(void *) * 0xFFu;
				curhi5 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curhi4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curhi5);
				++offsetscompanion[6 * 256 + static_cast<size_t>(curhi)];
				// register pressure performance issue on several platforms: do the low half here second
				U curlo0{curlo & 0xFFu};
				U curlo1{curlo >> (8 - log2ptrs)};
				U curlo2{curlo >> (16 - log2ptrs)};
				U curlo3{curlo >> (24 - log2ptrs)};
				U curlo4{curlo >> (32 - log2ptrs)};
				U curlo5{curlo >> (40 - log2ptrs)};
				curlo >>= 48;
				++offsetscompanion[curlo0];
				curlo1 &= sizeof(void *) * 0xFFu;
				curlo2 &= sizeof(void *) * 0xFFu;
				curlo3 &= sizeof(void *) * 0xFFu;
				curlo4 &= sizeof(void *) * 0xFFu;
				curlo5 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curlo4);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 5 * 256) + curlo5);
				++offsetscompanion[6 * 256 + static_cast<size_t>(curlo)];
			}while(i -= 2);
		}
	}else if constexpr(48 == CHAR_BIT * sizeof(T)){
		if constexpr(isrevorder){// also reverse the array at the same time
			if constexpr(isinputconst){
				// unsigned counter, not zero inclusive inside the loop
				size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
				pout += count - i;
				pdst += count - i;
				do{
					V *phi{input[0]};
					V *plo{input[1]};
					input += 2;
					pout[i] = phi;
					pdst[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					pout[i - 1] = plo;
					pdst[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					curhi >>= 40;
					++offsetscompanion[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curhi4);
					++offsetscompanion[5 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					curlo >>= 40;
					++offsetscompanion[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curlo4);
					++offsetscompanion[5 * 256 + static_cast<size_t>(curlo)];
				}while(i -= 2);
			}else{// !isinputconst
				V **pinputlo{input}, **pinputhi{input + count};
				V **poutputlo{pout}, **poutputhi{pout + count};
				size_t i{(count + 1 + 2) >> 2};// rounded up in the companion thread
				do{
					V *plo{pinputlo[0]};
					V *phi{pinputhi[0]};
					*pinputhi-- = plo;
					*poutputhi-- = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					*pinputlo++ = phi;
					*poutputlo++ = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curlo, curhi);
					}
					// register pressure performance issue on several platforms: first do the low half here
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					curlo >>= 40;
					++offsetscompanion[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curlo4);
					++offsetscompanion[5 * 256 + static_cast<size_t>(curlo)];
					// register pressure performance issue on several platforms: do the high half here second
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					curhi >>= 40;
					++offsetscompanion[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curhi4);
					++offsetscompanion[5 * 256 + static_cast<size_t>(curhi)];
				}while(--i);
			}
		}else{// 48-bit, not in reverse order
			// unsigned counter, not zero inclusive inside the loop
			size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
			input += count - i;
			pout += count - i;
			do{
				V *phi{input[i]};
				V *plo{input[i - 1]};
				pout[i] = phi;
				auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
				pout[i - 1] = plo;
				auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
				U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
				U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
				}
				// register pressure performance issue on several platforms: first do the high half here
				U curhi0{curhi & 0xFFu};
				U curhi1{curhi >> (8 - log2ptrs)};
				U curhi2{curhi >> (16 - log2ptrs)};
				U curhi3{curhi >> (24 - log2ptrs)};
				U curhi4{curhi >> (32 - log2ptrs)};
				curhi >>= 40;
				++offsetscompanion[curhi0];
				curhi1 &= sizeof(void *) * 0xFFu;
				curhi2 &= sizeof(void *) * 0xFFu;
				curhi3 &= sizeof(void *) * 0xFFu;
				curhi4 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curhi4);
				++offsetscompanion[5 * 256 + static_cast<size_t>(curhi)];
				// register pressure performance issue on several platforms: do the low half here second
				U curlo0{curlo & 0xFFu};
				U curlo1{curlo >> (8 - log2ptrs)};
				U curlo2{curlo >> (16 - log2ptrs)};
				U curlo3{curlo >> (24 - log2ptrs)};
				U curlo4{curlo >> (32 - log2ptrs)};
				curlo >>= 40;
				++offsetscompanion[curlo0];
				curlo1 &= sizeof(void *) * 0xFFu;
				curlo2 &= sizeof(void *) * 0xFFu;
				curlo3 &= sizeof(void *) * 0xFFu;
				curlo4 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 4 * 256) + curlo4);
				++offsetscompanion[5 * 256 + static_cast<size_t>(curlo)];
			}while(i -= 2);
		}
	}else if constexpr(40 == CHAR_BIT * sizeof(T)){
		if constexpr(isrevorder){// also reverse the array at the same time
			if constexpr(isinputconst){
				// unsigned counter, not zero inclusive inside the loop
				size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
				pout += count - i;
				pdst += count - i;
				do{
					V *phi{input[0]};
					V *plo{input[1]};
					input += 2;
					pout[i] = phi;
					pdst[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					pout[i - 1] = plo;
					pdst[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					curhi >>= 32;
					++offsetscompanion[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
					++offsetscompanion[4 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					curlo >>= 32;
					++offsetscompanion[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
					++offsetscompanion[4 * 256 + static_cast<size_t>(curlo)];
				}while(i -= 2);
			}else{// !isinputconst
				V **pinputlo{input}, **pinputhi{input + count};
				V **poutputlo{pout}, **poutputhi{pout + count};
				size_t i{(count + 1 + 2) >> 2};// rounded up in the companion thread
				do{
					V *plo{pinputlo[0]};
					V *phi{pinputhi[0]};
					*pinputhi-- = plo;
					*poutputhi-- = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					*pinputlo++ = phi;
					*poutputlo++ = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curlo, curhi);
					}
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					curlo >>= 32;
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					curhi >>= 32;
					++offsetscompanion[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++offsetscompanion[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
					++offsetscompanion[4 * 256 + static_cast<size_t>(curlo)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
					++offsetscompanion[4 * 256 + static_cast<size_t>(curhi)];
				}while(--i);
			}
		}else{// 40-bit, not in reverse order
			// unsigned counter, not zero inclusive inside the loop
			size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
			input += count - i;
			pout += count - i;
			do{
				V *phi{input[i]};
				V *plo{input[i - 1]};
				pout[i] = phi;
				auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
				pout[i - 1] = plo;
				auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
				U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
				U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
				}
				U curhi0{curhi & 0xFFu};
				U curhi1{curhi >> (8 - log2ptrs)};
				U curhi2{curhi >> (16 - log2ptrs)};
				U curhi3{curhi >> (24 - log2ptrs)};
				curhi >>= 32;
				U curlo0{curlo & 0xFFu};
				U curlo1{curlo >> (8 - log2ptrs)};
				U curlo2{curlo >> (16 - log2ptrs)};
				U curlo3{curlo >> (24 - log2ptrs)};
				curlo >>= 32;
				++offsetscompanion[curhi0];
				curhi1 &= sizeof(void *) * 0xFFu;
				curhi2 &= sizeof(void *) * 0xFFu;
				curhi3 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
				++offsetscompanion[curlo0];
				curlo1 &= sizeof(void *) * 0xFFu;
				curlo2 &= sizeof(void *) * 0xFFu;
				curlo3 &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curhi1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curhi2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curhi3);
				++offsetscompanion[4 * 256 + static_cast<size_t>(curhi)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + curlo1);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + curlo2);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 3 * 256) + curlo3);
				++offsetscompanion[4 * 256 + static_cast<size_t>(curlo)];
			}while(i -= 2);
		}
	}else if constexpr(32 == CHAR_BIT * sizeof(T)){
		if constexpr(isrevorder){// also reverse the array at the same time
			if constexpr(isinputconst){
				// unsigned counter, not zero inclusive inside the loop
				size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
				pout += count - i;
				pdst += count - i;
				do{
					V *pa{input[0]};
					V *pb{input[1]};
					input += 2;
					pout[i] = pa;
					pdst[i] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					pout[i - 1] = pb;
					pdst[i - 1] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					U cur2a{cura >> (16 - log2ptrs)};
					cura >>= 24;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					U cur2b{curb >> (16 - log2ptrs)};
					curb >>= 24;
					++offsetscompanion[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					cur2a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsetscompanion[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					cur2b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1a);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + cur2a);
					++offsetscompanion[3 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1b);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + cur2b);
					++offsetscompanion[3 * 256 + static_cast<size_t>(curb)];
				}while(i -= 2);
			}else{// !isinputconst
				V **pinputlo{input}, **pinputhi{input + count};
				V **poutputlo{pout}, **poutputhi{pout + count};
				size_t i{(count + 1 + 2) >> 2};// rounded up in the companion thread
				do{
					V *pa{pinputlo[0]};
					V *pb{pinputhi[0]};
					*pinputhi-- = pa;
					*poutputhi-- = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					*pinputlo++ = pb;
					*poutputlo++ = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					U cur2a{cura >> (16 - log2ptrs)};
					cura >>= 24;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					U cur2b{curb >> (16 - log2ptrs)};
					curb >>= 24;
					++offsetscompanion[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					cur2a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsetscompanion[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					cur2b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1a);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + cur2a);
					++offsetscompanion[3 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1b);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + cur2b);
					++offsetscompanion[3 * 256 + static_cast<size_t>(curb)];
				}while(--i);
			}
		}else{// 32-bit, not in reverse order
			// unsigned counter, not zero inclusive inside the loop
			size_t i{((count + 1 + 2) >> 2) * 2};// rounded up in the companion thread
			input += count - i;
			pout += count - i;
			do{
				V *pa{input[i]};
				V *pb{input[i - 1]};
				pout[i] = pa;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				pout[i - 1] = pb;
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
				}
				U cur0a{cura & 0xFFu};
				U cur1a{cura >> (8 - log2ptrs)};
				U cur2a{cura >> (16 - log2ptrs)};
				cura >>= 24;
				U cur0b{curb & 0xFFu};
				U cur1b{curb >> (8 - log2ptrs)};
				U cur2b{curb >> (16 - log2ptrs)};
				curb >>= 24;
				++offsetscompanion[cur0a];
				cur1a &= sizeof(void *) * 0xFFu;
				cur2a &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
				++offsetscompanion[cur0b];
				cur1b &= sizeof(void *) * 0xFFu;
				cur2b &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1a);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + cur2a);
				++offsetscompanion[3 * 256 + static_cast<size_t>(cura)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1b);
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 2 * 256) + cur2b);
				++offsetscompanion[3 * 256 + static_cast<size_t>(curb)];
			}while(i -= 2);
		}
	}else if constexpr(24 == CHAR_BIT * sizeof(T)){
		if constexpr(isrevorder){// also reverse the array at the same time
			if constexpr(isinputconst){
				// unsigned counter, not zero inclusive inside the loop
				size_t i{((count + 1 + 3) / 6) * 3};// rounded up in the companion thread
				pout += count - i;
				pdst += count - i;
				do{
					V *pa{input[0]};
					V *pb{input[1]};
					V *pc{input[2]};
					input += 3;
					pout[i] = pa;
					pdst[i] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					pout[i - 1] = pb;
					pdst[i - 1] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					pout[i - 2] = pc;
					pdst[i - 2] = pc;
					auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					cura >>= 16;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					curb >>= 16;
					U cur0c{curc & 0xFFu};
					U cur1c{curc >> (8 - log2ptrs)};
					curc >>= 16;
					++offsetscompanion[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsetscompanion[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsetscompanion[cur0c];
					cur1c &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1a);
					++offsetscompanion[2 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1b);
					++offsetscompanion[2 * 256 + static_cast<size_t>(curb)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1c);
					++offsetscompanion[2 * 256 + static_cast<size_t>(curc)];
				}while(i -= 3);
			}else{// !isinputconst
				V **pinputlo{input}, **pinputhi{input + count};
				V **poutputlo{pout}, **poutputhi{pout + count};
				size_t i{(count + 1 + 6) / 12};// rounded up in the companion thread
				do{
					V *pa{pinputlo[0]};
					V *pb{pinputhi[0]};
					V *pc{pinputlo[1]};
					V *pd{pinputhi[-1]};
					pinputhi[0] = pa;
					poutputhi[0] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					pinputlo[0] = pb;
					poutputlo[0] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					pinputhi[1] = pc;
					poutputhi[1] = pc;
					auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
					// register pressure performance issue on several platforms: first do the high half here
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					cura >>= 16;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					curb >>= 16;
					U cur0c{curc & 0xFFu};
					U cur1c{curc >> (8 - log2ptrs)};
					curc >>= 16;
					++offsetscompanion[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsetscompanion[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsetscompanion[cur0c];
					cur1c &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1a);
					++offsetscompanion[2 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1b);
					++offsetscompanion[2 * 256 + static_cast<size_t>(curb)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1c);
					++offsetscompanion[2 * 256 + static_cast<size_t>(curc)];
					V *pe{pinputlo[2]};
					V *pf{pinputhi[-2]};
					pinputlo[1] = pd;
					poutputlo[1] = pd;
					auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
					pinputhi[-2] = pe;
					pinputhi -= 3;
					poutputhi[-2] = pe;
					poutputhi -= 3;
					auto ime{indirectinput1<indirection1, isindexed2, T, V>(pe, varparameters...)};
					pinputlo[2] = pf;
					pinputlo += 3;
					poutputlo[2] = pf;
					poutputlo += 3;
					auto imf{indirectinput1<indirection1, isindexed2, T, V>(pf, varparameters...)};
					U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
					U cure{indirectinput2<indirection1, indirection2, isindexed2, T>(ime, varparameters...)};
					U curf{indirectinput2<indirection1, indirection2, isindexed2, T>(ime, varparameters...)};
					// register pressure performance issue on several platforms: do the low half here second
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curd, cure, curf);
					}
					U cur0d{curd & 0xFFu};
					U cur1d{curd >> (8 - log2ptrs)};
					curd >>= 16;
					U cur0e{cure & 0xFFu};
					U cur1e{cure >> (8 - log2ptrs)};
					cure >>= 16;
					U cur0f{curf & 0xFFu};
					U cur1f{curf >> (8 - log2ptrs)};
					curf >>= 16;
					++offsetscompanion[cur0d];
					cur1d &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curd &= 0x7Fu;
					++offsetscompanion[cur0e];
					cur1e &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cure &= 0x7Fu;
					++offsetscompanion[cur0f];
					cur1f &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curf &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1d);
					++offsetscompanion[2 * 256 + static_cast<size_t>(curd)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1e);
					++offsetscompanion[2 * 256 + static_cast<size_t>(cure)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1f);
					++offsetscompanion[2 * 256 + static_cast<size_t>(curf)];
				}while(--i);
			}
		}else{// 24-bit, not in reverse order
			// unsigned counter, not zero inclusive inside the loop
			size_t i{((count + 1 + 3) / 6) * 3};// rounded up in the companion thread
			input += count - i;
			pout += count - i;
			do{
				V *pa{input[i]};
				V *pb{input[i - 1]};
				V *pc{input[i - 2]};
				pout[i] = pa;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				pout[i - 1] = pb;
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				pout[i - 2] = pc;
				auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
				U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc);
				}
				U cur0a{cura & 0xFFu};
				U cur1a{cura >> (8 - log2ptrs)};
				cura >>= 16;
				U cur0b{curb & 0xFFu};
				U cur1b{curb >> (8 - log2ptrs)};
				curb >>= 16;
				U cur0c{curc & 0xFFu};
				U cur1c{curc >> (8 - log2ptrs)};
				curc >>= 16;
				++offsetscompanion[cur0a];
				cur1a &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
				++offsetscompanion[cur0b];
				cur1b &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
				++offsetscompanion[cur0c];
				cur1c &= sizeof(void *) * 0xFFu;
				if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1a);
				++offsetscompanion[2 * 256 + static_cast<size_t>(cura)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1b);
				++offsetscompanion[2 * 256 + static_cast<size_t>(curb)];
				++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsetscompanion + 256) + cur1c);
				++offsetscompanion[2 * 256 + static_cast<size_t>(curc)];
			}while(i -= 3);
		}
	}else if constexpr(16 == CHAR_BIT * sizeof(T)){
		if constexpr(isrevorder){// also reverse the array at the same time
			if constexpr(isinputconst){
				// unsigned counter, not zero inclusive inside the loop
				size_t i{((count + 1 + 4) >> 3) * 4};// rounded up in the companion thread
				pout += count - i;
				pdst += count - i;
				do{
					V *pa{input[0]};
					V *pb{input[1]};
					V *pc{input[2]};
					V *pd{input[3]};
					input += 4;
					pout[i] = pa;
					pdst[i] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					pout[i - 1] = pb;
					pdst[i - 1] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					pout[i - 2] = pc;
					pdst[i - 2] = pc;
					auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
					pout[i - 3] = pd;
					pdst[i - 3] = pd;
					auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
					U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd);
					}
					U cur0a{cura & 0xFFu};
					cura >>= 8;
					U cur0b{curb & 0xFFu};
					curb >>= 8;
					U cur0c{curc & 0xFFu};
					curc >>= 8;
					U cur0d{curd & 0xFFu};
					curd >>= 8;
					++offsetscompanion[cur0a];
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsetscompanion[cur0b];
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsetscompanion[cur0c];
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++offsetscompanion[cur0d];
					if constexpr(isabsvalue && issignmode && isfltpmode) curd &= 0x7Fu;
					++offsetscompanion[256 + static_cast<size_t>(cura)];
					++offsetscompanion[256 + static_cast<size_t>(curb)];
					++offsetscompanion[256 + static_cast<size_t>(curc)];
					++offsetscompanion[256 + static_cast<size_t>(curd)];
				}while(i -= 4);
			}else{// !isinputconst
				V **pinputlo{input}, **pinputhi{input + count};
				V **poutputlo{pout}, **poutputhi{pout + count};
				size_t i{(count + 1 + 4) >> 3};// rounded up in the companion thread
				do{
					V *pa{pinputlo[0]};
					V *pb{pinputhi[0]};
					V *pc{pinputlo[1]};
					V *pd{pinputhi[-1]};
					pinputhi[0] = pa;
					poutputhi[0] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					pinputlo[0] = pb;
					poutputlo[0] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					pinputhi[-1] = pc;
					pinputhi -= 2;
					poutputhi[-1] = pc;
					poutputhi -= 2;
					auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
					pinputlo[1] = pd;
					pinputlo += 2;
					poutputlo[1] = pd;
					poutputlo += 2;
					auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
					U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd);
					}
					U cur0a{cura & 0xFFu};
					cura >>= 8;
					U cur0b{curb & 0xFFu};
					curb >>= 8;
					U cur0c{curc & 0xFFu};
					curc >>= 8;
					U cur0d{curd & 0xFFu};
					curd >>= 8;
					++offsetscompanion[cur0a];
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsetscompanion[cur0b];
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsetscompanion[cur0c];
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++offsetscompanion[cur0d];
					if constexpr(isabsvalue && issignmode && isfltpmode) curd &= 0x7Fu;
					++offsetscompanion[256 + static_cast<size_t>(cura)];
					++offsetscompanion[256 + static_cast<size_t>(curb)];
					++offsetscompanion[256 + static_cast<size_t>(curc)];
					++offsetscompanion[256 + static_cast<size_t>(curd)];
				}while(--i);
			}
		}else{// 16-bit, not in reverse order
			// unsigned counter, not zero inclusive inside the loop
			size_t i{((count + 1 + 4) >> 3) * 4};// rounded up in the companion thread
			input += count - i;
			pout += count - i;
			do{
				V *pa{input[i]};
				V *pb{input[i - 1]};
				V *pc{input[i - 2]};
				V *pd{input[i - 3]};
				pout[i] = pa;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				pout[i - 1] = pb;
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				pout[i - 2] = pc;
				auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
				pout[i - 3] = pd;
				auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
				U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
				U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
				if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
					filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd);
				}
				U cur0a{cura & 0xFFu};
				cura >>= 8;
				U cur0b{curb & 0xFFu};
				curb >>= 8;
				U cur0c{curc & 0xFFu};
				curc >>= 8;
				U cur0d{curd & 0xFFu};
				curd >>= 8;
				++offsetscompanion[cur0a];
				if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
				++offsetscompanion[cur0b];
				if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
				++offsetscompanion[cur0c];
				if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
				++offsetscompanion[cur0d];
				if constexpr(isabsvalue && issignmode && isfltpmode) curd &= 0x7Fu;
				++offsetscompanion[256 + static_cast<size_t>(cura)];
				++offsetscompanion[256 + static_cast<size_t>(curb)];
				++offsetscompanion[256 + static_cast<size_t>(curc)];
				++offsetscompanion[256 + static_cast<size_t>(curd)];
			}while(i -= 4);
		}
	}else static_assert(false, "Implementing larger types will require additional work and optimisation for this library.");
}

// main part, multi-threading companion for the radixsortnoallocmulti() function implementation template for multi-part types with indirection
// Do not use this function directly.
template<auto indirection1, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	64 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>) &&
	8 < CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortnoallocmultimainmtc(size_t count, V *const input[], V *pdst[], V *pdstnext[], size_t offsetscompanion[], unsigned runsteps, std::atomic_uintptr_t &atomiclightbarrier, vararguments... varparameters)noexcept{
	using T = tounifunsigned<std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>>;
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(7 <= count);// this function is not for small arrays, 8 is the minimum original array count for 16-bit inputs
	// do not pass a nullptr here
	assert(input);
	assert(pdst);
	assert(pdstnext);
	assert(offsetscompanion);
	assert(runsteps);
	unsigned shifter{bitscanforwardportable(runsteps)};// at least 1 bit is set inside runsteps as by previous check
	V **psrchi;
	if constexpr(isrevorder){
		psrchi = pdstnext + count;
	}else{
		psrchi = const_cast<V **>(input) + count;// psrchi will never be written to
	}
	// skip a step if possible
	runsteps >>= shifter;
	size_t *poffset{offsetscompanion + static_cast<size_t>(shifter) * 256};
	if constexpr(!isabsvalue && isfltpmode) if(CHAR_BIT * sizeof(T) / 8 - 1 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
		[[unlikely]]
#endif
		goto handletop8;// rare, but possible
	shifter *= 8;
	for(;;){
		{
			size_t j{(count + 1 + 4) >> 3};// rounded up in the top part
			do{// fill the array, four at a time
				V *pa{psrchi[0]};
				V *pb{psrchi[-1]};
				V *pc{psrchi[-2]};
				V *pd{psrchi[-3]};
				psrchi -= 4;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
				auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
				auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				auto outc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
				auto outd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
				auto[cura, curb, curc, curd]{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd, shifter)};
				size_t offseta{poffset[cura]--};// the next item will be placed one lower
				size_t offsetb{poffset[curb]--};
				size_t offsetc{poffset[curc]--};
				size_t offsetd{poffset[curd]--};
				pdst[offseta] = pa;
				pdst[offsetb] = pb;
				pdst[offsetc] = pc;
				pdst[offsetd] = pd;
			}while(--j);
		}
		runsteps >>= 1;
		if(!runsteps)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
			break;
		{
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
			[[maybe_unused]]
#endif
			unsigned index;
			if constexpr(16 < CHAR_BIT * sizeof(T)) index = bitscanforwardportable(runsteps);// at least 1 bit is set inside runsteps as by previous check
			shifter += 8;
			poffset += 256;
			// swap the pointers for the next round, data is moved on each iteration
			psrchi = pdst;
			uintptr_t old{atomiclightbarrier.fetch_add(~static_cast<uintptr_t>(0))};
			pdst = pdstnext;
			pdstnext = psrchi;
			psrchi += count;
			// skip a step if possible
			if constexpr(16 < CHAR_BIT * sizeof(T)){
				runsteps >>= index;
				shifter += index * 8;
				poffset += static_cast<size_t>(index) * 256;
			}
			if(!old) do{
				spinpause();
			}while(atomiclightbarrier.load(std::memory_order_relaxed));
		}
		// handle the top part for floating-point differently
		if(!isabsvalue && isfltpmode && CHAR_BIT * sizeof(T) - 8 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
		{
handletop8:// this prevents "!isabsvalue && isfltpmode" to be made constexpr here, but that's fine
			size_t j{(count + 1 + 4) >> 3};// rounded up in the top part
			do{// fill the array, four at a time
				V *pa{psrchi[0]};
				V *pb{psrchi[-1]};
				V *pc{psrchi[-2]};
				V *pd{psrchi[-3]};
				psrchi -= 4;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
				auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
				auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				auto outc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
				auto outd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
				auto[cura, curb, curc, curd]{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd)};
				size_t offseta{offsetscompanion[cura + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]--};// the next item will be placed one lower
				size_t offsetb{offsetscompanion[curb + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]--};
				size_t offsetc{offsetscompanion[curc + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]--};
				size_t offsetd{offsetscompanion[curd + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]--};
				pdst[offseta] = pa;
				pdst[offsetb] = pb;
				pdst[offsetc] = pc;
				pdst[offsetd] = pd;
			}while(--j);
			break;// no further processing beyond the top part
		}
	}
}

// main part for the radixsortcopynoallocmulti() and radixsortnoallocmulti() function implementation templates for 80-bit-based long double types with indirection
// Do not use this function directly.
template<auto indirection1, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, bool ismultithreadcapable, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	64 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>) &&
	8 < CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortnoallocmultimain(size_t count, V *const input[], V *pdst[], V *pdstnext[], size_t offsets[], unsigned runsteps, std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread, std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t &, std::nullptr_t> atomiclightbarrier, vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	using T = tounifunsigned<std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>>;
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(count && count != MAXSIZE_T);
	// do not pass a nullptr here
	assert(input);
	assert(pdst);
	assert(pdstnext);
	assert(offsets);
	assert(runsteps);
	unsigned shifter{bitscanforwardportable(runsteps)};// at least 1 bit is set inside runsteps as by previous check
	V **psrclo;
	if constexpr(isrevorder){
		psrclo = pdstnext;
	}else{
		psrclo = const_cast<V **>(input);// psrclo will never be written to
	}
	// skip a step if possible
	runsteps >>= shifter;
	size_t *poffset{offsets + static_cast<size_t>(shifter) * 256};
	if constexpr(!isabsvalue && isfltpmode) if(CHAR_BIT * sizeof(T) / 8 - 1 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
		[[unlikely]]
#endif
		goto handletop8;// rare, but possible
	shifter *= 8;
	for(;;){
		{
			ptrdiff_t j;// rounded down in the bottom part, or no multithreading
			if constexpr(!ismultithreadcapable) j = static_cast<ptrdiff_t>((count + 1) >> 2);
			else j = static_cast<ptrdiff_t>((count + 1) >> (2 + usemultithread));
			while(0 <= --j){// fill the array, four at a time
				V *pa{psrclo[0]};
				V *pb{psrclo[1]};
				V *pc{psrclo[2]};
				V *pd{psrclo[3]};
				psrclo += 4;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
				auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
				auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				auto outc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
				auto outd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
				auto[cura, curb, curc, curd]{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd, shifter)};
				size_t offseta{poffset[cura]++};// the next item will be placed one higher
				size_t offsetb{poffset[curb]++};
				size_t offsetc{poffset[curc]++};
				size_t offsetd{poffset[curd]++};
				pdst[offseta] = pa;
				pdst[offsetb] = pb;
				pdst[offsetc] = pc;
				pdst[offsetd] = pd;
			}
		}
		if(2 & count + 1){// fill in the final two items for a remainder of 2 or 3
			V *pa{psrclo[0]};
			V *pb{psrclo[1]};
			psrclo += 2;
			auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
			auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
			auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
			auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
			auto[cura, curb]{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, shifter)};
			size_t offseta{poffset[cura]++};// the next item will be placed one higher
			size_t offsetb{poffset[curb]++};
			pdst[offseta] = pa;
			pdst[offsetb] = pb;
		}
		if(!(1 & count)){// fill in the final item for odd counts
			V *p{psrclo[0]};
			auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
			auto out{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
			size_t cur{filtershift8<isabsvalue, issignmode, isfltpmode, T, U>(out, shifter)};
			size_t offset{poffset[cur]};
			pdst[offset] = p;
		}
		runsteps >>= 1;
		if(!runsteps)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
			break;
		{
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
			[[maybe_unused]]
#endif
			unsigned index;
			if constexpr(16 < CHAR_BIT * sizeof(T)) index = bitscanforwardportable(runsteps);// at least 1 bit is set inside runsteps as by previous check
			shifter += 8;
			poffset += 256;
			// swap the pointers for the next round, data is moved on each iteration
			psrclo = pdst;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
			[[maybe_unused]]
#endif
			uintptr_t old;
			if constexpr(ismultithreadcapable) old = atomiclightbarrier.fetch_add(usemultithread);
			pdst = pdstnext;
			pdstnext = psrclo;
			// skip a step if possible
			if constexpr(16 < CHAR_BIT * sizeof(T)){
				runsteps >>= index;
				shifter += index * 8;
				poffset += static_cast<size_t>(index) * 256;
			}
			if constexpr(ismultithreadcapable) if(old < usemultithread) do{
				spinpause();
			}while(atomiclightbarrier.load(std::memory_order_relaxed));
		}
		// handle the top part for floating-point differently
		if(!isabsvalue && isfltpmode && CHAR_BIT * sizeof(T) - 8 == shifter)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(unlikely)
			[[unlikely]]
#endif
		{
handletop8:// this prevents "!isabsvalue && isfltpmode" to be made constexpr here, but that's fine
			ptrdiff_t j;// rounded down in the bottom part, or no multithreading
			if constexpr(!ismultithreadcapable) j = static_cast<ptrdiff_t>((count + 1) >> 2);
			else j = static_cast<ptrdiff_t>((count + 1) >> (2 + usemultithread));
			while(0 <= --j){// fill the array, four at a time
				V *pa{psrclo[0]};
				V *pb{psrclo[1]};
				V *pc{psrclo[2]};
				V *pd{psrclo[3]};
				psrclo += 4;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
				auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
				auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				auto outc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
				auto outd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
				auto[cura, curb, curc, curd]{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd)};
				size_t offseta{offsets[cura + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]++};// the next item will be placed one higher
				size_t offsetb{offsets[curb + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]++};
				size_t offsetc{offsets[curc + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]++};
				size_t offsetd{offsets[curd + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]++};
				pdst[offseta] = pa;
				pdst[offsetb] = pb;
				pdst[offsetc] = pc;
				pdst[offsetd] = pd;
			}
			if(2 & count + 1){// fill in the final two items for a remainder of 2 or 3
				V *pa{psrclo[0]};
				V *pb{psrclo[1]};
				psrclo += 2;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				auto[cura, curb]{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb)};
				size_t offseta{offsets[cura + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]++};// the next item will be placed one higher
				size_t offsetb{offsets[curb + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]++};
				pdst[offseta] = pa;
				pdst[offsetb] = pb;
			}
			if(!(1 & count)){// fill in the final item for odd counts
				V *p{psrclo[0]};
				auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
				auto out{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
				size_t cur{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(out)};
				size_t offset{offsets[cur + (CHAR_BIT * sizeof(T) - 8) * 256 / 8]};
				pdst[offset] = p;
			}
			break;// no further processing beyond the top part
		}
	}
}

// multi-threading companion for the radixsortcopynoallocmulti() function implementation template for multi-part types with indirection
// Do not use this function directly.
template<auto indirection1, bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	64 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>) &&
	8 < CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortcopynoallocmultimtc(size_t count, V *const input[], V *output[], V *buffer[], std::atomic_uintptr_t &atomiclightbarrier, vararguments... varparameters)noexcept{
	using T = std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>;
	// do not pass a nullptr here
	assert(input);
	assert(output);
	assert(buffer);
	static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
	size_t offsetscompanion[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
	radixsortnoallocmultiinitmtc<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, true, V>(count, input, output, buffer, offsetscompanion, varparameters...);

	size_t *offsets;
	{// barrier and pointer exchange with the main thread
		uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsetscompanion))};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed);
			}while(reinterpret_cast<uintptr_t>(offsetscompanion) == other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		offsets = reinterpret_cast<size_t *>(other);// retrieve the pointer
	}

	// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
	auto[runsteps, paritybool]{generateoffsetsmultimtc<isdescsort, isabsvalue, issignmode, isfltpmode, T>(count, offsets, offsetscompanion)};

	{// barrier and (flipped bits) runsteps, paritybool value exchange with the main thread
		// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
		uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + 1};
		while(reinterpret_cast<uintptr_t>(offsetscompanion) == atomiclightbarrier.load(std::memory_order_relaxed)){
			spinpause();// catch up
		}
		uintptr_t other{atomiclightbarrier.fetch_add(compound)};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
			}while(!other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		other += compound;// combine
		unsigned lowercarryoutbits{2 + paritybool};
		paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
		other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
		runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
	}

	// perform the bidirectional 8-bit sorting sequence
	// flip the relevant bits inside runsteps first
	if(runsteps ^= (1u << CHAR_BIT * sizeof(T) / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
		[[likely]]
#endif
	{// perform the bidirectional 8-bit sorting sequence
		V **pdst{buffer}, **pdstnext{output};// for the next iteration
		if(paritybool){// swap if the count of sorting actions to do is odd
			pdst = output;
			pdstnext = buffer;
		}
		radixsortnoallocmultimainmtc<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V>(count, input, pdst, pdstnext, offsetscompanion, runsteps, atomiclightbarrier, varparameters...);
	}
}

// radixsortcopynoalloc() function implementation template for multi-part types with indirection
template<auto indirection1, bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_NORMAL std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	64 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>) &&
	8 < CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortcopynoallocmulti(size_t count, V *const input[], V *output[], V *buffer[], vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	using T = tounifunsigned<std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>>;
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	static bool constexpr ismultithreadcapable{
#ifdef RSBD8_DISABLE_MULTITHREADING
		false
#else
		std::is_nothrow_invocable_v<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>
#endif
	};
	// do not pass a nullptr here, even though it's safe if count is 0
	assert(input);
	assert(output);
	assert(buffer);
	// All the code in this function is adapted for count to be one below its input value here.
	--count;
	if(0 < static_cast<ptrdiff_t>(count)){// a 0 or 1 count array is legal here
		static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
		// conditionally enable multi-threading here
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread;// filled in as a boolean 0 or 1, used as unsigned input later on
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t, std::nullptr_t> atomiclightbarrier;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::future<void>, std::nullptr_t> asynchandle;

		// count the 256 configurations, all in one go
		if constexpr(ismultithreadcapable){
			usemultithread = 0;
			// TODO: fine-tune, right now the threshold is set to the 7-bit limit (the minimum is 1 to 7, depending on the size of T)
			if(0x7Fu < count && 1 < std::thread::hardware_concurrency()){
				try{
					asynchandle = std::async(std::launch::async, radixsortcopynoallocmultimtc<indirection1, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V, vararguments...>, count, input, output, buffer, std::ref(atomiclightbarrier), varparameters...);
					usemultithread = 1;
				}catch(...){// std::async may fail gracefully here
					assert(false);
				}
			}
		}
		size_t offsets[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
		if constexpr(64 == CHAR_BIT * sizeof(T)){
			ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
			[[maybe_unused]]
#endif
			std::conditional_t<ismultithreadcapable, ptrdiff_t, std::nullptr_t> stride;
			if constexpr(ismultithreadcapable){
				stride = -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				i -= stride;
			}
			if constexpr(isrevorder){// also reverse the array at the same time
				V *const *pinput{input};
				if constexpr(ismultithreadcapable) pinput += stride;
				do{
					V *phi{pinput[0]};
					V *plo{pinput[1]};
					pinput += 2;
					output[i] = phi;
					buffer[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					output[i - 1] = plo;
					buffer[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					U curhi6{curhi >> (48 - log2ptrs)};
					curhi >>= 56;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					curhi6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curhi5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curhi6);
					++offsets[7 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					U curlo6{curlo >> (48 - log2ptrs)};
					curlo >>= 56;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					curlo6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curlo5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curlo6);
					++offsets[7 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					V *p{pinput[0]};
					output[i] = p;
					buffer[i] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					U cur5{cur >> (40 - log2ptrs)};
					U cur6{cur >> (48 - log2ptrs)};
					cur >>= 56;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					cur5 &= sizeof(void *) * 0xFFu;
					cur6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + cur5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + cur6);
					++offsets[7 * 256 + static_cast<size_t>(cur)];
				}
			}else{// 64-bit, not in reverse order
				do{
					V *phi{input[i]};
					V *plo{input[i - 1]};
					output[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					output[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					U curhi6{curhi >> (48 - log2ptrs)};
					curhi >>= 56;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					curhi6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curhi5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curhi6);
					++offsets[7 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					U curlo6{curlo >> (48 - log2ptrs)};
					curlo >>= 56;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					curlo6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curlo5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curlo6);
					++offsets[7 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					V *p{input[0]};
					output[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					U cur5{cur >> (40 - log2ptrs)};
					U cur6{cur >> (48 - log2ptrs)};
					cur >>= 56;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					cur5 &= sizeof(void *) * 0xFFu;
					cur6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + cur5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + cur6);
					++offsets[7 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(56 == CHAR_BIT * sizeof(T)){
			ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
			[[maybe_unused]]
#endif
			std::conditional_t<ismultithreadcapable, ptrdiff_t, std::nullptr_t> stride;
			if constexpr(ismultithreadcapable){
				stride = -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				i -= stride;
			}
			if constexpr(isrevorder){// also reverse the array at the same time
				V *const *pinput{input};
				if constexpr(ismultithreadcapable) pinput += stride;
				do{
					V *phi{pinput[0]};
					V *plo{pinput[1]};
					pinput += 2;
					output[i] = phi;
					buffer[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					output[i - 1] = plo;
					buffer[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					curhi >>= 48;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curhi5);
					++offsets[6 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					curlo >>= 48;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curlo5);
					++offsets[6 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					V *p{pinput[0]};
					output[i] = p;
					buffer[i] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					U cur5{cur >> (40 - log2ptrs)};
					cur >>= 48;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					cur5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + cur5);
					++offsets[6 * 256 + static_cast<size_t>(cur)];
				}
			}else{// 56-bit, not in reverse order
				do{
					V *phi{input[i]};
					V *plo{input[i - 1]};
					output[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					output[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					curhi >>= 48;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curhi5);
					++offsets[6 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					curlo >>= 48;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curlo5);
					++offsets[6 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					V *p{input[0]};
					output[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					U cur5{cur >> (40 - log2ptrs)};
					cur >>= 48;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					cur5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + cur5);
					++offsets[6 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(48 == CHAR_BIT * sizeof(T)){
			ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
			[[maybe_unused]]
#endif
			std::conditional_t<ismultithreadcapable, ptrdiff_t, std::nullptr_t> stride;
			if constexpr(ismultithreadcapable){
				stride = -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				i -= stride;
			}
			if constexpr(isrevorder){// also reverse the array at the same time
				V *const *pinput{input};
				if constexpr(ismultithreadcapable) pinput += stride;
				do{
					V *phi{pinput[0]};
					V *plo{pinput[1]};
					pinput += 2;
					output[i] = phi;
					buffer[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					output[i - 1] = plo;
					buffer[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					curhi >>= 40;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++offsets[5 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					curlo >>= 40;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++offsets[5 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					V *p{pinput[0]};
					output[i] = p;
					buffer[i] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					cur >>= 40;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++offsets[5 * 256 + static_cast<size_t>(cur)];
				}
			}else{// 48-bit, not in reverse order
				do{
					V *phi{input[i]};
					V *plo{input[i - 1]};
					output[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					output[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					curhi >>= 40;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++offsets[5 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					curlo >>= 40;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++offsets[5 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					V *p{input[0]};
					output[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					cur >>= 40;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++offsets[5 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(40 == CHAR_BIT * sizeof(T)){
			ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
			[[maybe_unused]]
#endif
			std::conditional_t<ismultithreadcapable, ptrdiff_t, std::nullptr_t> stride;
			if constexpr(ismultithreadcapable){
				stride = -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				i -= stride;
			}
			if constexpr(isrevorder){// also reverse the array at the same time
				V *const *pinput{input};
				if constexpr(ismultithreadcapable) pinput += stride;
				do{
					V *phi{pinput[0]};
					V *plo{pinput[1]};
					pinput += 2;
					output[i] = phi;
					buffer[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					output[i - 1] = plo;
					buffer[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					curhi >>= 32;
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					curlo >>= 32;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++offsets[4 * 256 + static_cast<size_t>(curhi)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++offsets[4 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					V *p{pinput[0]};
					output[i] = p;
					buffer[i] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					cur >>= 32;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++offsets[4 * 256 + static_cast<size_t>(cur)];
				}
			}else{// 40-bit, not in reverse order
				do{
					V *phi{input[i]};
					V *plo{input[i - 1]};
					output[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					output[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					curhi >>= 32;
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					curlo >>= 32;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++offsets[4 * 256 + static_cast<size_t>(curhi)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++offsets[4 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					V *p{input[0]};
					output[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					cur >>= 32;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++offsets[4 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(32 == CHAR_BIT * sizeof(T)){
			ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
			[[maybe_unused]]
#endif
			std::conditional_t<ismultithreadcapable, ptrdiff_t, std::nullptr_t> stride;
			if constexpr(ismultithreadcapable){
				stride = -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				i -= stride;
			}
			if constexpr(isrevorder){// also reverse the array at the same time
				V *const *pinput{input};
				if constexpr(ismultithreadcapable) pinput += stride;
				do{
					V *pa{pinput[0]};
					V *pb{pinput[1]};
					pinput += 2;
					output[i] = pa;
					buffer[i] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					output[i - 1] = pb;
					buffer[i - 1] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					U cur2a{cura >> (16 - log2ptrs)};
					cura >>= 24;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					U cur2b{curb >> (16 - log2ptrs)};
					curb >>= 24;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					cur2a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					cur2b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2a);
					++offsets[3 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2b);
					++offsets[3 * 256 + static_cast<size_t>(curb)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					V *p{pinput[0]};
					output[0] = p;
					buffer[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{static_cast<unsigned>(cur) >> (8 - log2ptrs)};
					U cur2{static_cast<unsigned>(cur) >> (16 - log2ptrs)};
					cur >>= 24;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++offsets[3 * 256 + static_cast<size_t>(cur)];
				}
			}else{// 32-bit, not in reverse order
				do{
					V *pa{input[i]};
					V *pb{input[i - 1]};
					output[i] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					output[i - 1] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					U cur2a{cura >> (16 - log2ptrs)};
					cura >>= 24;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					U cur2b{curb >> (16 - log2ptrs)};
					curb >>= 24;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					cur2a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					cur2b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2a);
					++offsets[3 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2b);
					++offsets[3 * 256 + static_cast<size_t>(curb)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					V *p{input[0]};
					output[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{static_cast<unsigned>(cur) >> (8 - log2ptrs)};
					U cur2{static_cast<unsigned>(cur) >> (16 - log2ptrs)};
					cur >>= 24;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++offsets[3 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(24 == CHAR_BIT * sizeof(T)){
			ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
			[[maybe_unused]]
#endif
			std::conditional_t<ismultithreadcapable, ptrdiff_t, std::nullptr_t> stride;
			if constexpr(ismultithreadcapable){
				stride = -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 3) / 6) * 3;
				i -= stride;
			}
			if constexpr(isrevorder){// also reverse the array at the same time
				V *const *pinput{input};
				if constexpr(ismultithreadcapable) pinput += stride;
				i -= 2;
				if(0 <= i)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
					[[likely]]
#endif
					do{
					V *pa{pinput[0]};
					V *pb{pinput[1]};
					V *pc{pinput[2]};
					pinput += 3;
					output[i + 3] = pa;
					buffer[i + 3] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					output[i + 2] = pb;
					buffer[i + 2] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					output[i + 1] = pc;
					buffer[i + 1] = pc;
					auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					cura >>= 16;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					curb >>= 16;
					U cur0c{curc & 0xFFu};
					U cur1c{curc >> (8 - log2ptrs)};
					curc >>= 16;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[cur0c];
					cur1c &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++offsets[2 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++offsets[2 * 256 + static_cast<size_t>(curb)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1c);
					++offsets[2 * 256 + static_cast<size_t>(curc)];
					i -= 3;
				}while(0 <= i);
				if(2 & i){
					V *pa{pinput[0]};
					V *pb{pinput[1]};
					pinput += 2;
					output[i + 3] = pa;
					buffer[i + 3] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					output[i + 2] = pb;
					buffer[i + 2] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					cura >>= 16;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					curb >>= 16;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++offsets[2 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++offsets[2 * 256 + static_cast<size_t>(curb)];
				}
				if(1 & i){// fill in the final item for odd counts
					V *p{pinput[0]};
					output[0] = p;
					buffer[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{static_cast<unsigned>(cur) >> (8 - log2ptrs)};
					cur >>= 16;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++offsets[2 * 256 + static_cast<size_t>(cur)];
				}
			}else{// 24-bit, not in reverse order
				i -= 2;
				if(0 <= i)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
					[[likely]]
#endif
					do{
					V *pa{input[i + 2]};
					V *pb{input[i + 1]};
					V *pc{input[i]};
					output[i + 2] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					output[i + 1] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					output[i] = pc;
					auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					cura >>= 16;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					curb >>= 16;
					U cur0c{curc & 0xFFu};
					U cur1c{curc >> (8 - log2ptrs)};
					curc >>= 16;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[cur0c];
					cur1c &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++offsets[2 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++offsets[2 * 256 + static_cast<size_t>(curb)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1c);
					++offsets[2 * 256 + static_cast<size_t>(curc)];
					i -= 3;
				}while(0 <= i);
				if(2 & i){// fill in the final two items for a remainder of 2 or 3
					V *pa{input[i + 3]};
					V *pb{input[i + 2]};
					output[i + 3] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					output[i + 2] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					cura >>= 16;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					curb >>= 16;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++offsets[2 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++offsets[2 * 256 + static_cast<size_t>(curb)];
				}
				if(1 & i){// fill in the final item for odd counts
					V *p{input[0]};
					output[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{static_cast<unsigned>(cur) >> (8 - log2ptrs)};
					cur >>= 16;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++offsets[2 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(16 == CHAR_BIT * sizeof(T)){
			ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
			[[maybe_unused]]
#endif
			std::conditional_t<ismultithreadcapable, ptrdiff_t, std::nullptr_t> stride;
			if constexpr(ismultithreadcapable){
				stride = -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 4) >> 3) * 4;
				i -= stride;
			}
			if constexpr(isrevorder){// also reverse the array at the same time
				V *const *pinput{input};
				if constexpr(ismultithreadcapable) pinput += stride;
				i -= 3;
				if(0 <= i)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
					[[likely]]
#endif
					do{
					V *pa{pinput[0]};
					V *pb{pinput[1]};
					V *pc{pinput[2]};
					V *pd{pinput[3]};
					pinput += 4;
					output[i + 3] = pa;
					buffer[i + 3] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					output[i + 2] = pb;
					buffer[i + 2] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					output[i + 1] = pc;
					buffer[i + 1] = pc;
					auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
					output[i] = pd;
					buffer[i] = pd;
					auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
					U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd);
					}
					U cur0a{cura & 0xFFu};
					cura >>= 8;
					U cur0b{curb & 0xFFu};
					curb >>= 8;
					U cur0c{curc & 0xFFu};
					curc >>= 8;
					U cur0d{curd & 0xFFu};
					curd >>= 8;
					++offsets[cur0a];
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[cur0c];
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++offsets[cur0d];
					if constexpr(isabsvalue && issignmode && isfltpmode) curd &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cura)];
					++offsets[256 + static_cast<size_t>(curb)];
					++offsets[256 + static_cast<size_t>(curc)];
					++offsets[256 + static_cast<size_t>(curd)];
					i -= 4;
				}while(0 <= i);
				if(2 & i){
					V *pa{pinput[0]};
					V *pb{pinput[1]};
					pinput += 2;
					output[i + 3] = pa;
					buffer[i + 3] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					output[i + 2] = pb;
					buffer[i + 2] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
					}
					U cur0a{cura & 0xFFu};
					cura >>= 8;
					U cur0b{curb & 0xFFu};
					curb >>= 8;
					++offsets[cur0a];
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cura)];
					++offsets[256 + static_cast<size_t>(curb)];
				}
				if(1 & i){// fill in the final item for odd counts
					V *p{pinput[0]};
					output[0] = p;
					buffer[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					cur >>= 8;
					++offsets[cur0];
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cur)];
				}
			}else{// 16-bit, not in reverse order
				i -= 3;
				if(0 <= i)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
					[[likely]]
#endif
					do{
					V *pa{input[i + 3]};
					V *pb{input[i + 2]};
					V *pc{input[i + 1]};
					V *pd{input[i]};
					output[i + 3] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					output[i + 2] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					output[i + 1] = pc;
					auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
					output[i] = pd;
					auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
					U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd);
					}
					U cur0a{cura & 0xFFu};
					cura >>= 8;
					U cur0b{curb & 0xFFu};
					curb >>= 8;
					U cur0c{curc & 0xFFu};
					curc >>= 8;
					U cur0d{curd & 0xFFu};
					curd >>= 8;
					++offsets[cur0a];
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[cur0c];
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++offsets[cur0d];
					if constexpr(isabsvalue && issignmode && isfltpmode) curd &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cura)];
					++offsets[256 + static_cast<size_t>(curb)];
					++offsets[256 + static_cast<size_t>(curc)];
					++offsets[256 + static_cast<size_t>(curd)];
					i -= 4;
				}while(0 <= i);
				if(2 & i){// fill in the final two items for a remainder of 2 or 3
					V *pa{input[i + 3]};
					V *pb{input[i + 2]};
					output[i + 3] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					output[i + 2] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
					}
					U cur0a{cura & 0xFFu};
					cura >>= 8;
					U cur0b{curb & 0xFFu};
					curb >>= 8;
					++offsets[cur0a];
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cura)];
					++offsets[256 + static_cast<size_t>(curb)];
				}
				if(1 & i){// fill in the final item for odd counts
					V *p{input[0]};
					output[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					cur >>= 8;
					++offsets[cur0];
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cur)];
				}
			}
		}else static_assert(false, "Implementing larger types will require additional work and optimisation for this library.");

		// barrier and pointer exchange with the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, size_t *, std::nullptr_t> offsetscompanion;
		if constexpr(ismultithreadcapable){
			uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsets) & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed);
				}while(reinterpret_cast<uintptr_t>(offsets) == other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			// this will just be zero if usemultithread is zero
			offsetscompanion = reinterpret_cast<size_t *>(other);// retrieve the pointer
		}

		// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
		auto[runsteps, paritybool]{generateoffsetsmulti<isdescsort, isabsvalue, issignmode, isfltpmode, ismultithreadcapable, T>(count, offsets, offsetscompanion, usemultithread)};

		// barrier and (flipped bits) runsteps, paritybool value exchange with the companion thread
		if constexpr(ismultithreadcapable){
			// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
			uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + static_cast<uintptr_t>(usemultithread)};
			while(reinterpret_cast<uintptr_t>(offsets) == atomiclightbarrier.load(std::memory_order_relaxed)){
				spinpause();// catch up
			}
			uintptr_t other{atomiclightbarrier.fetch_add(compound & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
				}while(!other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			other += compound;// combine
			unsigned lowercarryoutbits{2 * usemultithread + paritybool};
			paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
			other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
			runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
		}

		// perform the bidirectional 8-bit sorting sequence
		// flip the relevant bits inside runsteps first
		if(runsteps ^= (1u << CHAR_BIT * sizeof(T) / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
		{
			V **pdst{buffer}, **pdstnext{output};// for the next iteration
			if(paritybool){// swap if the count of sorting actions to do is odd
				pdst = output;
				pdstnext = buffer;
			}
			radixsortnoallocmultimain<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, ismultithreadcapable, V>(count, input, pdst, pdstnext, offsets, runsteps, usemultithread, atomiclightbarrier, varparameters...);
		}
	}
}

// multi-threading companion for the radixsortnoallocmulti() function implementation template for multi-part types with indirection
// Do not use this function directly.
template<auto indirection1, bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	64 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>) &&
	8 < CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortnoallocmultimtc(size_t count, V *input[], V *buffer[], std::atomic_uintptr_t &atomiclightbarrier, vararguments... varparameters)noexcept{
	using T = std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>;
	// do not pass a nullptr here
	assert(input);
	assert(buffer);
	static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
	size_t offsetscompanion[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
	radixsortnoallocmultiinitmtc<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, false, V>(count, input, buffer, nullptr, offsetscompanion, varparameters...);

	size_t *offsets;
	{// barrier and pointer exchange with the main thread
		uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsetscompanion))};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed);
			}while(reinterpret_cast<uintptr_t>(offsetscompanion) == other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		offsets = reinterpret_cast<size_t *>(other);// retrieve the pointer
	}

	// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
	auto[runsteps, paritybool]{generateoffsetsmultimtc<isdescsort, isabsvalue, issignmode, isfltpmode, T>(count, offsets, offsetscompanion)};

	{// barrier and (flipped bits) runsteps, paritybool value exchange with the main thread
		// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
		uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + 1};
		while(reinterpret_cast<uintptr_t>(offsetscompanion) == atomiclightbarrier.load(std::memory_order_relaxed)){
			spinpause();// catch up
		}
		uintptr_t other{atomiclightbarrier.fetch_add(compound)};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
			}while(!other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		other += compound;// combine
		unsigned lowercarryoutbits{2 + paritybool};
		paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
		other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
		runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
	}

	// perform the bidirectional 8-bit sorting sequence
	// flip the relevant bits inside runsteps first
	if(runsteps ^= (1u << CHAR_BIT * sizeof(T) / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
		[[likely]]
#endif
	{// perform the bidirectional 8-bit sorting sequence
		V **psrclo{input}, **pdst{buffer};
		if(paritybool){// swap if the count of sorting actions to do is odd
			psrclo = buffer;
			pdst = input;
		}
		radixsortnoallocmultimainmtc<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V>(count, psrclo, pdst, psrclo, offsetscompanion, runsteps, atomiclightbarrier, varparameters...);
	}
}

// radixsortnoalloc() function implementation template for multi-part types with indirection
template<auto indirection1, bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_NORMAL std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	64 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>) &&
	8 < CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortnoallocmulti(size_t count, V *input[], V *buffer[], bool movetobuffer = false, vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	using T = tounifunsigned<std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>>;
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	static bool constexpr ismultithreadcapable{
#ifdef RSBD8_DISABLE_MULTITHREADING
		false
#else
		std::is_nothrow_invocable_v<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>
#endif
	};
	// do not pass a nullptr here, even though it's safe if count is 0
	assert(input);
	assert(buffer);
	// All the code in this function is adapted for count to be one below its input value here.
	--count;
	if(0 < static_cast<ptrdiff_t>(count)){// a 0 or 1 count array is legal here
		static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode)};// shrink the offsets size if possible
		// conditionally enable multi-threading here
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread;// filled in as a boolean 0 or 1, used as unsigned input later on
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t, std::nullptr_t> atomiclightbarrier;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::future<void>, std::nullptr_t> asynchandle;

		// count the 256 configurations, all in one go
		if constexpr(ismultithreadcapable){
			usemultithread = 0;
			// TODO: fine-tune, right now the threshold is set to the 7-bit limit (the minimum is 1 to 7, depending on the size of T)
			if(0x7Fu < count && 1 < std::thread::hardware_concurrency()){
				try{
					asynchandle = std::async(std::launch::async, radixsortnoallocmultimtc<indirection1, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V, vararguments...>, count, input, buffer, std::ref(atomiclightbarrier), varparameters...);
					usemultithread = 1;
				}catch(...){// std::async may fail gracefully here
					assert(false);
				}
			}
		}
		size_t offsets[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
		if constexpr(64 == CHAR_BIT * sizeof(T)){
			if constexpr(isrevorder){// also reverse the array at the same time
				V **pinputlo, **pinputhi, **pbufferlo, **pbufferhi;
				if constexpr(!ismultithreadcapable){
					pinputlo = input;
					pinputhi = input + count;
					pbufferlo = buffer;
					pbufferhi = buffer + count;
				}else{// if mulitithreaded, the half count will be rounded up in the companion thread
					ptrdiff_t stride{-static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2)};
					pinputlo = input + stride;
					pinputhi = input + (count - stride);
					pbufferlo = buffer + stride;
					pbufferhi = buffer + (count - stride);
				}
				do{
					V *plo{pinputlo[0]};
					V *phi{pinputhi[0]};
					*pinputhi-- = plo;
					*pbufferhi-- = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					*pinputlo++ = phi;
					*pbufferlo++ = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curlo, curhi);
					}
					// register pressure performance issue on several platforms: first do the low half here
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					U curlo6{curlo >> (48 - log2ptrs)};
					curlo >>= 56;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					curlo6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curlo5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curlo6);
					++offsets[7 * 256 + static_cast<size_t>(curlo)];
					// register pressure performance issue on several platforms: do the high half here second
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					U curhi6{curhi >> (48 - log2ptrs)};
					curhi >>= 56;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					curhi6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curhi5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curhi6);
					++offsets[7 * 256 + static_cast<size_t>(curhi)];
				}while(pinputlo < pinputhi);
				if(pinputlo == pinputhi){// fill in the final item for odd counts
					V *p{pinputlo[0]};
					// no write to input, as this is the midpoint
					*pbufferhi = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					U cur5{cur >> (40 - log2ptrs)};
					U cur6{cur >> (48 - log2ptrs)};
					cur >>= 56;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					cur5 &= sizeof(void *) * 0xFFu;
					cur6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + cur5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + cur6);
					++offsets[7 * 256 + static_cast<size_t>(cur)];
				}
			}else{// 64-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				do{
					V *phi{input[i]};
					V *plo{input[i - 1]};
					buffer[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					buffer[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					U curhi6{curhi >> (48 - log2ptrs)};
					curhi >>= 56;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					curhi6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curhi5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curhi6);
					++offsets[7 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					U curlo6{curlo >> (48 - log2ptrs)};
					curlo >>= 56;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					curlo6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curlo5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + curlo6);
					++offsets[7 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					V *p{input[0]};
					buffer[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					U cur5{cur >> (40 - log2ptrs)};
					U cur6{cur >> (48 - log2ptrs)};
					cur >>= 56;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					cur5 &= sizeof(void *) * 0xFFu;
					cur6 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + cur5);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 6 * 256) + cur6);
					++offsets[7 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(56 == CHAR_BIT * sizeof(T)){
			if constexpr(isrevorder){// also reverse the array at the same time
				V **pinputlo, **pinputhi, **pbufferlo, **pbufferhi;
				if constexpr(!ismultithreadcapable){
					pinputlo = input;
					pinputhi = input + count;
					pbufferlo = buffer;
					pbufferhi = buffer + count;
				}else{// if mulitithreaded, the half count will be rounded up in the companion thread
					ptrdiff_t stride{-static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2)};
					pinputlo = input + stride;
					pinputhi = input + (count - stride);
					pbufferlo = buffer + stride;
					pbufferhi = buffer + (count - stride);
				}
				do{
					V *plo{pinputlo[0]};
					V *phi{pinputhi[0]};
					*pinputhi-- = plo;
					*pbufferhi-- = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					*pinputlo++ = phi;
					*pbufferlo++ = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curlo, curhi);
					}
					// register pressure performance issue on several platforms: first do the low half here
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					curlo >>= 48;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curlo5);
					++offsets[6 * 256 + static_cast<size_t>(curlo)];
					// register pressure performance issue on several platforms: do the high half here second
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					curhi >>= 48;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curhi5);
					++offsets[6 * 256 + static_cast<size_t>(curhi)];
				}while(pinputlo < pinputhi);
				if(pinputlo == pinputhi){// fill in the final item for odd counts
					V *p{pinputlo[0]};
					// no write to input, as this is the midpoint
					*pbufferhi = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					U cur5{cur >> (40 - log2ptrs)};
					cur >>= 48;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					cur5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + cur5);
					++offsets[6 * 256 + static_cast<size_t>(cur)];
				}
			}else{// 56-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				do{
					V *phi{input[i]};
					V *plo{input[i - 1]};
					buffer[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					buffer[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					U curhi5{curhi >> (40 - log2ptrs)};
					curhi >>= 48;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					curhi5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curhi5);
					++offsets[6 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					U curlo5{curlo >> (40 - log2ptrs)};
					curlo >>= 48;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					curlo5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + curlo5);
					++offsets[6 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					V *p{input[0]};
					buffer[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, buffer);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					U cur5{cur >> (40 - log2ptrs)};
					cur >>= 48;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					cur5 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 5 * 256) + cur5);
					++offsets[6 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(48 == CHAR_BIT * sizeof(T)){
			if constexpr(isrevorder){// also reverse the array at the same time
				V **pinputlo, **pinputhi, **pbufferlo, **pbufferhi;
				if constexpr(!ismultithreadcapable){
					pinputlo = input;
					pinputhi = input + count;
					pbufferlo = buffer;
					pbufferhi = buffer + count;
				}else{// if mulitithreaded, the half count will be rounded up in the companion thread
					ptrdiff_t stride{-static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2)};
					pinputlo = input + stride;
					pinputhi = input + (count - stride);
					pbufferlo = buffer + stride;
					pbufferhi = buffer + (count - stride);
				}
				do{
					V *plo{pinputlo[0]};
					V *phi{pinputhi[0]};
					*pinputhi-- = plo;
					*pbufferhi-- = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					*pinputlo++ = phi;
					*pbufferlo++ = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curlo, curhi);
					}
					// register pressure performance issue on several platforms: first do the low half here
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					curlo >>= 40;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++offsets[5 * 256 + static_cast<size_t>(curlo)];
					// register pressure performance issue on several platforms: do the high half here second
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					curhi >>= 40;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++offsets[5 * 256 + static_cast<size_t>(curhi)];
				}while(pinputlo < pinputhi);
				if(pinputlo == pinputhi){// fill in the final item for odd counts
					U cur{pinputlo[0]};
					// no write to input, as this is the midpoint
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					cur >>= 40;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++offsets[5 * 256 + static_cast<size_t>(cur)];
				}
			}else{// 48-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				do{
					V *phi{input[i]};
					V *plo{input[i - 1]};
					buffer[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					buffer[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					// register pressure performance issue on several platforms: first do the high half here
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					U curhi4{curhi >> (32 - log2ptrs)};
					curhi >>= 40;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					curhi4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curhi4);
					++offsets[5 * 256 + static_cast<size_t>(curhi)];
					// register pressure performance issue on several platforms: do the low half here second
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					U curlo4{curlo >> (32 - log2ptrs)};
					curlo >>= 40;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					curlo4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + curlo4);
					++offsets[5 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					V *p{input[0]};
					buffer[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					U cur4{cur >> (32 - log2ptrs)};
					cur >>= 40;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					cur4 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 4 * 256) + cur4);
					++offsets[5 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(40 == CHAR_BIT * sizeof(T)){
			if constexpr(isrevorder){// also reverse the array at the same time
				V **pinputlo, **pinputhi, **pbufferlo, **pbufferhi;
				if constexpr(!ismultithreadcapable){
					pinputlo = input;
					pinputhi = input + count;
					pbufferlo = buffer;
					pbufferhi = buffer + count;
				}else{// if mulitithreaded, the half count will be rounded up in the companion thread
					ptrdiff_t stride{-static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2)};
					pinputlo = input + stride;
					pinputhi = input + (count - stride);
					pbufferlo = buffer + stride;
					pbufferhi = buffer + (count - stride);
				}
				do{
					V *plo{pinputlo[0]};
					V *phi{pinputhi[0]};
					*pinputhi-- = plo;
					*pbufferhi-- = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					*pinputlo++ = phi;
					*pbufferlo++ = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curlo, curhi);
					}
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					curlo >>= 32;
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					curhi >>= 32;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++offsets[4 * 256 + static_cast<size_t>(curlo)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++offsets[4 * 256 + static_cast<size_t>(curhi)];
				}while(pinputlo < pinputhi);
				if(pinputlo == pinputhi){// fill in the final item for odd counts
					V *p{pinputlo[0]};
					// no write to input, as this is the midpoint
					*pbufferhi = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					cur >>= 32;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++offsets[4 * 256 + static_cast<size_t>(cur)];
				}
			}else{// 40-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				do{
					V *phi{input[i]};
					V *plo{input[i - 1]};
					buffer[i] = phi;
					auto imhi{indirectinput1<indirection1, isindexed2, T, V>(phi, varparameters...)};
					buffer[i - 1] = plo;
					auto imlo{indirectinput1<indirection1, isindexed2, T, V>(plo, varparameters...)};
					U curhi{indirectinput2<indirection1, indirection2, isindexed2, T>(imhi, varparameters...)};
					U curlo{indirectinput2<indirection1, indirection2, isindexed2, T>(imlo, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curhi, curlo);
					}
					U curhi0{curhi & 0xFFu};
					U curhi1{curhi >> (8 - log2ptrs)};
					U curhi2{curhi >> (16 - log2ptrs)};
					U curhi3{curhi >> (24 - log2ptrs)};
					curhi >>= 32;
					U curlo0{curlo & 0xFFu};
					U curlo1{curlo >> (8 - log2ptrs)};
					U curlo2{curlo >> (16 - log2ptrs)};
					U curlo3{curlo >> (24 - log2ptrs)};
					curlo >>= 32;
					++offsets[curhi0];
					curhi1 &= sizeof(void *) * 0xFFu;
					curhi2 &= sizeof(void *) * 0xFFu;
					curhi3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curhi &= 0x7Fu;
					++offsets[curlo0];
					curlo1 &= sizeof(void *) * 0xFFu;
					curlo2 &= sizeof(void *) * 0xFFu;
					curlo3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curlo &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curhi1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curhi2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curhi3);
					++offsets[4 * 256 + static_cast<size_t>(curhi)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + curlo1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + curlo2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + curlo3);
					++offsets[4 * 256 + static_cast<size_t>(curlo)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					V *p{input[0]};
					buffer[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{cur >> (8 - log2ptrs)};
					U cur2{cur >> (16 - log2ptrs)};
					U cur3{cur >> (24 - log2ptrs)};
					cur >>= 32;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					cur3 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 3 * 256) + cur3);
					++offsets[4 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(32 == CHAR_BIT * sizeof(T)){
			if constexpr(isrevorder){// also reverse the array at the same time
				V **pinputlo, **pinputhi, **pbufferlo, **pbufferhi;
				if constexpr(!ismultithreadcapable){
					pinputlo = input;
					pinputhi = input + count;
					pbufferlo = buffer;
					pbufferhi = buffer + count;
				}else{// if mulitithreaded, the half count will be rounded up in the companion thread
					ptrdiff_t stride{-static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2)};
					pinputlo = input + stride;
					pinputhi = input + (count - stride);
					pbufferlo = buffer + stride;
					pbufferhi = buffer + (count - stride);
				}
				do{
					V *pa{pinputlo[0]};
					V *pb{pinputhi[0]};
					*pinputhi-- = pa;
					*pbufferhi-- = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					*pinputlo++ = pb;
					*pbufferlo++ = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					U cur2a{cura >> (16 - log2ptrs)};
					cura >>= 24;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					U cur2b{curb >> (16 - log2ptrs)};
					curb >>= 24;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					cur2a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					cur2b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2a);
					++offsets[3 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2b);
					++offsets[3 * 256 + static_cast<size_t>(curb)];
				}while(pinputlo < pinputhi);
				if(pinputlo == pinputhi){// fill in the final item for odd counts
					V *p{pinputlo[0]};
					// no write to input, as this is the midpoint
					*pbufferhi = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{static_cast<unsigned>(cur) >> (8 - log2ptrs)};
					U cur2{static_cast<unsigned>(cur) >> (16 - log2ptrs)};
					cur >>= 24;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++offsets[3 * 256 + static_cast<size_t>(cur)];
				}
			}else{// 32-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 2) >> 2) * 2;
				do{
					V *pa{input[i]};
					V *pb{input[i - 1]};
					buffer[i] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					buffer[i - 1] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					U cur2a{cura >> (16 - log2ptrs)};
					cura >>= 24;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					U cur2b{curb >> (16 - log2ptrs)};
					curb >>= 24;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					cur2a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					cur2b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2a);
					++offsets[3 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2b);
					++offsets[3 * 256 + static_cast<size_t>(curb)];
					i -= 2;
				}while(0 < i);
				if(!(1 & i)){// fill in the final item for odd counts
					V *p{input[0]};
					buffer[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{static_cast<unsigned>(cur) >> (8 - log2ptrs)};
					U cur2{static_cast<unsigned>(cur) >> (16 - log2ptrs)};
					cur >>= 24;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					cur2 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 2 * 256) + cur2);
					++offsets[3 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(24 == CHAR_BIT * sizeof(T)){
			if constexpr(isrevorder){// also reverse the array at the same time
				V **pinputlo, **pinputhi, **pbufferlo, **pbufferhi;
				size_t initialcount;
				if constexpr(!ismultithreadcapable){
					initialcount = count + 1;
					pinputlo = input;
					pinputhi = input + count;
					pbufferlo = buffer;
					pbufferhi = buffer + count;
				}else{// if mulitithreaded, the half count will be rounded up in the companion thread
					ptrdiff_t stride{-static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 6) / 12)};
					initialcount = count - stride + 1;
					pinputlo = input + stride;
					pinputhi = input + (count - stride);
					pbufferlo = buffer + stride;
					pbufferhi = buffer + (count - stride);
				}
				initialcount %= 6;
				if(4 & initialcount){// possibly initialise with 4 entries before the loop below
					V *pa{pinputlo[0]};
					V *pb{pinputhi[0]};
					V *pc{pinputlo[1]};
					V *pd{pinputhi[-1]};
					pinputhi[0] = pa;
					pbufferhi[0] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					pinputlo[0] = pb;
					pbufferlo[0] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					pinputhi[1] = pc;
					pinputhi -= 2;
					pbufferhi[1] = pc;
					pbufferhi -= 2;
					auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
					pinputlo[-1] = pd;
					pinputlo += 2;
					pbufferlo[-1] = pd;
					pbufferlo += 2;
					auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
					U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					cura >>= 16;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					curb >>= 16;
					U cur0c{curc & 0xFFu};
					U cur1c{curc >> (8 - log2ptrs)};
					curc >>= 16;
					U cur0d{curd & 0xFFu};
					U cur1d{curd >> (8 - log2ptrs)};
					curd >>= 16;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[cur0c];
					cur1c &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++offsets[cur0d];
					cur1d &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curd &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++offsets[2 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++offsets[2 * 256 + static_cast<size_t>(curb)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1c);
					++offsets[2 * 256 + static_cast<size_t>(curc)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1d);
					++offsets[2 * 256 + static_cast<size_t>(curd)];
				}else if(2 & initialcount){// possibly initialise with 2 entries before the loop below
					V *pa{pinputlo[0]};
					V *pb{pinputhi[0]};
					*pinputhi-- = pa;
					*pbufferhi-- = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					*pinputlo++ = pb;
					*pbufferlo++ = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					cura >>= 16;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					curb >>= 16;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++offsets[2 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++offsets[2 * 256 + static_cast<size_t>(curb)];
				}
				if(5 <= count)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
					[[likely]]
#endif
					do{
					V *pa{pinputlo[0]};
					V *pb{pinputhi[0]};
					V *pc{pinputlo[1]};
					V *pd{pinputhi[-1]};
					pinputhi[0] = pa;
					pbufferhi[0] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					pinputlo[0] = pb;
					pbufferlo[0] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					pinputhi[1] = pc;
					pbufferhi[1] = pc;
					auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
					// register pressure performance issue on several platforms: first do the high half here
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					cura >>= 16;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					curb >>= 16;
					U cur0c{curc & 0xFFu};
					U cur1c{curc >> (8 - log2ptrs)};
					curc >>= 16;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[cur0c];
					cur1c &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++offsets[2 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++offsets[2 * 256 + static_cast<size_t>(curb)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1c);
					++offsets[2 * 256 + static_cast<size_t>(curc)];
					V *pe{pinputlo[2]};
					V *pf{pinputhi[-2]};
					pinputlo[1] = pd;
					pbufferlo[1] = pd;
					auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
					pinputhi[-2] = pe;
					pinputhi -= 3;
					pbufferhi[-2] = pe;
					pbufferhi -= 3;
					auto ime{indirectinput1<indirection1, isindexed2, T, V>(pe, varparameters...)};
					pinputlo[2] = pf;
					pinputlo += 3;
					pbufferlo[2] = pf;
					pbufferlo += 3;
					auto imf{indirectinput1<indirection1, isindexed2, T, V>(pf, varparameters...)};
					U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
					U cure{indirectinput2<indirection1, indirection2, isindexed2, T>(ime, varparameters...)};
					U curf{indirectinput2<indirection1, indirection2, isindexed2, T>(ime, varparameters...)};
					// register pressure performance issue on several platforms: do the low half here second
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(curd, cure, curf);
					}
					U cur0d{curd & 0xFFu};
					U cur1d{curd >> (8 - log2ptrs)};
					curd >>= 16;
					U cur0e{cure & 0xFFu};
					U cur1e{cure >> (8 - log2ptrs)};
					cure >>= 16;
					U cur0f{curf & 0xFFu};
					U cur1f{curf >> (8 - log2ptrs)};
					curf >>= 16;
					++offsets[cur0d];
					cur1d &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curd &= 0x7Fu;
					++offsets[cur0e];
					cur1e &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cure &= 0x7Fu;
					++offsets[cur0f];
					cur1f &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curf &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1d);
					++offsets[2 * 256 + static_cast<size_t>(curd)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1e);
					++offsets[2 * 256 + static_cast<size_t>(cure)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1f);
					++offsets[2 * 256 + static_cast<size_t>(curf)];
				}while(pinputlo < pinputhi);
				if(pinputlo == pinputhi){// fill in the final item for odd counts
					V *p{pinputlo[0]};
					// no write to input, as this is the midpoint
					*pbufferhi = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{static_cast<unsigned>(cur) >> (8 - log2ptrs)};
					cur >>= 16;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++offsets[2 * 256 + static_cast<size_t>(cur)];
				}
			}else{// 24-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 3) / 6) * 3;
				i -= 2;
				if(0 <= i)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
					[[likely]]
#endif
					do{
					V *pa{input[i + 2]};
					V *pb{input[i + 1]};
					V *pc{input[i]};
					buffer[i + 2] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					buffer[i + 1] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					buffer[i] = pc;
					auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					cura >>= 16;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					curb >>= 16;
					U cur0c{curc & 0xFFu};
					U cur1c{curc >> (8 - log2ptrs)};
					curc >>= 16;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[cur0c];
					cur1c &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++offsets[2 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++offsets[2 * 256 + static_cast<size_t>(curb)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1c);
					++offsets[2 * 256 + static_cast<size_t>(curc)];
					i -= 3;
				}while(0 <= i);
				if(2 & i){// fill in the final two items for a remainder of 2 or 3
					V *pa{input[i + 2]};
					V *pb{input[i + 1]};
					buffer[i + 2] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					buffer[i + 1] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
					}
					U cur0a{cura & 0xFFu};
					U cur1a{cura >> (8 - log2ptrs)};
					cura >>= 16;
					U cur0b{curb & 0xFFu};
					U cur1b{curb >> (8 - log2ptrs)};
					curb >>= 16;
					++offsets[cur0a];
					cur1a &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					cur1b &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1a);
					++offsets[2 * 256 + static_cast<size_t>(cura)];
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1b);
					++offsets[2 * 256 + static_cast<size_t>(curb)];
				}
				if(1 & i){// fill in the final item for odd counts
					V *p{input[0]};
					buffer[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					U cur1{static_cast<unsigned>(cur) >> (8 - log2ptrs)};
					cur >>= 16;
					++offsets[cur0];
					cur1 &= sizeof(void *) * 0xFFu;
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++*reinterpret_cast<size_t *>(reinterpret_cast<std::byte *>(offsets + 256) + cur1);
					++offsets[2 * 256 + static_cast<size_t>(cur)];
				}
			}
		}else if constexpr(16 == CHAR_BIT * sizeof(T)){
			if constexpr(isrevorder){// also reverse the array at the same time
				V **pinputlo, **pinputhi, **pbufferlo, **pbufferhi;
				if constexpr(!ismultithreadcapable){
					pinputlo = input;
					pinputhi = input + count;
					pbufferlo = buffer;
					pbufferhi = buffer + count;
				}else{// if mulitithreaded, the half count will be rounded up in the companion thread
					ptrdiff_t stride{-static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 4) >> 3)};
					pinputlo = input + stride;
					pinputhi = input + (count - stride);
					pbufferlo = buffer + stride;
					pbufferhi = buffer + (count - stride);
				}
				if(2 & count + 1){// possibly initialise with 2 entries before the loop below
					V *pa{pinputlo[0]};
					V *pb{pinputhi[0]};
					*pinputhi-- = pa;
					*pbufferhi-- = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					*pinputlo++ = pb;
					*pbufferlo++ = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
					}
					U cur0a{cura & 0xFFu};
					cura >>= 8;
					U cur0b{curb & 0xFFu};
					curb >>= 8;
					++offsets[cur0a];
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cura)];
					++offsets[256 + static_cast<size_t>(curb)];
				}
				if(3 <= count)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
					[[likely]]
#endif
					do{
					V *pa{pinputlo[0]};
					V *pb{pinputhi[0]};
					V *pc{pinputlo[1]};
					V *pd{pinputhi[-1]};
					pinputhi[0] = pa;
					pbufferhi[0] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					pinputlo[0] = pb;
					pbufferlo[0] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					pinputhi[-1] = pc;
					pinputhi -= 2;
					pbufferhi[-1] = pc;
					pbufferhi -= 2;
					auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
					pinputlo[1] = pd;
					pinputlo += 2;
					pbufferlo[1] = pd;
					pbufferlo += 2;
					auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
					U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd);
					}
					U cur0a{cura & 0xFFu};
					cura >>= 8;
					U cur0b{curb & 0xFFu};
					curb >>= 8;
					U cur0c{curc & 0xFFu};
					curc >>= 8;
					U cur0d{curd & 0xFFu};
					curd >>= 8;
					++offsets[cur0a];
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[cur0c];
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++offsets[cur0d];
					if constexpr(isabsvalue && issignmode && isfltpmode) curd &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cura)];
					++offsets[256 + static_cast<size_t>(curb)];
					++offsets[256 + static_cast<size_t>(curc)];
					++offsets[256 + static_cast<size_t>(curd)];
				}while(pinputlo < pinputhi);
				if(pinputlo == pinputhi){// fill in the final item for odd counts
					V *p{pinputlo[0]};
					// no write to input, as this is the midpoint
					*pbufferhi = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					cur >>= 8;
					++offsets[cur0];
					if constexpr(isabsvalue && issignmode && isfltpmode) cur &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cur)];
				}
			}else{// 16-bit, not in reverse order
				ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
				if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 4) >> 3) * 4;
				i -= 3;
				if(0 <= i)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
					[[likely]]
#endif
					do{
					V *pa{input[i + 3]};
					V *pb{input[i + 2]};
					V *pc{input[i + 1]};
					V *pd{input[i]};
					buffer[i + 3] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					buffer[i + 2] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					buffer[i + 1] = pc;
					auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
					buffer[i] = pd;
					auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
					U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd);
					}
					U cur0a{cura & 0xFFu};
					cura >>= 8;
					U cur0b{curb & 0xFFu};
					curb >>= 8;
					U cur0c{curc & 0xFFu};
					curc >>= 8;
					U cur0d{curd & 0xFFu};
					curd >>= 8;
					++offsets[cur0a];
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[cur0c];
					if constexpr(isabsvalue && issignmode && isfltpmode) curc &= 0x7Fu;
					++offsets[cur0d];
					if constexpr(isabsvalue && issignmode && isfltpmode) curd &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cura)];
					++offsets[256 + static_cast<size_t>(curb)];
					++offsets[256 + static_cast<size_t>(curc)];
					++offsets[256 + static_cast<size_t>(curd)];
					i -= 4;
				}while(0 <= i);
				if(2 & i){// fill in the final two items for a remainder of 2 or 3
					V *pa{input[i + 3]};
					V *pb{input[i + 2]};
					buffer[i + 3] = pa;
					auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
					buffer[i + 2] = pb;
					auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
					U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
					U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
					}
					U cur0a{cura & 0xFFu};
					cura >>= 8;
					U cur0b{curb & 0xFFu};
					curb >>= 8;
					++offsets[cur0a];
					if constexpr(isabsvalue && issignmode && isfltpmode) cura &= 0x7Fu;
					++offsets[cur0b];
					if constexpr(isabsvalue && issignmode && isfltpmode) curb &= 0x7Fu;
					++offsets[256 + static_cast<size_t>(cura)];
					++offsets[256 + static_cast<size_t>(curb)];
				}
				if(1 & i){// fill in the final item for odd counts
					V *p{input[0]};
					buffer[0] = p;
					auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
					U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
					if constexpr(isfltpmode != isabsvalue || isabsvalue && !issignmode){
						filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
					}
					U cur0{cur & 0xFFu};
					cur >>= 8;
					++offsets[cur0];
					++offsets[256 + static_cast<size_t>(cur)];
				}
			}
		}else static_assert(false, "Implementing larger types will require additional work and optimisation for this library.");

		// barrier and pointer exchange with the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, size_t *, std::nullptr_t> offsetscompanion;
		if constexpr(ismultithreadcapable){
			uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsets) & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed);
				}while(reinterpret_cast<uintptr_t>(offsets) == other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			// this will just be zero if usemultithread is zero
			offsetscompanion = reinterpret_cast<size_t *>(other);// retrieve the pointer
		}

		// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
		auto[runsteps, paritybool]{generateoffsetsmulti<isdescsort, isabsvalue, issignmode, isfltpmode, ismultithreadcapable, T>(count, offsets, offsetscompanion, usemultithread, movetobuffer)};

		// barrier and (flipped bits) runsteps, paritybool value exchange with the companion thread
		if constexpr(ismultithreadcapable){
			// paritybool is either 0 or 1 here, so we can pack it together with runsteps and add usemultithread on top
			uintptr_t compound{static_cast<uintptr_t>(runsteps) * 2 + static_cast<uintptr_t>(paritybool) + static_cast<uintptr_t>(usemultithread)};
			while(reinterpret_cast<uintptr_t>(offsets) == atomiclightbarrier.load(std::memory_order_relaxed)){
				spinpause();// catch up
			}
			uintptr_t other{atomiclightbarrier.fetch_add(compound & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed) - compound;
				}while(!other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			other += compound;// combine
			unsigned lowercarryoutbits{2 * usemultithread + paritybool};
			paritybool = static_cast<unsigned>(other) & 1;// piece together the parity from both threads
			other -= lowercarryoutbits;// this will remove possiby two bits of carry-out before the next right shift
			runsteps = static_cast<unsigned>(other >> 1);// this can shift out a 0 or a 1 bit here, depending on the leftovers of parity
		}

		// perform the bidirectional 8-bit sorting sequence
		// flip the relevant bits inside runsteps first
		if(runsteps ^= (1u << CHAR_BIT * sizeof(T) / 8) - 1)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
		{
			V **psrclo{input}, **pdst{buffer};
			if(paritybool){// swap if the count of sorting actions to do is odd
				psrclo = buffer;
				pdst = input;
			}
			radixsortnoallocmultimain<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, ismultithreadcapable, V>(count, psrclo, pdst, psrclo, offsets, runsteps, usemultithread, atomiclightbarrier, varparameters...);
		}
	}
}

// Function implementation templates for single-part types

// initialisation part, multi-threading companion for the radixsortcopynoallocsingle() and radixsortnoallocsingle() function implementation templates for single-part types without indirection
// Do not use this function directly.
template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	!std::is_same_v<bool, T> &&
	8 >= CHAR_BIT * sizeof(T),
	void> radixsortnoallocsingleinitmtc(size_t count, T const input[], T pout[], size_t offsetscompanion[])noexcept{
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(3 <= count);// this function is not for small arrays, 4 is the minimum original array count
	// do not pass a nullptr here
	assert(input);
	assert(pout);
	assert(offsetscompanion);
	// unsigned counter, not zero inclusive inside the loop
	size_t i{((count + 1 + 8) >> 4) * 8};// rounded up in the companion thread
	input += count - i;
	pout += count - i;
	do{
		U cura{input[i]};
		U curb{input[i - 1]};
		U curc{input[i - 2]};
		U curd{input[i - 3]};
		if constexpr(isabsvalue != isfltpmode){// two-register filters only
			// register pressure performance issue on several platforms: first do the high half here
			filterinput<isabsvalue, issignmode, isfltpmode, T>(
				cura, pout + i,
				curb, pout + i - 1,
				curc, pout + i - 2,
				curd, pout + i - 3);
			++offsetscompanion[cura];
			++offsetscompanion[curb];
			++offsetscompanion[curc];
			++offsetscompanion[curd];
		}
		U cure{input[i - 4]};
		U curf{input[i - 5]};
		U curg{input[i - 6]};
		U curh{input[i - 7]};
		if constexpr(isabsvalue != isfltpmode){// two-register filters only
			// register pressure performance issue on several platforms: do the low half here second
			filterinput<isabsvalue, issignmode, isfltpmode, T>(
				cure, pout + i - 4,
				curf, pout + i - 5,
				curg, pout + i - 6,
				curh, pout + i - 7);
			++offsetscompanion[cure];
			++offsetscompanion[curf];
			++offsetscompanion[curg];
			++offsetscompanion[curh];
		}else{
			pout[i] = static_cast<T>(cura);
			pout[i - 1] = static_cast<T>(curb);
			pout[i - 2] = static_cast<T>(curc);
			pout[i - 3] = static_cast<T>(curd);
			pout[i - 4] = static_cast<T>(cure);
			pout[i - 5] = static_cast<T>(curf);
			pout[i - 6] = static_cast<T>(curg);
			pout[i - 7] = static_cast<T>(curh);
			if constexpr(isabsvalue && isfltpmode){// one-register filters only
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd, cure, curf, curg, curh);
			}
			++offsetscompanion[cura];
			++offsetscompanion[curb];
			++offsetscompanion[curc];
			++offsetscompanion[curd];
			++offsetscompanion[cure];
			++offsetscompanion[curf];
			++offsetscompanion[curg];
			++offsetscompanion[curh];
		}
	}while(i -= 8);
}

// main part, multi-threading companion for the radixsortcopynoallocsingle() and radixsortnoallocsingle() function implementation templates for single-part types without indirection
// Do not use this function directly.
template<bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	!std::is_same_v<bool, T> &&
	8 >= CHAR_BIT * sizeof(T),
	void> radixsortnoallocsinglemainmtc(size_t count, T const psrclo[], T pdst[], size_t offsetscompanion[])noexcept{
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(3 <= count);// this function is not for small arrays, 4 is the minimum original array count
	// do not pass a nullptr here
	assert(psrclo);
	assert(pdst);
	assert(offsetscompanion);
	T const *psrchi{psrclo + count};
	size_t j{(count + 1 + 4) >> 3};// rounded up in the top part
	do{// fill the array, four at a time
		U outa{psrchi[0]};
		U outb{psrchi[-1]};
		U outc{psrchi[-2]};
		U outd{psrchi[-3]};
		psrchi -= 4;
		auto[cura, curb, curc, curd]{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd)};
		size_t offseta, offsetb, offsetc, offsetd;// this is only allowed for the single-part version, containing just one sorting pass
		if constexpr(false){// useless if not using indirection: isrevorder){
			offseta = offsetscompanion[cura]++;// the next item will be placed one higher
			offsetb = offsetscompanion[curb]++;
			offsetc = offsetscompanion[curc]++;
			offsetd = offsetscompanion[curd]++;
		}else{
			offseta = offsetscompanion[cura]--;// the next item will be placed one lower
			offsetb = offsetscompanion[curb]--;
			offsetc = offsetscompanion[curc]--;
			offsetd = offsetscompanion[curd]--;
		}
		pdst[offseta] = static_cast<T>(outa);
		pdst[offsetb] = static_cast<T>(outb);
		pdst[offsetc] = static_cast<T>(outc);
		pdst[offsetd] = static_cast<T>(outd);
	}while(--j);
}

// main part for the radixsortcopynoallocsingle() and radixsortnoallocsingle() function implementation templates for single-part types without indirection
// Do not use this function directly.
template<bool isabsvalue, bool issignmode, bool isfltpmode, bool ismultithreadcapable, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	!std::is_same_v<bool, T> &&
	8 >= CHAR_BIT * sizeof(T),
	void> radixsortnoallocsinglemain(size_t count, T const psrclo[], T pdst[], size_t offsets[], std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread)noexcept{
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(count && count != MAXSIZE_T);
	// do not pass a nullptr here
	assert(psrclo);
	assert(pdst);
	assert(offsets);
	ptrdiff_t j;// rounded down in the bottom part, or no multithreading
	if constexpr(!ismultithreadcapable) j = static_cast<ptrdiff_t>((count + 1) >> 2);
	else j = static_cast<ptrdiff_t>((count + 1) >> (2 + usemultithread));
	while(0 <= --j){// fill the array, four at a time
		U outa{psrclo[0]};
		U outb{psrclo[1]};
		U outc{psrclo[2]};
		U outd{psrclo[3]};
		psrclo += 4;
		auto[cura, curb, curc, curd]{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd)};
		size_t offseta, offsetb, offsetc, offsetd;// this is only allowed for the single-part version, containing just one sorting pass
		if constexpr(false){// useless if not using indirection: isrevorder){
			offseta = offsets[cura]--;// the next item will be placed one lower
			offsetb = offsets[curb]--;
			offsetc = offsets[curc]--;
			offsetd = offsets[curd]--;
		}else{
			offseta = offsets[cura]++;// the next item will be placed one higher
			offsetb = offsets[curb]++;
			offsetc = offsets[curc]++;
			offsetd = offsets[curd]++;
		}
		pdst[offseta] = static_cast<T>(outa);
		pdst[offsetb] = static_cast<T>(outb);
		pdst[offsetc] = static_cast<T>(outc);
		pdst[offsetd] = static_cast<T>(outd);
	}
	if(2 & count + 1){// fill in the final two items for a remainder of 2 or 3
		U outa{psrclo[0]};
		U outb{psrclo[1]};
		psrclo += 2;
		auto[cura, curb]{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb)};
		size_t offseta, offsetb;// this is only allowed for the single-part version, containing just one sorting pass
		if constexpr(false){// useless if not using indirection: isrevorder){
			offseta = offsets[cura]--;// the next item will be placed one lower
			offsetb = offsets[curb]--;
		}else{
			offseta = offsets[cura]++;// the next item will be placed one higher
			offsetb = offsets[curb]++;
		}
		pdst[offseta] = static_cast<T>(outa);
		pdst[offsetb] = static_cast<T>(outb);
	}
	if(!(1 & count)){// fill in the final item for odd counts
		U out{psrclo[0]};
		size_t cur{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(out)};
		size_t offset;// this is only allowed for the single-part version, containing just one sorting pass
		if constexpr(false){// useless if not using indirection: isrevorder){
			offset = offsets[cur]--;// the next item will be placed one lower
		}else{
			offset = offsets[cur]++;// the next item will be placed one higher
		}
		pdst[offset] = static_cast<T>(out);
	}
}

// main part, multi-threading companion for the radixsortcopynoallocsingle() function implementation template for single-part types without indirection
// Do not use this function directly.
template<bool isdescsort, bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	!std::is_same_v<bool, T> &&
	8 >= CHAR_BIT * sizeof(T),
	void> radixsortcopynoallocsinglemtc(size_t count, T const input[], T output[], std::atomic_uintptr_t &atomiclightbarrier)noexcept{
	// do not pass a nullptr here
	assert(input);
	assert(output);
	static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode) - (!isabsvalue && issignmode && isfltpmode)};// shrink the offsets size if possible
	size_t offsetscompanion[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
	radixsortnoallocsingleinitmtc<isabsvalue, issignmode, isfltpmode, T>(count, input, output, offsetscompanion);

	size_t *offsets;
	{// barrier and pointer exchange with the main thread
		uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsetscompanion))};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed);
			}while(reinterpret_cast<uintptr_t>(offsetscompanion) == other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		offsets = reinterpret_cast<size_t *>(other);// retrieve the pointer
	}

	// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
	unsigned allareidentical{generateoffsetssinglemtc<isdescsort, false, isabsvalue, issignmode, isfltpmode>(count, offsets, offsetscompanion)};// isrevorder is set to false because it's useless when not using indirection

	{// barrier and allareidentical value exchange with the main thread
		++allareidentical;// send over a 1 or a 2
		while(reinterpret_cast<uintptr_t>(offsetscompanion) == atomiclightbarrier.load(std::memory_order_relaxed)){
			spinpause();// catch up
		}
		uintptr_t other{atomiclightbarrier.fetch_add(allareidentical)};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed) - allareidentical;
			}while(!other);
		}
		// only one of the two threads can set the all are identical state
		allareidentical -= static_cast<unsigned>(other);// codes:
		// 0: continue processing
		// -1: input from the main thread
		// 1: input from this thread
	}

	if(!allareidentical)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
		[[likely]]
#endif
	{// perform the bidirectional 8-bit sorting sequence
		radixsortnoallocsinglemainmtc<isabsvalue, issignmode, isfltpmode, T>(count, input, output, offsetscompanion);
	}
}

// radixsortcopynoalloc() function implementation template for single-part types without indirection
template<bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_NORMAL std::enable_if_t<
	std::is_unsigned_v<T> &&
	!std::is_same_v<bool, T> &&
	8 >= CHAR_BIT * sizeof(T),
	void> radixsortcopynoallocsingle(size_t count, T const input[], T output[])noexcept{
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	static bool constexpr ismultithreadcapable{
#ifdef RSBD8_DISABLE_MULTITHREADING
		false
#else
		true
#endif
	};
	// do not pass a nullptr here, even though it's safe if count is 0
	assert(input);
	assert(output);
	// All the code in this function is adapted for count to be one below its input value here.
	--count;
	if(0 < static_cast<ptrdiff_t>(count)){// a 0 or 1 count array is legal here
		static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode) - (!isabsvalue && issignmode && isfltpmode)};// shrink the offsets size if possible
		// conditionally enable multi-threading here
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread;// filled in as a boolean 0 or 1, used as unsigned input later on
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t, std::nullptr_t> atomiclightbarrier;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::future<void>, std::nullptr_t> asynchandle;

		// count the 256 configurations, all in one go
		if constexpr(ismultithreadcapable){
			usemultithread = 0;
			// TODO: fine-tune, right now the threshold is set to the 7-bit limit (the minimum is 15)
			if(0x7Fu < count && 1 < std::thread::hardware_concurrency()){
				try{
					asynchandle = std::async(std::launch::async, radixsortcopynoallocsinglemtc<isdescsort, isabsvalue, issignmode, isfltpmode, T>, count, input, output, std::ref(atomiclightbarrier));
					usemultithread = 1;
				}catch(...){// std::async may fail gracefully here
					assert(false);
				}
			}
		}
		size_t offsets[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
		ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
		if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 8) >> 4) * 8;
		i -= 7;
		if(0 <= i)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
			do{
			U cura{input[i + 7]};
			U curb{input[i + 6]};
			U curc{input[i + 5]};
			U curd{input[i + 4]};
			if constexpr(isabsvalue != isfltpmode){// two-register filters only
				// register pressure performance issue on several platforms: first do the high half here
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd);
				++offsets[cura];
				++offsets[curb];
				++offsets[curc];
				++offsets[curd];
			}
			U cure{input[i + 3]};
			U curf{input[i + 2]};
			U curg{input[i + 1]};
			U curh{input[i]};
			if constexpr(isabsvalue != isfltpmode){// two-register filters only
				// register pressure performance issue on several platforms: do the low half here second
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cure, curf, curg, curh);
			}else{
				if constexpr(isabsvalue && isfltpmode){// one-register filters only
					filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd, cure, curf, curg, curh);
				}
				++offsets[cura];
				++offsets[curb];
				++offsets[curc];
				++offsets[curd];
			}
			++offsets[cure];
			++offsets[curf];
			++offsets[curg];
			++offsets[curh];
			i -= 8;
		}while(0 <= i);
		if(4 & i){// fill in the final four items for a remainder of 4 to 7
			U cura{input[i + 7]};
			U curb{input[i + 6]};
			U curc{input[i + 5]};
			U curd{input[i + 4]};
			i -= 4;// required for the "if(2 & i){" part
			if constexpr(isabsvalue || isfltpmode){
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd);
			}
			++offsets[cura];
			++offsets[curb];
			++offsets[curc];
			++offsets[curd];
		}
		if(2 & i){// fill in the final two items for a remainder of 2 or 3
			U cura{input[i + 7]};
			U curb{input[i + 6]};
			if constexpr(isabsvalue || isfltpmode){
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
			}
			++offsets[cura];
			++offsets[curb];
		}
		if(1 & i){// fill in the final item for odd counts
			U cur{input[0]};
			if constexpr(isabsvalue || isfltpmode){
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
			}
			++offsets[cur];
		}

		// barrier and pointer exchange with the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, size_t *, std::nullptr_t> offsetscompanion;
		if constexpr(ismultithreadcapable){
			uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsets) & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed);
				}while(reinterpret_cast<uintptr_t>(offsets) == other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			// this will just be zero if usemultithread is zero
			offsetscompanion = reinterpret_cast<size_t *>(other);// retrieve the pointer
		}

		// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
		unsigned allareidentical{generateoffsetssingle<isdescsort, false, isabsvalue, issignmode, isfltpmode, ismultithreadcapable>(count, offsets, offsetscompanion, usemultithread)};// isrevorder is set to false because it's useless when not using indirection

		// barrier and allareidentical value exchange with the companion thread
		if constexpr(ismultithreadcapable){
			allareidentical += usemultithread;// send over a 1 or a 2 when multithreading
			while(reinterpret_cast<uintptr_t>(offsets) == atomiclightbarrier.load(std::memory_order_relaxed)){
				spinpause();// catch up
			}
			uintptr_t other{atomiclightbarrier.fetch_add(allareidentical)};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed) - allareidentical;
				}while(!other);
			}
			// only one of the two threads can set the all are identical state
			allareidentical -= static_cast<unsigned>(other);// codes:
			// 0: continue processing
			// -1: input from the companion thread
			// 1: input from this thread
		}

		if(!allareidentical)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
		{// perform the bidirectional 8-bit sorting sequence
			radixsortnoallocsinglemain<isabsvalue, issignmode, isfltpmode, ismultithreadcapable, T>(count, input, output, offsets, usemultithread);
		}
	}
}

// main part, multi-threading companion for the radixsortnoallocsingle() function implementation template for single-part types without indirection
// Do not use this function directly.
template<bool isdescsort, bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_unsigned_v<T> &&
	!std::is_same_v<bool, T> &&
	8 >= CHAR_BIT * sizeof(T),
	void> radixsortnoallocsinglemtc(size_t count, T input[], T buffer[], std::atomic_uintptr_t &atomiclightbarrier)noexcept{
	static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode) - (!isabsvalue && issignmode && isfltpmode)};// shrink the offsets size if possible
	// do not pass a nullptr here, even though it's safe if count is 0
	assert(input);
	assert(buffer);
	size_t offsetscompanion[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
	radixsortnoallocsingleinitmtc<isabsvalue, issignmode, isfltpmode, T>(count, input, buffer, offsetscompanion);

	size_t *offsets;
	{// barrier and pointer exchange with the main thread
		uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsetscompanion))};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed);
			}while(reinterpret_cast<uintptr_t>(offsetscompanion) == other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		offsets = reinterpret_cast<size_t *>(other);// retrieve the pointer
	}

	// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
	unsigned allareidentical{generateoffsetssinglemtc<isdescsort, false, isabsvalue, issignmode, isfltpmode>(count, offsets, offsetscompanion)};// isrevorder is set to false because it's useless when not using indirection

	{// barrier and allareidentical value exchange with the main thread
		++allareidentical;// send over a 1 or a 2
		while(reinterpret_cast<uintptr_t>(offsetscompanion) == atomiclightbarrier.load(std::memory_order_relaxed)){
			spinpause();// catch up
		}
		uintptr_t other{atomiclightbarrier.fetch_add(allareidentical)};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed) - allareidentical;
			}while(!other);
		}
		// only one of the two threads can set the all are identical state
		allareidentical -= static_cast<unsigned>(other);// codes:
		// 0: continue processing
		// -1: input from the main thread
		// 1: input from this thread
	}

	if(!allareidentical)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
		[[likely]]
#endif
	{// perform the bidirectional 8-bit sorting sequence
		radixsortnoallocsinglemainmtc<isabsvalue, issignmode, isfltpmode, T>(count, buffer, input, offsetscompanion);
	}
}

// radixsortnoalloc() function implementation template for single-part types without indirection
template<bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, typename T>
RSBD8_FUNC_NORMAL std::enable_if_t<
	std::is_unsigned_v<T> &&
	!std::is_same_v<bool, T> &&
	8 >= CHAR_BIT * sizeof(T),
	void> radixsortnoallocsingle(size_t count, T input[], T buffer[])noexcept{
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	static bool constexpr ismultithreadcapable{
#ifdef RSBD8_DISABLE_MULTITHREADING
		false
#else
		true
#endif
	};
	// do not pass a nullptr here, even though it's safe if count is 0
	assert(input);
	assert(buffer);
	// All the code in this function is adapted for count to be one below its input value here.
	--count;
	if(0 < static_cast<ptrdiff_t>(count)){// a 0 or 1 count array is legal here
		static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode) - (!isabsvalue && issignmode && isfltpmode)};// shrink the offsets size if possible
		// conditionally enable multi-threading here
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread;// filled in as a boolean 0 or 1, used as unsigned input later on
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t, std::nullptr_t> atomiclightbarrier;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::future<void>, std::nullptr_t> asynchandle;

		// count the 256 configurations, all in one go
		if constexpr(ismultithreadcapable){
			usemultithread = 0;
			// TODO: fine-tune, right now the threshold is set to the 7-bit limit (the minimum is 15)
			if(0x7Fu < count && 1 < std::thread::hardware_concurrency()){
				try{
					asynchandle = std::async(std::launch::async, radixsortnoallocsinglemtc<isdescsort, isabsvalue, issignmode, isfltpmode, T>, count, input, buffer, std::ref(atomiclightbarrier));
					usemultithread = 1;
				}catch(...){// std::async may fail gracefully here
					assert(false);
				}
			}
		}
		size_t offsets[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
		ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
		if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 8) >> 4) * 8;
		i -= 7;
		if(0 <= i)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
			do{
			U cura{input[i + 7]};
			U curb{input[i + 6]};
			U curc{input[i + 5]};
			U curd{input[i + 4]};
			if constexpr(isabsvalue != isfltpmode){// two-register filters only
				// register pressure performance issue on several platforms: first do the high half here
				filterinput<isabsvalue, issignmode, isfltpmode, T>(
					cura, buffer + i + 7,
					curb, buffer + i + 6,
					curc, buffer + i + 5,
					curd, buffer + i + 4);
				++offsets[cura];
				++offsets[curb];
				++offsets[curc];
				++offsets[curd];
			}
			U cure{input[i + 3]};
			U curf{input[i + 2]};
			U curg{input[i + 1]};
			U curh{input[i]};
			if constexpr(isabsvalue != isfltpmode){// two-register filters only
				// register pressure performance issue on several platforms: do the low half here second
				filterinput<isabsvalue, issignmode, isfltpmode, T>(
					cure, buffer + i + 3,
					curf, buffer + i + 2,
					curg, buffer + i + 1,
					curh, buffer + i);
				++offsets[cure];
				++offsets[curf];
				++offsets[curg];
			}else if constexpr(isabsvalue && isfltpmode){// one-register filters only
				filterinput<isabsvalue, issignmode, isfltpmode, T>(
					cura, buffer + i + 7,
					curb, buffer + i + 6,
					curc, buffer + i + 5,
					curd, buffer + i + 4,
					cure, buffer + i + 3,
					curf, buffer + i + 2,
					curg, buffer + i + 1,
					curh, buffer + i);
				++offsets[cura];
				++offsets[curb];
				++offsets[curc];
				++offsets[curd];
				++offsets[cure];
				++offsets[curf];
				++offsets[curg];
			}else{
				buffer[i + 7] = static_cast<T>(cura);
				++offsets[cura];
				buffer[i + 6] = static_cast<T>(curb);
				++offsets[curb];
				buffer[i + 5] = static_cast<T>(curc);
				++offsets[curc];
				buffer[i + 4] = static_cast<T>(curd);
				++offsets[curd];
				buffer[i + 3] = static_cast<T>(cure);
				++offsets[cure];
				buffer[i + 2] = static_cast<T>(curf);
				++offsets[curf];
				buffer[i + 1] = static_cast<T>(curg);
				++offsets[curg];
				buffer[i] = static_cast<T>(curh);
			}
			++offsets[curh];
			i -= 8;
		}while(0 <= i);
		if(4 & i){// fill in the final four items for a remainder of 4 to 7
			U cura{input[i + 7]};
			U curb{input[i + 6]};
			U curc{input[i + 5]};
			U curd{input[i + 4]};
			if constexpr(isabsvalue || isfltpmode){
				filterinput<isabsvalue, issignmode, isfltpmode, T>(
					cura, buffer + i + 7,
					curb, buffer + i + 6,
					curc, buffer + i + 5,
					curd, buffer + i + 4);
				i -= 4;// required for the "if(2 & i){" part
				++offsets[cura];
				++offsets[curb];
				++offsets[curc];
			}else{
				buffer[i + 7] = static_cast<T>(cura);
				++offsets[cura];
				buffer[i + 6] = static_cast<T>(curb);
				++offsets[curb];
				buffer[i + 5] = static_cast<T>(curc);
				++offsets[curc];
				buffer[i + 4] = static_cast<T>(curd);
				i -= 4;// required for the "if(2 & i){" part
			}
			++offsets[curd];
		}
		if(2 & i){// fill in the final two items for a remainder of 2 or 3
			U cura{input[i + 7]};
			U curb{input[i + 6]};
			if constexpr(isabsvalue || isfltpmode){
				filterinput<isabsvalue, issignmode, isfltpmode, T>(
					cura, buffer + i + 7,
					curb, buffer + i + 6);
				++offsets[cura];
			}else{
				buffer[i + 7] = static_cast<T>(cura);
				++offsets[cura];
				buffer[i + 6] = static_cast<T>(curb);
			}
			++offsets[curb];
		}
		if(1 & i){// fill in the final item for odd counts
			U cur{input[0]};
			if constexpr(isabsvalue || isfltpmode){
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cur, buffer);
			}else buffer[0] = static_cast<T>(cur);
			++offsets[cur];
		}

		// barrier and pointer exchange with the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, size_t *, std::nullptr_t> offsetscompanion;
		if constexpr(ismultithreadcapable){
			uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsets) & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed);
				}while(reinterpret_cast<uintptr_t>(offsets) == other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			// this will just be zero if usemultithread is zero
			offsetscompanion = reinterpret_cast<size_t *>(other);// retrieve the pointer
		}

		// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
		unsigned allareidentical{generateoffsetssingle<isdescsort, false, isabsvalue, issignmode, isfltpmode, ismultithreadcapable>(count, offsets, offsetscompanion, usemultithread)};// isrevorder is set to false because it's useless when not using indirection

		// barrier and allareidentical value exchange with the companion thread
		if constexpr(ismultithreadcapable){
			allareidentical += usemultithread;// send over a 1 or a 2 when multithreading
			while(reinterpret_cast<uintptr_t>(offsets) == atomiclightbarrier.load(std::memory_order_relaxed)){
				spinpause();// catch up
			}
			uintptr_t other{atomiclightbarrier.fetch_add(allareidentical)};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed) - allareidentical;
				}while(!other);
			}
			// only one of the two threads can set the all are identical state
			allareidentical -= static_cast<unsigned>(other);// codes:
			// 0: continue processing
			// -1: input from the companion thread
			// 1: input from this thread
		}

		if(!allareidentical)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
		{// perform the bidirectional 8-bit sorting sequence
			radixsortnoallocsinglemain<isabsvalue, issignmode, isfltpmode, ismultithreadcapable, T>(count, buffer, input, offsets, usemultithread);
		}
	}
}

// initialisation part, multi-threading companion for the radixsortcopynoallocsingle() and radixsortnoallocsingle() function implementation templates for single-part types with indirection
// Do not use this function directly.
template<auto indirection1, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_function_pointer_v<decltype(indirection1)> &&
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortnoallocsingleinitmtc(size_t count, V *const input[], V *pout[], size_t offsetscompanion[], vararguments... varparameters)noexcept{
	using T = tounifunsigned<std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>>;
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(15 <= count);// this function is not for small arrays, 16 is the minimum original array count
	// do not pass a nullptr here
	assert(input);
	assert(pout);
	assert(offsetscompanion);
	// unsigned counter, not zero inclusive inside the loop
	size_t i{((count + 1 + 8) >> 4) * 8};// rounded up in the companion thread
	input += count - i;
	pout += count - i;
	do{
		V *pa{input[i]};
		V *pb{input[i - 1]};
		V *pc{input[i - 2]};
		V *pd{input[i - 3]};
		if constexpr(isabsvalue != isfltpmode){// two-register filters only
			pout[i] = pa;
			auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
			pout[i - 1] = pb;
			auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
			pout[i - 2] = pc;
			auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
			pout[i - 3] = pd;
			auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
			U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
			U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
			U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
			U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
			// register pressure performance issue on several platforms: first do the high half here
			filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd);
			++offsetscompanion[cura];
			++offsetscompanion[curb];
			++offsetscompanion[curc];
			++offsetscompanion[curd];
		}
		V *pe{input[i - 4]};
		V *pf{input[i - 5]};
		V *pg{input[i - 6]};
		V *ph{input[i - 7]};
		if constexpr(isabsvalue != isfltpmode){// two-register filters only
			pout[i - 4] = pe;
			auto ime{indirectinput1<indirection1, isindexed2, T, V>(pe, varparameters...)};
			pout[i - 5] = pf;
			auto imf{indirectinput1<indirection1, isindexed2, T, V>(pf, varparameters...)};
			pout[i - 6] = pg;
			auto img{indirectinput1<indirection1, isindexed2, T, V>(pg, varparameters...)};
			pout[i - 7] = ph;
			auto imh{indirectinput1<indirection1, isindexed2, T, V>(ph, varparameters...)};
			U cure{indirectinput2<indirection1, indirection2, isindexed2, T>(ime, varparameters...)};
			U curf{indirectinput2<indirection1, indirection2, isindexed2, T>(imf, varparameters...)};
			U curg{indirectinput2<indirection1, indirection2, isindexed2, T>(img, varparameters...)};
			U curh{indirectinput2<indirection1, indirection2, isindexed2, T>(imh, varparameters...)};
			// register pressure performance issue on several platforms: do the low half here second
			filterinput<isabsvalue, issignmode, isfltpmode, T>(cure, curf, curg, curh);
			++offsetscompanion[cure];
			++offsetscompanion[curf];
			++offsetscompanion[curg];
			++offsetscompanion[curh];
		}else{
			pout[i] = pa;
			auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
			pout[i - 1] = pb;
			auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
			pout[i - 2] = pc;
			auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
			pout[i - 3] = pd;
			auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
			pout[i - 4] = pe;
			auto ime{indirectinput1<indirection1, isindexed2, T, V>(pe, varparameters...)};
			pout[i - 5] = pf;
			auto imf{indirectinput1<indirection1, isindexed2, T, V>(pf, varparameters...)};
			pout[i - 6] = pg;
			auto img{indirectinput1<indirection1, isindexed2, T, V>(pg, varparameters...)};
			pout[i - 7] = ph;
			auto imh{indirectinput1<indirection1, isindexed2, T, V>(ph, varparameters...)};
			U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
			U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
			U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
			U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
			U cure{indirectinput2<indirection1, indirection2, isindexed2, T>(ime, varparameters...)};
			U curf{indirectinput2<indirection1, indirection2, isindexed2, T>(imf, varparameters...)};
			U curg{indirectinput2<indirection1, indirection2, isindexed2, T>(img, varparameters...)};
			U curh{indirectinput2<indirection1, indirection2, isindexed2, T>(imh, varparameters...)};
			if constexpr(isabsvalue && isfltpmode){// one-register filters only
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd, cure, curf, curg, curh);
			}
			++offsetscompanion[cura];
			++offsetscompanion[curb];
			++offsetscompanion[curc];
			++offsetscompanion[curd];
			++offsetscompanion[cure];
			++offsetscompanion[curf];
			++offsetscompanion[curg];
			++offsetscompanion[curh];
		}
	}while(i -= 8);
}

// main part, multi-threading companion for the radixsortcopynoallocsingle() and radixsortnoallocsingle() function implementation templates for single-part types with indirection
// Do not use this function directly.
template<auto indirection1, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_function_pointer_v<decltype(indirection1)> &&
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortnoallocsinglemainmtc(size_t count, V *const psrclo[], V *pdst[], size_t offsetscompanion[], vararguments... varparameters)noexcept{
	using T = tounifunsigned<std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>>;
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	assert(15 <= count);// this function is not for small arrays, 16 is the minimum original array count
	// do not pass a nullptr here
	assert(psrclo);
	assert(pdst);
	assert(offsetscompanion);
	V *const *psrchi{psrclo + count};
	size_t j{(count + 1 + 4) >> 3};// rounded up in the top part
	do{// fill the array, four at a time
		V *pa{psrchi[0]};
		V *pb{psrchi[-1]};
		V *pc{psrchi[-2]};
		V *pd{psrchi[-3]};
		psrchi -= 4;
		auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
		auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
		auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
		auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
		auto outa{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
		auto outb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
		auto outc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
		auto outd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
		auto[cura, curb, curc, curd]{filtertop8<isabsvalue, issignmode, isfltpmode, T, U>(outa, outb, outc, outd)};
		size_t offseta, offsetb, offsetc, offsetd;// this is only allowed for the single-part version, containing just one sorting pass
		if constexpr(isrevorder){
			offseta = offsetscompanion[cura]++;// the next item will be placed one higher
			offsetb = offsetscompanion[curb]++;
			offsetc = offsetscompanion[curc]++;
			offsetd = offsetscompanion[curd]++;
		}else{
			offseta = offsetscompanion[cura]--;// the next item will be placed one lower
			offsetb = offsetscompanion[curb]--;
			offsetc = offsetscompanion[curc]--;
			offsetd = offsetscompanion[curd]--;
		}
		pdst[offseta] = pa;
		pdst[offsetb] = pb;
		pdst[offsetc] = pc;
		pdst[offsetd] = pd;
	}while(--j);
}

// multi-threading companion for the radixsortcopynoallocsingle() function implementation template for single-part types with indirection
// Do not use this function directly.
template<auto indirection1, bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_function_pointer_v<decltype(indirection1)> &&
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortcopynoallocsinglemtc(size_t count, V *const input[], V *output[], std::atomic_uintptr_t &atomiclightbarrier, vararguments... varparameters)noexcept{
	using T = tounifunsigned<std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>>;
	// do not pass a nullptr here
	assert(input);
	assert(output);
	static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode) - (!isabsvalue && issignmode && isfltpmode)};// shrink the offsets size if possible
	size_t offsetscompanion[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
	radixsortnoallocsingleinitmtc<indirection1, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V>(count, input, output, offsetscompanion, varparameters...);

	size_t *offsets;
	{// barrier and pointer exchange with the main thread
		uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsetscompanion))};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed);
			}while(reinterpret_cast<uintptr_t>(offsetscompanion) == other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		offsets = reinterpret_cast<size_t *>(other);// retrieve the pointer
	}

	// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
	unsigned allareidentical{generateoffsetssinglemtc<isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, T>(count, offsets, offsetscompanion)};

	{// barrier and allareidentical value exchange with the main thread
		++allareidentical;// send over a 1 or a 2
		while(reinterpret_cast<uintptr_t>(offsetscompanion) == atomiclightbarrier.load(std::memory_order_relaxed)){
			spinpause();// catch up
		}
		uintptr_t other{atomiclightbarrier.fetch_add(allareidentical)};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed) - allareidentical;
			}while(!other);
		}
		// only one of the two threads can set the all are identical state
		allareidentical -= static_cast<unsigned>(other);// codes:
		// 0: continue processing
		// -1: input from the main thread
		// 1: input from this thread
	}

	if(!allareidentical)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
		[[likely]]
#endif
	{// perform the bidirectional 8-bit sorting sequence
		radixsortnoallocsinglemainmtc<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V>(count, input, output, offsetscompanion, varparameters...);
	}
}

// radixsortcopynoalloc() function implementation template for single-part types with indirection
template<auto indirection1, bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_NORMAL std::enable_if_t<
	std::is_member_function_pointer_v<decltype(indirection1)> &&
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortcopynoallocsingle(size_t count, V *const input[], V *output[], vararguments... varparameters)	noexcept(std::is_nothrow_invocable_v<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	using T = tounifunsigned<std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>>;
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	static bool constexpr ismultithreadcapable{
#ifdef RSBD8_DISABLE_MULTITHREADING
		false
#else
		std::is_nothrow_invocable_v<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>
#endif
	};
	// do not pass a nullptr here, even though it's safe if count is 0
	assert(input);
	assert(output);
	// All the code in this function is adapted for count to be one below its input value here.
	--count;
	if(0 < static_cast<ptrdiff_t>(count)){// a 0 or 1 count array is legal here
		static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode) - (!isabsvalue && issignmode && isfltpmode)};// shrink the offsets size if possible
		// conditionally enable multi-threading here
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread;// filled in as a boolean 0 or 1, used as unsigned input later on
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t, std::nullptr_t> atomiclightbarrier;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::future<void>, std::nullptr_t> asynchandle;

		// count the 256 configurations, all in one go
		if constexpr(ismultithreadcapable){
			usemultithread = 0;
			// TODO: fine-tune, right now the threshold is set to the 7-bit limit (the minimum is 15)
			if(0x7Fu < count && 1 < std::thread::hardware_concurrency()){
				try{
					asynchandle = std::async(std::launch::async, radixsortcopynoallocsinglemtc<indirection1, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V, vararguments...>, count, input, output, std::ref(atomiclightbarrier), varparameters...);
					usemultithread = 1;
				}catch(...){// std::async may fail gracefully here
					assert(false);
				}
			}
		}
		size_t offsets[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
		ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
		if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 8) >> 4) * 8;
		i -= 7;
		if(0 <= i)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
			do{
			V *pa{input[i + 7]};
			V *pb{input[i + 6]};
			V *pc{input[i + 5]};
			V *pd{input[i + 4]};
			if constexpr(isabsvalue != isfltpmode){// two-register filters only
				output[i + 7] = pa;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				output[i + 6] = pb;
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				output[i + 5] = pc;
				auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
				output[i + 4] = pd;
				auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
				U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
				U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
				// register pressure performance issue on several platforms: first do the high half here
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd);
				++offsets[cura];
				++offsets[curb];
				++offsets[curc];
				++offsets[curd];
			}
			V *pe{input[i + 3]};
			V *pf{input[i + 2]};
			V *pg{input[i + 1]};
			V *ph{input[i]};
			if constexpr(isabsvalue != isfltpmode){// two-register filters only
				output[i + 3] = pe;
				auto ime{indirectinput1<indirection1, isindexed2, T, V>(pe, varparameters...)};
				output[i + 2] = pf;
				auto imf{indirectinput1<indirection1, isindexed2, T, V>(pf, varparameters...)};
				output[i + 1] = pg;
				auto img{indirectinput1<indirection1, isindexed2, T, V>(pg, varparameters...)};
				output[i] = ph;
				auto imh{indirectinput1<indirection1, isindexed2, T, V>(ph, varparameters...)};
				U cure{indirectinput2<indirection1, indirection2, isindexed2, T>(ime, varparameters...)};
				U curf{indirectinput2<indirection1, indirection2, isindexed2, T>(imf, varparameters...)};
				U curg{indirectinput2<indirection1, indirection2, isindexed2, T>(img, varparameters...)};
				U curh{indirectinput2<indirection1, indirection2, isindexed2, T>(imh, varparameters...)};
				// register pressure performance issue on several platforms: do the low half here second
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cure, curf, curg, curh);
				++offsets[cure];
				++offsets[curf];
				++offsets[curg];
				++offsets[curh];
			}else{
				output[i + 7] = pa;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				output[i + 6] = pb;
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				output[i + 5] = pc;
				auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
				output[i + 4] = pd;
				auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
				output[i + 3] = pe;
				auto ime{indirectinput1<indirection1, isindexed2, T, V>(pe, varparameters...)};
				output[i + 2] = pf;
				auto imf{indirectinput1<indirection1, isindexed2, T, V>(pf, varparameters...)};
				output[i + 1] = pg;
				auto img{indirectinput1<indirection1, isindexed2, T, V>(pg, varparameters...)};
				output[i] = ph;
				auto imh{indirectinput1<indirection1, isindexed2, T, V>(ph, varparameters...)};
				U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
				U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
				U cure{indirectinput2<indirection1, indirection2, isindexed2, T>(ime, varparameters...)};
				U curf{indirectinput2<indirection1, indirection2, isindexed2, T>(imf, varparameters...)};
				U curg{indirectinput2<indirection1, indirection2, isindexed2, T>(img, varparameters...)};
				U curh{indirectinput2<indirection1, indirection2, isindexed2, T>(imh, varparameters...)};
				if constexpr(isabsvalue && isfltpmode){// one-register filters only
					filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd, cure, curf, curg, curh);
				}
				++offsets[cura];
				++offsets[curb];
				++offsets[curc];
				++offsets[curd];
				++offsets[cure];
				++offsets[curf];
				++offsets[curg];
				++offsets[curh];
			}
			i -= 8;
		}while(0 <= i);
		if(4 & i){// fill in the final four items for a remainder of 4 to 7
			V *pa{input[i + 7]};
			V *pb{input[i + 6]};
			V *pc{input[i + 5]};
			V *pd{input[i + 4]};
			output[i + 7] = pa;
			auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
			output[i + 6] = pb;
			auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
			output[i + 5] = pc;
			auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
			output[i + 4] = pd;
			i -= 4;// required for the "if(2 & i){" part
			auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
			U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
			U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
			U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
			U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
			if constexpr(isabsvalue || isfltpmode){
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd);
			}
			++offsets[cura];
			++offsets[curb];
			++offsets[curc];
			++offsets[curd];
		}
		if(2 & i){// fill in the final two items for a remainder of 2 or 3
			V *pa{input[i + 7]};
			V *pb{input[i + 6]};
			output[i + 7] = pa;
			auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
			output[i + 6] = pb;
			auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
			U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
			U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
			if constexpr(isabsvalue || isfltpmode){
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
			}
			++offsets[cura];
			++offsets[curb];
		}
		if(1 & i){// fill in the final item for odd counts
			V *p{input[0]};
			output[0] = p;
			auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
			U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
			if constexpr(isabsvalue || isfltpmode){
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
			}
			++offsets[cur];
		}

		// barrier and pointer exchange with the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, size_t *, std::nullptr_t> offsetscompanion;
		if constexpr(ismultithreadcapable){
			uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsets) & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed);
				}while(reinterpret_cast<uintptr_t>(offsets) == other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			// this will just be zero if usemultithread is zero
			offsetscompanion = reinterpret_cast<size_t *>(other);// retrieve the pointer
		}

		// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
		unsigned allareidentical{generateoffsetssingle<isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, ismultithreadcapable>(count, offsets, offsetscompanion, usemultithread)};

		// barrier and allareidentical value exchange with the companion thread
		if constexpr(ismultithreadcapable){
			allareidentical += usemultithread;// send over a 1 or a 2 when multithreading
			while(reinterpret_cast<uintptr_t>(offsets) == atomiclightbarrier.load(std::memory_order_relaxed)){
				spinpause();// catch up
			}
			uintptr_t other{atomiclightbarrier.fetch_add(allareidentical)};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed) - allareidentical;
				}while(!other);
			}
			// only one of the two threads can set the all are identical state
			allareidentical -= static_cast<unsigned>(other);// codes:
			// 0: continue processing
			// -1: input from the companion thread
			// 1: input from this thread
		}

		if(!allareidentical)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
		{// perform the bidirectional 8-bit sorting sequence
			radixsortnoallocsinglemain<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, ismultithreadcapable, V>(count, input, output, offsets, usemultithread, varparameters...);
		}
	}
}

// multi-threading companion for the radixsortnoallocsingle() function implementation template for single-part types with indirection
// Do not use this function directly.
template<auto indirection1, bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_function_pointer_v<decltype(indirection1)> &&
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortnoallocsinglemtc(size_t count, V *input[], V *buffer[], std::atomic_uintptr_t &atomiclightbarrier, vararguments... varparameters)noexcept{
	using T = tounifunsigned<std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>>;
	// do not pass a nullptr here
	assert(input);
	assert(buffer);
	static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode) - (!isabsvalue && issignmode && isfltpmode)};// shrink the offsets size if possible
	size_t offsetscompanion[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
	radixsortnoallocsingleinitmtc<indirection1, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V>(count, input, buffer, offsetscompanion, varparameters...);

	size_t *offsets;
	{// barrier and pointer exchange with the main thread
		uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsetscompanion))};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed);
			}while(reinterpret_cast<uintptr_t>(offsetscompanion) == other);
			// reset the barrier after use, only one thread will do this
			// no busy-wait dependency on this store, hence relaxed memory order is fine
			reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
			// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
		}
		offsets = reinterpret_cast<size_t *>(other);// retrieve the pointer
	}

	// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
	unsigned allareidentical{generateoffsetssinglemtc<isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, T>(count, offsets, offsetscompanion)};

	{// barrier and allareidentical value exchange with the main thread
		++allareidentical;// send over a 1 or a 2
		while(reinterpret_cast<uintptr_t>(offsetscompanion) == atomiclightbarrier.load(std::memory_order_relaxed)){
			spinpause();// catch up
		}
		uintptr_t other{atomiclightbarrier.fetch_add(allareidentical)};
		if(!other){
			do{
				spinpause();
				other = atomiclightbarrier.load(std::memory_order_relaxed) - allareidentical;
			}while(!other);
		}
		// only one of the two threads can set the all are identical state
		allareidentical -= static_cast<unsigned>(other);// codes:
		// 0: continue processing
		// -1: input from the main thread
		// 1: input from this thread
	}

	if(!allareidentical)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
		[[likely]]
#endif
	{// perform the bidirectional 8-bit sorting sequence
		radixsortnoallocsinglemainmtc<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V>(count, buffer, input, offsetscompanion, varparameters...);
	}
}

// radixsortnoalloc() function implementation template for single-part types with indirection
template<auto indirection1, bool isdescsort, bool isrevorder, bool isabsvalue, bool issignmode, bool isfltpmode, ptrdiff_t indirection2, bool isindexed2, typename V, typename... vararguments>
RSBD8_FUNC_NORMAL std::enable_if_t<
	std::is_member_function_pointer_v<decltype(indirection1)> &&
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortnoallocsingle(size_t count, V *input[], V *buffer[], vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	using T = tounifunsigned<std::remove_pointer_t<std::decay_t<memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>>;
	using U = std::conditional_t<sizeof(T) < sizeof(unsigned), unsigned, T>;// assume zero-extension to be basically free for U on basically all modern machines
	static bool constexpr ismultithreadcapable{
#ifdef RSBD8_DISABLE_MULTITHREADING
		false
#else
		std::is_nothrow_invocable_v<decltype(splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>
#endif
	};
	// do not pass a nullptr here, even though it's safe if count is 0
	assert(input);
	assert(buffer);
	// All the code in this function is adapted for count to be one below its input value here.
	--count;
	if(0 < static_cast<ptrdiff_t>(count)){// a 0 or 1 count array is legal here
		static size_t constexpr offsetsstride{CHAR_BIT * sizeof(T) * 256 / 8 - (isabsvalue && issignmode) * (127 + isfltpmode) - (!isabsvalue && issignmode && isfltpmode)};// shrink the offsets size if possible
		// conditionally enable multi-threading here
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, unsigned, std::nullptr_t> usemultithread;// filled in as a boolean 0 or 1, used as unsigned input later on
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::atomic_uintptr_t, std::nullptr_t> atomiclightbarrier;
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, std::future<void>, std::nullptr_t> asynchandle;

		// count the 256 configurations, all in one go
		if constexpr(ismultithreadcapable){
			usemultithread = 0;
			// TODO: fine-tune, right now the threshold is set to the 7-bit limit (the minimum is 15)
			if(0x7Fu < count && 1 < std::thread::hardware_concurrency()){
				try{
					asynchandle = std::async(std::launch::async, radixsortnoallocsinglemtc<indirection1, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V, vararguments...>, count, input, buffer, std::ref(atomiclightbarrier), varparameters...);
					usemultithread = 1;
				}catch(...){// std::async may fail gracefully here
					assert(false);
				}
			}
		}
		size_t offsets[offsetsstride]{};// a sizeable amount of indices, but it's worth it, zeroed in advance here
		ptrdiff_t i{static_cast<ptrdiff_t>(count)};// if mulitithreaded, the half count will be rounded up in the companion thread
		if constexpr(ismultithreadcapable) i -= -static_cast<ptrdiff_t>(usemultithread) & static_cast<ptrdiff_t>((count + 1 + 8) >> 4) * 8;
		i -= 7;
		if(0 <= i)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
			do{
			V *pa{input[i + 7]};
			V *pb{input[i + 6]};
			V *pc{input[i + 5]};
			V *pd{input[i + 4]};
			if constexpr(isabsvalue != isfltpmode){// two-register filters only
				buffer[i + 7] = pa;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				buffer[i + 6] = pb;
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				buffer[i + 5] = pc;
				auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
				buffer[i + 4] = pd;
				auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
				U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
				U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
				// register pressure performance issue on several platforms: first do the high half here
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd);
				++offsets[cura];
				++offsets[curb];
				++offsets[curc];
				++offsets[curd];
			}
			V *pe{input[i + 3]};
			V *pf{input[i + 2]};
			V *pg{input[i + 1]};
			V *ph{input[i]};
			if constexpr(isabsvalue != isfltpmode){// two-register filters only
				buffer[i + 3] = pe;
				auto ime{indirectinput1<indirection1, isindexed2, T, V>(pe, varparameters...)};
				buffer[i + 2] = pf;
				auto imf{indirectinput1<indirection1, isindexed2, T, V>(pf, varparameters...)};
				buffer[i + 1] = pg;
				auto img{indirectinput1<indirection1, isindexed2, T, V>(pg, varparameters...)};
				buffer[i] = ph;
				auto imh{indirectinput1<indirection1, isindexed2, T, V>(ph, varparameters...)};
				U cure{indirectinput2<indirection1, indirection2, isindexed2, T>(ime, varparameters...)};
				U curf{indirectinput2<indirection1, indirection2, isindexed2, T>(imf, varparameters...)};
				U curg{indirectinput2<indirection1, indirection2, isindexed2, T>(img, varparameters...)};
				U curh{indirectinput2<indirection1, indirection2, isindexed2, T>(imh, varparameters...)};
				// register pressure performance issue on several platforms: do the low half here second
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cure, curf, curg, curh);
				++offsets[cure];
				++offsets[curf];
				++offsets[curg];
				++offsets[curh];
			}else{
				buffer[i + 7] = pa;
				auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
				buffer[i + 6] = pb;
				auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
				buffer[i + 5] = pc;
				auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
				buffer[i + 4] = pd;
				auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
				buffer[i + 3] = pe;
				auto ime{indirectinput1<indirection1, isindexed2, T, V>(pe, varparameters...)};
				buffer[i + 2] = pf;
				auto imf{indirectinput1<indirection1, isindexed2, T, V>(pf, varparameters...)};
				buffer[i + 1] = pg;
				auto img{indirectinput1<indirection1, isindexed2, T, V>(pg, varparameters...)};
				buffer[i] = ph;
				auto imh{indirectinput1<indirection1, isindexed2, T, V>(ph, varparameters...)};
				U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
				U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
				U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
				U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
				U cure{indirectinput2<indirection1, indirection2, isindexed2, T>(ime, varparameters...)};
				U curf{indirectinput2<indirection1, indirection2, isindexed2, T>(imf, varparameters...)};
				U curg{indirectinput2<indirection1, indirection2, isindexed2, T>(img, varparameters...)};
				U curh{indirectinput2<indirection1, indirection2, isindexed2, T>(imh, varparameters...)};
				if constexpr(isabsvalue && isfltpmode){// one-register filters only
					filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd, cure, curf, curg, curh);
				}
				++offsets[cura];
				++offsets[curb];
				++offsets[curc];
				++offsets[curd];
				++offsets[cure];
				++offsets[curf];
				++offsets[curg];
				++offsets[curh];
			}
			i -= 8;
		}while(0 <= i);
		if(4 & i){// fill in the final four items for a remainder of 4 to 7
			V *pa{input[i + 7]};
			V *pb{input[i + 6]};
			V *pc{input[i + 5]};
			V *pd{input[i + 4]};
			buffer[i + 7] = pa;
			auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
			buffer[i + 6] = pb;
			auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
			buffer[i + 5] = pc;
			auto imc{indirectinput1<indirection1, isindexed2, T, V>(pc, varparameters...)};
			buffer[i + 4] = pd;
			i -= 4;// required for the "if(2 & i){" part
			auto imd{indirectinput1<indirection1, isindexed2, T, V>(pd, varparameters...)};
			U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
			U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
			U curc{indirectinput2<indirection1, indirection2, isindexed2, T>(imc, varparameters...)};
			U curd{indirectinput2<indirection1, indirection2, isindexed2, T>(imd, varparameters...)};
			if constexpr(isabsvalue || isfltpmode){
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb, curc, curd);
			}
			++offsets[cura];
			++offsets[curb];
			++offsets[curc];
			++offsets[curd];
		}
		if(2 & i){// fill in the final two items for a remainder of 2 or 3
			V *pa{input[i + 7]};
			V *pb{input[i + 6]};
			buffer[i + 7] = pa;
			auto ima{indirectinput1<indirection1, isindexed2, T, V>(pa, varparameters...)};
			buffer[i + 6] = pb;
			auto imb{indirectinput1<indirection1, isindexed2, T, V>(pb, varparameters...)};
			U cura{indirectinput2<indirection1, indirection2, isindexed2, T>(ima, varparameters...)};
			U curb{indirectinput2<indirection1, indirection2, isindexed2, T>(imb, varparameters...)};
			if constexpr(isabsvalue || isfltpmode){
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cura, curb);
			}
			++offsets[cura];
			++offsets[curb];
		}
		if(1 & i){// fill in the final item for odd counts
			V *p{input[0]};
			buffer[0] = p;
			auto im{indirectinput1<indirection1, isindexed2, T, V>(p, varparameters...)};
			U cur{indirectinput2<indirection1, indirection2, isindexed2, T>(im, varparameters...)};
			if constexpr(isabsvalue || isfltpmode){
				filterinput<isabsvalue, issignmode, isfltpmode, T>(cur);
			}
			++offsets[cur];
		}

		// barrier and pointer exchange with the companion thread
#if defined(__has_cpp_attribute) && __has_cpp_attribute(maybe_unused)
		[[maybe_unused]]
#endif
		std::conditional_t<ismultithreadcapable, size_t *, std::nullptr_t> offsetscompanion;
		if constexpr(ismultithreadcapable){
			uintptr_t other{atomiclightbarrier.exchange(reinterpret_cast<uintptr_t>(offsets) & -static_cast<intptr_t>(usemultithread))};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed);
				}while(reinterpret_cast<uintptr_t>(offsets) == other);
				// reset the barrier after use, only one thread will do this
				// no busy-wait dependency on this store, hence relaxed memory order is fine
				reinterpret_cast<uintptr_t &>(atomiclightbarrier) = 0;//atomiclightbarrier.store(0, std::memory_order_relaxed); TODO: fix this, as the original failed to inline anything in an early testing round
				// the next write to atomiclightbarrier will simply lock on and synchronise based on 0
			}
			// this will just be zero if usemultithread is zero
			offsetscompanion = reinterpret_cast<size_t *>(other);// retrieve the pointer
		}

		// transform counts into base offsets for each set of 256 items, both for the low and high half of offsets here
		unsigned allareidentical{generateoffsetssingle<isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, ismultithreadcapable>(count, offsets, offsetscompanion, usemultithread)};

		// barrier and allareidentical value exchange with the companion thread
		if constexpr(ismultithreadcapable){
			allareidentical += usemultithread;// send over a 1 or a 2 when multithreading
			while(reinterpret_cast<uintptr_t>(offsets) == atomiclightbarrier.load(std::memory_order_relaxed)){
				spinpause();// catch up
			}
			uintptr_t other{atomiclightbarrier.fetch_add(allareidentical)};
			// simply do not spin if usemultithread is zero
			if(usemultithread > other){
				do{
					spinpause();
					other = atomiclightbarrier.load(std::memory_order_relaxed) - allareidentical;
				}while(!other);
			}
			// only one of the two threads can set the all are identical state
			allareidentical -= static_cast<unsigned>(other);// codes:
			// 0: continue processing
			// -1: input from the companion thread
			// 1: input from this thread
		}

		if(!allareidentical)
#if defined(__has_cpp_attribute) && __has_cpp_attribute(likely)
			[[likely]]
#endif
		{// perform the bidirectional 8-bit sorting sequence
			radixsortnoallocsinglemain<indirection1, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, ismultithreadcapable, V>(count, buffer, input, offsets, usemultithread, varparameters...);
		}
	}
}

}// namespace helper

// Definition of the GetOffsetOf template
// Altered, to gain C++17 compatibility and make the simpler getoffsetof template with only one input.
// Temporary, until a revision of "offsetof" is standardized in C++ with constexpr.
// This part isn't used internally, but serves as a tool to the user for calculating compile-time offsets.
// Section start of all rights reserved for the respective author (Sulley, 2024-06-15):
// https://sulley.cc/2024/06/15/16/18/

#pragma pack(push, 1)
template<typename M, std::size_t Offset>
struct MemberAt
{
	char padding[Offset];
	M member;
};
#pragma pack(pop)

template<typename M>
struct MemberAt<M, 0>
{
	M member;
};

template<typename B, typename M, std::size_t Offset>
union PaddedUnion
{
	char c;
	B base;
	MemberAt<M, Offset> member;
};

// ~~~~~ Begin core modification ~~~~~
template <
	auto MemberPtr,
	typename B,
	std::size_t Low,
	std::size_t High,
	std::size_t Mid = (Low + High) / 2>
struct OffsetHelper
{
	using M = std::remove_reference_t<decltype(std::declval<B>().*MemberPtr)>;

	constexpr static PaddedUnion<B, M, Mid> dummy{};
	constexpr static std::size_t GetOffsetOf()
	{
		if constexpr (&(dummy.base.*MemberPtr) > &dummy.member.member)
		{
			return OffsetHelper<MemberPtr, B, Mid + 1, High>::GetOffsetOf();
		}
		else if constexpr (&(dummy.base.*MemberPtr) < &dummy.member.member)
		{
			return OffsetHelper<MemberPtr, B, Low, Mid>::GetOffsetOf();
		}
		else
		{
			return Mid;
		}
	}
};
// ~~~~~ End core modification ~~~~~

template <auto MemberPtr, typename B>
constexpr std::size_t GetOffsetOf = OffsetHelper<MemberPtr, B, 0, sizeof(B)>::GetOffsetOf();
// Section end

template<auto memberptr>
struct memberptrsplitter{
	template<typename C, typename M>
	static constexpr C *classgrabber(M C:: *in)noexcept{static_cast<void>(in); return{};}
	static constexpr auto classptr{classgrabber(memberptr)};
	using classtype = std::remove_pointer_t<decltype(classptr)>;
};

template<auto memberptr>
constexpr size_t getoffsetof{OffsetHelper<memberptr, typename memberptrsplitter<memberptr>::classtype, 0, sizeof(typename memberptrsplitter<memberptr>::classtype)>::GetOffsetOf()};

// Generic large array allocation and deallocation functions

template<typename T>
#if defined(__has_cpp_attribute) && __has_cpp_attribute(nodiscard)
[[nodiscard]]
#endif
#if !defined(_WIN32) && defined(_POSIX_C_SOURCE)// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
RSBD8_FUNC_INLINE std::pair<T *, size_t>
#else
RSBD8_FUNC_INLINE T *
#endif
	allocatearray(size_t count
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
	, size_t largepagesize = 0
#elif defined(_POSIX_C_SOURCE)
	, int mmapflags = MAP_ANONYMOUS | MAP_PRIVATE
#endif
	)noexcept{
	size_t allocsize{count * sizeof(T)};
#ifdef _WIN32
	assert(!(largepagesize - 1 & largepagesize));// a maximum of one bit should be set in the value of largepagesize
	size_t largeallocsize{(largepagesize - 1 & -static_cast<ptrdiff_t>(allocsize)) + allocsize};// round up to the nearest multiple of largepagesize
	DWORD alloctype{MEM_RESERVE | MEM_COMMIT};
	DWORD largealloctype{MEM_LARGE_PAGES | MEM_RESERVE | MEM_COMMIT};
	if(largepagesize) allocsize = largeallocsize, alloctype = largealloctype;
	T *buffer{reinterpret_cast<T *>(VirtualAlloc(nullptr, allocsize, alloctype, PAGE_READWRITE))};
	return{buffer};
#elif defined(_POSIX_C_SOURCE)
	void *pempty{};
#ifdef MAP_HUGETLB
	if(MAP_HUGETLB & mmapflags){// use the 6 bits associated with the huge TLB functionality
		size_t pagesize{static_cast<size_t>(1) << (static_cast<unsigned>(mmapflags) >> MAP_HUGE_SHIFT & ((1 << 6) - 1))};
		allocsize = (pagesize - 1 & -static_cast<ptrdiff_t>(allocsize)) + allocsize};// round up to the nearest multiple of pagesize
#ifdef __ia64__// Only IA64 requires this part for this type of allocation
		pempty = reinterpret_cast<void *>(0x8000000000000000UL);
#endif
	}
#endif
	T *buffer{reinterpret_cast<T *>(mmap(pempty, allocsize, PROT_READ | PROT_WRITE, mmapflags, -1, 0))};
	return{buffer, allocsize};
#else
	T *buffer{new(std::nothrow) T[allocsize]};
	return{buffer};
#endif
}

template<typename T>
RSBD8_FUNC_INLINE void deallocatearray(T *buffer
#if !defined(_WIN32) && defined(_POSIX_C_SOURCE)// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
	, size_t allocsize
#endif
	)noexcept{
#ifdef _WIN32
	BOOL boVirtualFree{VirtualFree(buffer, 0, MEM_RELEASE)};
	static_cast<void>(boVirtualFree);
	assert(boVirtualFree);
#elif defined(_POSIX_C_SOURCE)
	int imunmap{munmap(buffer, allocsize);
	static_cast<void>(imunmap);
	assert(!imunmap);
#else
	delete[] buffer;
#endif
}

// This class is a simple RAII wrapper for the buffer memory allocated with allocatearray().
template<typename T>
struct buffermemorywrapper{
	T *ptr;
#if defined(_POSIX_C_SOURCE)
	size_t size;
#endif
	RSBD8_FUNC_INLINE ~buffermemorywrapper()noexcept{
		deallocatearray(ptr
#if defined(_POSIX_C_SOURCE)
			, size
#endif
		);}
	// disable copy and move mechanisms
	buffermemorywrapper(buffermemorywrapper const &) = delete;
	buffermemorywrapper &operator=(buffermemorywrapper const &) = delete;
	RSBD8_FUNC_INLINE buffermemorywrapper(T *ptrmem
#if defined(_POSIX_C_SOURCE)
		, sizemem
#endif
		)noexcept : ptr(ptrmem)
#if defined(_POSIX_C_SOURCE)
		, size(sizemem)
#endif
	{}
};

// Wrapper template functions for the main sorting functions in this library

// Wrapper for the multi-part radixsortcopynoalloc() function without indirection
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	128 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T),
	void> radixsortcopynoalloc(size_t count, T const input[], T output[], T buffer[])noexcept{
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && (std::is_floating_point_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = helper::tounifunsigned<T>;
	helper::radixsortcopynoallocmulti<isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, U>(count, reinterpret_cast<U const *>(input), reinterpret_cast<U *>(output), reinterpret_cast<U *>(buffer));
}

// Wrapper for the multi-part radixsortnoalloc() function without indirection
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	128 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T),
	void> radixsortnoalloc(size_t count, T input[], T buffer[], bool movetobuffer = false)noexcept{
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && (std::is_floating_point_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = helper::tounifunsigned<T>;
	helper::radixsortnoallocmulti<isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, U>(count, reinterpret_cast<U *>(input), reinterpret_cast<U *>(buffer), movetobuffer);
}

// Wrapper for the single-part radixsortcopynoalloc() function without indirection
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	8 >= CHAR_BIT * sizeof(T),
	void> radixsortcopynoalloc(size_t count, T const input[], T output[])noexcept{
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && (std::is_floating_point_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = helper::tounifunsigned<T>;
	helper::radixsortcopynoallocsingle<isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, U>(count, reinterpret_cast<U const *>(input), reinterpret_cast<U *>(output));
}

// Wrapper for the single-part radixsortcopynoalloc() function without indirection with a dummy buffer argument
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	8 >= CHAR_BIT * sizeof(T),
	void> radixsortcopynoalloc(size_t count, T const input[], T output[], T buffer[])noexcept{
	static_cast<void>(buffer);// the single-part version never needs an extra buffer
	radixsortcopynoalloc<direction, mode, T>(count, input, output);
}

// Wrapper for the single-part radixsortnoalloc() function without indirection
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	8 >= CHAR_BIT * sizeof(T),
	void> radixsortnoalloc(size_t count, T input[], T buffer[])noexcept{
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && (std::is_floating_point_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = helper::tounifunsigned<T>;
	helper::radixsortnoallocsingle<isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, U>(count, reinterpret_cast<U *>(input), reinterpret_cast<U *>(buffer));
}

// Wrapper for the single-part radixsortnoalloc() and radixsortcopynoalloc() functions without indirection
// This variant does not set the default "false" for the "movetobuffer" parameter.
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, typename T>
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	8 >= CHAR_BIT * sizeof(T),
	void> radixsortnoalloc(size_t count, T input[], T buffer[], bool movetobuffer)noexcept{
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && (std::is_floating_point_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = helper::tounifunsigned<T>;
	if(!movetobuffer) helper::radixsortnoallocsingle<isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, U>(count, reinterpret_cast<U *>(input), reinterpret_cast<U *>(buffer));
	else helper::radixsortcopynoallocsingle<isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, U>(count, reinterpret_cast<U *>(input), reinterpret_cast<U *>(buffer));
}

// Wrapper to implement the radixsort() function without indirection, which only allocates some memory prior to sorting arrays
// This requires no specialisation for handling the single-part types.
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, typename T>
#if defined(__has_cpp_attribute) && __has_cpp_attribute(nodiscard)
[[nodiscard]]
#endif
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	128 >= CHAR_BIT * sizeof(T),
	bool> radixsort(size_t count, T input[]
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, size_t largepagesize = 0
#elif defined(_POSIX_C_SOURCE)
		, int mmapflags = MAP_ANONYMOUS | MAP_PRIVATE
#endif
		)noexcept{
	auto
#if defined(_POSIX_C_SOURCE)
		[buffer, allocsize]
#else
		buffer
#endif
		{allocatearray<T>(count
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, largepagesize
#elif defined(_POSIX_C_SOURCE)
		, mmapflags
#endif
		)};
	if(buffer){
		radixsortnoalloc<direction, mode, T>(count, input, buffer);// last parameter not filled in on purpose
		deallocatearray(buffer
#if defined(_POSIX_C_SOURCE)
			, allocsize
#endif
			);
		return{true};
	}
	return{false};
}

// Wrapper to implement the multi-part radixsortcopy() function without indirection, which only allocates some memory prior to sorting arrays
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, typename T>
#if defined(__has_cpp_attribute) && __has_cpp_attribute(nodiscard)
[[nodiscard]]
#endif
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	128 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T),
	bool> radixsortcopy(size_t count, T const input[], T output[]
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, size_t largepagesize = 0
#elif defined(_POSIX_C_SOURCE)
		, int mmapflags = MAP_ANONYMOUS | MAP_PRIVATE
#endif
		)noexcept{
	auto
#if defined(_POSIX_C_SOURCE)
		[buffer, allocsize]
#else
		buffer
#endif
		{allocatearray<T>(count
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, largepagesize
#elif defined(_POSIX_C_SOURCE)
		, mmapflags
#endif
		)};
	if(buffer){
		radixsortcopynoalloc<direction, mode, T>(count, input, output, buffer);
		deallocatearray(buffer
#if defined(_POSIX_C_SOURCE)
			, allocsize
#endif
			);
		return{true};
	}
	return{false};
}

// Wrapper to implement the single-part radixsortcopy() function without indirection, which only allocates some memory prior to sorting arrays
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, typename T>
#if defined(__has_cpp_attribute) && __has_cpp_attribute(nodiscard)
[[nodiscard]]
#endif
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	8 >= CHAR_BIT * sizeof(T),
	bool> radixsortcopy(size_t count, T const input[], T output[]
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, size_t largepagesize = 0
#elif defined(_POSIX_C_SOURCE)
		, int mmapflags = MAP_ANONYMOUS | MAP_PRIVATE
#endif
		)noexcept{
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
	assert(!(largepagesize - 1 & largepagesize));// a maximum of one bit should be set in the value of largepagesize
	static_cast<void>(largepagesize);
#elif defined(_POSIX_C_SOURCE)
	static_cast<void>(mmapflags);
#endif
	// the single-part version never needs an extra buffer
	radixsortcopynoalloc<direction, mode, T>(count, input, output);
	return{true};
}

// Wrapper for the multi-part radixsortcopynoalloc() function with simple second-level indirection
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename T, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	128 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T),
	void> radixsortcopynoalloc(size_t count, T *const input[], T *output[], T *buffer[], vararguments... varparameters)noexcept{
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && (std::is_floating_point_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = helper::tounifunsigned<T>;
	using V = std::conditional_t<std::is_const_v<T> && std::is_volatile_v<T>, const volatile helper::memberobjectgenerator<U, 0>,
		std::conditional_t<std::is_const_v<T>, const helper::memberobjectgenerator<U, 0>,
		std::conditional_t<std::is_volatile_v<T>, volatile helper::memberobjectgenerator<U, 0>,
		helper::memberobjectgenerator<U, 0>>>>;
	helper::radixsortcopynoallocmulti<&V::object, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2>(count, reinterpret_cast<V *const *>(input), reinterpret_cast<V **>(output), reinterpret_cast<V **>(buffer), varparameters...);
}

// Wrapper for the multi-part radixsortnoalloc() function with simple second-level indirection
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename T, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	128 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T),
	void> radixsortnoalloc(size_t count, T *input[], T *buffer[], bool movetobuffer = false, vararguments... varparameters)noexcept{
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && (std::is_floating_point_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = helper::tounifunsigned<T>;
	using V = std::conditional_t<std::is_const_v<T> && std::is_volatile_v<T>, const volatile helper::memberobjectgenerator<U, 0>,
		std::conditional_t<std::is_const_v<T>, const helper::memberobjectgenerator<U, 0>,
		std::conditional_t<std::is_volatile_v<T>, volatile helper::memberobjectgenerator<U, 0>,
		helper::memberobjectgenerator<U, 0>>>>;
	helper::radixsortnoallocmulti<&V::object, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2>(count, reinterpret_cast<V **>(input), reinterpret_cast<V **>(buffer), movetobuffer, varparameters...);
}

// Wrapper for the single-part radixsortcopynoalloc() function with simple second-level indirection
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename T, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	8 >= CHAR_BIT * sizeof(T),
	void> radixsortcopynoalloc(size_t count, T *const input[], T *output[], vararguments... varparameters)noexcept{
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && (std::is_floating_point_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = helper::tounifunsigned<T>;
	using V = std::conditional_t<std::is_const_v<T> && std::is_volatile_v<T>, const volatile helper::memberobjectgenerator<U, 0>,
		std::conditional_t<std::is_const_v<T>, const helper::memberobjectgenerator<U, 0>,
		std::conditional_t<std::is_volatile_v<T>, volatile helper::memberobjectgenerator<U, 0>,
		helper::memberobjectgenerator<U, 0>>>>;
	helper::radixsortcopynoallocsingle<&V::object, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2>(count, reinterpret_cast<V *const *>(input), reinterpret_cast<V **>(output), varparameters...);
}

// Wrapper for the single-part radixsortcopynoalloc() function with simple second-level indirection with a dummy buffer argument
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename T, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	8 >= CHAR_BIT * sizeof(T),
	void> radixsortcopynoalloc(size_t count, T *const input[], T *output[], T *buffer[], vararguments... varparameters)noexcept{
	static_cast<void>(buffer);// the single-part version never needs an extra buffer
	radixsortcopynoalloc<direction, mode, indirection2, isindexed2, T>(count, input, output, varparameters...);
}

// Wrapper for the single-part radixsortnoalloc() function with simple second-level indirection
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename T, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	8 >= CHAR_BIT * sizeof(T),
	void> radixsortnoalloc(size_t count, T *input[], T *buffer[], vararguments... varparameters)noexcept{
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && (std::is_floating_point_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = helper::tounifunsigned<T>;
	using V = std::conditional_t<std::is_const_v<T> && std::is_volatile_v<T>, const volatile helper::memberobjectgenerator<U, 0>,
		std::conditional_t<std::is_const_v<T>, const helper::memberobjectgenerator<U, 0>,
		std::conditional_t<std::is_volatile_v<T>, volatile helper::memberobjectgenerator<U, 0>,
		helper::memberobjectgenerator<U, 0>>>>;
	helper::radixsortnoallocsingle<&V::object, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2>(count, reinterpret_cast<V **>(input), reinterpret_cast<V **>(buffer), varparameters...);
}

// Wrapper for the single-part radixsortnoalloc() and radixsortcopynoalloc() functions with simple second-level indirection
// This variant does not set the default "false" for the "movetobuffer" parameter.
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename T, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	8 >= CHAR_BIT * sizeof(T),
	void> radixsortnoalloc(size_t count, T *input[], T *buffer[], bool movetobuffer, vararguments... varparameters)noexcept{
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && (std::is_floating_point_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = helper::tounifunsigned<T>;
	using V = std::conditional_t<std::is_const_v<T> && std::is_volatile_v<T>, const volatile helper::memberobjectgenerator<U, 0>,
		std::conditional_t<std::is_const_v<T>, const helper::memberobjectgenerator<U, 0>,
		std::conditional_t<std::is_volatile_v<T>, volatile helper::memberobjectgenerator<U, 0>,
		helper::memberobjectgenerator<U, 0>>>>;
	if(!movetobuffer) helper::radixsortnoallocsingle<&V::object, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode>(count, reinterpret_cast<V **>(input), reinterpret_cast<V **>(buffer), varparameters...);
	else helper::radixsortcopynoallocsingle<&V::object, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2>(count, reinterpret_cast<V **>(input), reinterpret_cast<V **>(buffer), varparameters...);
}

// Wrapper to implement the radixsort() function with simple second-level indirection, which only allocates some memory prior to sorting arrays
// This requires no specialisation for handling the single-part types.
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename T, typename... vararguments>
#if defined(__has_cpp_attribute) && __has_cpp_attribute(nodiscard)
[[nodiscard]]
#endif
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	128 >= CHAR_BIT * sizeof(T),
	bool> radixsort(size_t count, T *input[]
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, size_t largepagesize = 0
#elif defined(_POSIX_C_SOURCE)
		, int mmapflags = MAP_ANONYMOUS | MAP_PRIVATE
#endif
		, vararguments... varparameters)noexcept{
	auto
#if defined(_POSIX_C_SOURCE)
		[buffer, allocsize]
#else
		buffer
#endif
		{allocatearray<T *>(count
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, largepagesize
#elif defined(_POSIX_C_SOURCE)
		, mmapflags
#endif
		)};
	if(buffer){
		radixsortnoalloc<direction, mode, indirection2, isindexed2, T>(count, input, buffer, false, varparameters...);
		deallocatearray(buffer
#if defined(_POSIX_C_SOURCE)
			, allocsize
#endif
			);
		return{true};
	}
	return{false};
}

// Wrapper to implement the multi-part radixsortcopy() function with simple second-level indirection, which only allocates some memory prior to sorting arrays
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename T, typename... vararguments>
#if defined(__has_cpp_attribute) && __has_cpp_attribute(nodiscard)
[[nodiscard]]
#endif
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	128 >= CHAR_BIT * sizeof(T) &&
	8 < CHAR_BIT * sizeof(T),
	bool> radixsortcopy(size_t count, T *const input[], T *output[]
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, size_t largepagesize = 0
#elif defined(_POSIX_C_SOURCE)
		, int mmapflags = MAP_ANONYMOUS | MAP_PRIVATE
#endif
		, vararguments... varparameters)noexcept{
	auto
#if defined(_POSIX_C_SOURCE)
		[buffer, allocsize]
#else
		buffer
#endif
		{allocatearray<T *>(count
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, largepagesize
#elif defined(_POSIX_C_SOURCE)
		, mmapflags
#endif
		)};
	if(buffer){
		radixsortcopynoalloc<direction, mode, indirection2, isindexed2, T>(count, input, output, buffer, varparameters...);
		deallocatearray(buffer
#if defined(_POSIX_C_SOURCE)
			, allocsize
#endif
			);
		return{true};
	}
	return{false};
}

// Wrapper to implement the single-part radixsortcopy() function with simple second-level indirection, which only allocates some memory prior to sorting arrays
template<sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename T, typename... vararguments>
#if defined(__has_cpp_attribute) && __has_cpp_attribute(nodiscard)
[[nodiscard]]
#endif
RSBD8_FUNC_INLINE std::enable_if_t<
	!std::is_pointer_v<T> &&
	8 >= CHAR_BIT * sizeof(T),
	bool> radixsortcopy(size_t count, T *const input[], T *output[]
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, size_t largepagesize = 0
#elif defined(_POSIX_C_SOURCE)
		, int mmapflags = MAP_ANONYMOUS | MAP_PRIVATE
#endif
		, vararguments... varparameters)noexcept{
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
	assert(!(largepagesize - 1 & largepagesize));// a maximum of one bit should be set in the value of largepagesize
	static_cast<void>(largepagesize);
#elif defined(_POSIX_C_SOURCE)
	static_cast<void>(mmapflags);
#endif
	// the single-part version never needs an extra buffer
	radixsortcopynoalloc<direction, mode, indirection2, isindexed2, T>(count, input, output, varparameters...);
	return{true};
}

// Wrapper for the multi-part radixsortcopynoalloc() function with indirection
template<auto indirection1, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	128 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<helper::memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>) &&
	8 < CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<helper::memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortcopynoalloc(size_t count, V *const input[], V *output[], V *buffer[], vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(helper::splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	using W = std::decay_t<helper::memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>;
	using T = helper::tounifunsigned<std::remove_pointer_t<W>>;
	static_assert(!std::is_pointer_v<T>, "third level indirection is not supported");
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && (std::is_floating_point_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	helper::radixsortcopynoallocmulti<indirection1, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V>(count, input, output, buffer, varparameters...);
}

// Wrapper for the multi-part radixsortnoalloc() function with indirection
template<auto indirection1, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	128 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<helper::memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>) &&
	8 < CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<helper::memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortnoalloc(size_t count, V *input[], V *buffer[], bool movetobuffer = false, vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(helper::splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	using W = std::decay_t<helper::memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>;
	using T = helper::tounifunsigned<std::remove_pointer_t<W>>;
	static_assert(!std::is_pointer_v<T>, "third level indirection is not supported");
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && (std::is_floating_point_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	helper::radixsortnoallocmulti<indirection1, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V>(count, input, buffer, movetobuffer, varparameters...);
}

// Wrapper for the single-part radixsortcopynoalloc() function with indirection
template<auto indirection1, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<// disable the option for with the V *buffer[] argument here, and do not allow active compile-time template evaluation with it
	!std::is_same_v<V **, std::conditional_t<0 < sizeof...(vararguments),
		std::invoke_result_t<decltype(helper::splitparameter<vararguments...>), vararguments...>, void>> &&
	std::is_member_pointer_v<decltype(indirection1)> &&
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<
		typename std::enable_if<!std::is_same_v<V **, std::conditional_t<0 < sizeof...(vararguments),
			std::invoke_result_t<decltype(helper::splitparameter<vararguments...>), vararguments...>, void>>,
			helper::memberpointerdeducebody<indirection1, isindexed2, V, vararguments...>>::type>>),
	void> radixsortcopynoalloc(size_t count, V *const input[], V *output[], vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(helper::splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	using W = std::decay_t<helper::memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>;
	using T = helper::tounifunsigned<std::remove_pointer_t<W>>;
	static_assert(!std::is_pointer_v<T>, "third level indirection is not supported");
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && (std::is_floating_point_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	helper::radixsortcopynoallocsingle<indirection1, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V>(count, input, output, varparameters...);
}

// Wrapper for the single-part radixsortcopynoalloc() function with indirection with a dummy buffer argument
template<auto indirection1, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<helper::memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortcopynoalloc(size_t count, V *const input[], V *output[], V *buffer[], vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(helper::splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	static_cast<void>(buffer);// the single-part version never needs an extra buffer
	helper::radixsortcopynoallocsingle<indirection1, direction, mode, indirection2, isindexed2, V>(count, input, output, varparameters...);
}

// Wrapper for the single-part radixsortnoalloc() function with indirection
template<auto indirection1, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<// disable the option for with the bool movetobuffer argument here, and do not allow active compile-time template evaluation with it
	!std::is_same_v<bool, std::conditional_t<0 < sizeof...(vararguments),
		std::invoke_result_t<decltype(helper::splitparameter<vararguments...>), vararguments...>, void>> &&
	std::is_member_pointer_v<decltype(indirection1)> &&
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<
		typename std::enable_if<!std::is_same_v<bool, std::conditional_t<0 < sizeof...(vararguments),
			std::invoke_result_t<decltype(helper::splitparameter<vararguments...>), vararguments...>, void>>,
			helper::memberpointerdeducebody<indirection1, isindexed2, V, vararguments...>>::type>>),
	void> radixsortnoalloc(size_t count, V *input[], V *buffer[], vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(helper::splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	using W = std::decay_t<helper::memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>;
	using T = helper::tounifunsigned<std::remove_pointer_t<W>>;
	static_assert(!std::is_pointer_v<T>, "third level indirection is not supported");
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && (std::is_floating_point_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	helper::radixsortnoallocsingle<indirection1, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V>(count, input, buffer, varparameters...);
}

// Wrapper for the single-part radixsortnoalloc() and radixsortcopynoalloc() functions with indirection
// This variant does not set the default "false" for the "movetobuffer" parameter.
template<auto indirection1, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<helper::memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	void> radixsortnoalloc(size_t count, V *input[], V *buffer[], bool movetobuffer, vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(helper::splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	using W = std::decay_t<helper::memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>;
	using T = helper::tounifunsigned<std::remove_pointer_t<W>>;
	static_assert(!std::is_pointer_v<T>, "third level indirection is not supported");
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && (std::is_signed_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && (std::is_floating_point_v<T> || std::is_same_v<helper::longdoubletest128, T> || std::is_same_v<helper::longdoubletest96, T> || std::is_same_v<helper::longdoubletest80, T>)) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = helper::tounifunsigned<T>;
	if(!movetobuffer){
		helper::radixsortnoallocsingle<indirection1, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V>(count, input, buffer, varparameters...);
	}else{
		helper::radixsortcopynoallocsingle<indirection1, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, V>(count, input, buffer, varparameters...);
	}
}

// Wrapper to implement the radixsort() function with indirection, which only allocates some memory prior to sorting arrays
// This requires no specialisation for handling the single-part types.
template<auto indirection1, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
#if defined(__has_cpp_attribute) && __has_cpp_attribute(nodiscard)
[[nodiscard]]
#endif
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	128 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<helper::memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	bool> radixsort(size_t count, V *input[]
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, size_t largepagesize = 0
#elif defined(_POSIX_C_SOURCE)
		, int mmapflags = MAP_ANONYMOUS | MAP_PRIVATE
#endif
		, vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(helper::splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	auto
#if defined(_POSIX_C_SOURCE)
		[buffer, allocsize]
#else
		buffer
#endif
		{allocatearray<V *>(count
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, largepagesize
#elif defined(_POSIX_C_SOURCE)
		, mmapflags
#endif
		)};
	if(buffer){
		buffermemorywrapper<V *> guard{buffer
#if defined(_POSIX_C_SOURCE)
			, allocsize
#endif
		};// ensure the buffer is deallocated, even if an exception is thrown by the getter function here
		radixsortnoalloc<indirection1, direction, mode, indirection2, isindexed2, V>(count, input, buffer, false, varparameters...);
		return{true};
	}
	return{false};
}

// Wrapper to implement the multi-part radixsortcopy() function with indirection, which only allocates some memory prior to sorting arrays
template<auto indirection1, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
#if defined(__has_cpp_attribute) && __has_cpp_attribute(nodiscard)
[[nodiscard]]
#endif
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	128 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<helper::memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>) &&
	8 < CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<helper::memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	bool> radixsortcopy(size_t count, V *const input[], V output[]
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, size_t largepagesize = 0
#elif defined(_POSIX_C_SOURCE)
		, int mmapflags = MAP_ANONYMOUS | MAP_PRIVATE
#endif
		, vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(helper::splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
	auto
#if defined(_POSIX_C_SOURCE)
		[buffer, allocsize]
#else
		buffer
#endif
		{allocatearray<V *>(count
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, largepagesize
#elif defined(_POSIX_C_SOURCE)
		, mmapflags
#endif
		)};
	if(buffer){
		buffermemorywrapper<V *> guard{buffer
#if defined(_POSIX_C_SOURCE)
			, allocsize
#endif
		};// ensure the buffer is deallocated, even if an exception is thrown by the getter function here
		radixsortcopynoalloc<indirection1, direction, mode, indirection2, isindexed2, V>(count, input, output, buffer, varparameters...);
		return{true};
	}
	return{false};
}

// Wrapper to implement the single-part radixsortcopy() function with indirection, which only allocates some memory prior to sorting arrays
template<auto indirection1, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
#if defined(__has_cpp_attribute) && __has_cpp_attribute(nodiscard)
[[nodiscard]]
#endif
RSBD8_FUNC_INLINE std::enable_if_t<
	std::is_member_pointer_v<decltype(indirection1)> &&
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<helper::memberpointerdeduce<indirection1, isindexed2, V, vararguments...>>>),
	bool> radixsortcopy(size_t count, V *const input[], V *output[]
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, size_t largepagesize = 0
#elif defined(_POSIX_C_SOURCE)
		, int mmapflags = MAP_ANONYMOUS | MAP_PRIVATE
#endif
		, vararguments... varparameters)noexcept(std::is_nothrow_invocable_v<decltype(helper::splitget<indirection1, isindexed2, V, vararguments...>), V *, vararguments...>){
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
	assert(!(largepagesize - 1 & largepagesize));// a maximum of one bit should be set in the value of largepagesize
	static_cast<void>(largepagesize);
#elif defined(_POSIX_C_SOURCE)
	static_cast<void>(mmapflags);
#endif
	// the single-part version never needs an extra buffer
	radixsortcopynoalloc<indirection1, direction, mode, indirection2, isindexed2, V>(count, input, output, varparameters...);
	return{true};
}

// Wrapper for the multi-part radixsortcopynoalloc() function with type and offset pointer indirection
template<typename T, ptrdiff_t indirection1 = 0, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	128 >= CHAR_BIT * sizeof(std::remove_pointer_t<T>) &&
	8 < CHAR_BIT * sizeof(std::remove_pointer_t<T>),
	void> radixsortcopynoalloc(size_t count, V *const input[], V *output[], V *buffer[], vararguments... varparameters)noexcept{
	using W = helper::tounifunsigned<std::remove_pointer_t<T>>;
	static_assert(!std::is_pointer_v<W>, "third level indirection is not supported");
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && std::is_signed_v<W>) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && std::is_signed_v<W>) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && std::is_floating_point_v<W>) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = std::conditional_t<std::is_const_v<V> && std::is_volatile_v<V>, const volatile helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>,
		std::conditional_t<std::is_const_v<V>, const helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>,
		std::conditional_t<std::is_volatile_v<V>, volatile helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>,
		helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>>>>;
	helper::radixsortcopynoallocmulti<&U::object, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, U>(count, reinterpret_cast<U *const *>(input), reinterpret_cast<U **>(output), reinterpret_cast<U **>(buffer), varparameters...);
}

// Wrapper for the multi-part radixsortnoalloc() function with type and offset pointer indirection
template<typename T, ptrdiff_t indirection1 = 0, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	128 >= CHAR_BIT * sizeof(std::remove_pointer_t<T>) &&
	8 < CHAR_BIT * sizeof(std::remove_pointer_t<T>),
	void> radixsortnoalloc(size_t count, V *input[], V *buffer[], bool movetobuffer = false, vararguments... varparameters)noexcept{
	using W = helper::tounifunsigned<std::remove_pointer_t<T>>;
	static_assert(!std::is_pointer_v<W>, "third level indirection is not supported");
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && std::is_signed_v<W>) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && std::is_signed_v<W>) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && std::is_floating_point_v<W>) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = std::conditional_t<std::is_const_v<V> && std::is_volatile_v<V>, const volatile helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>,
		std::conditional_t<std::is_const_v<V>, const helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>,
		std::conditional_t<std::is_volatile_v<V>, volatile helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>,
		helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>>>>;
	helper::radixsortnoallocmulti<&U::object, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, U>(count, reinterpret_cast<U **>(input), reinterpret_cast<U **>(buffer), movetobuffer, varparameters...);
}

// Wrapper for the single-part radixsortcopynoalloc() function with type and offset pointer indirection
template<typename T, ptrdiff_t indirection1 = 0, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<// disable the option for with the V *buffer[] argument here, and do not allow active compile-time template evaluation with it
	!std::is_same_v<V **, std::conditional_t<0 < sizeof...(vararguments),
		std::invoke_result_t<decltype(helper::splitparameter<vararguments...>), vararguments...>, void>> &&
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<
		typename std::enable_if<!std::is_same_v<V **, std::conditional_t<0 < sizeof...(vararguments),
			std::invoke_result_t<decltype(helper::splitparameter<vararguments...>), vararguments...>, void>>,
			helper::memberpointerdeducebody<indirection1, isindexed2, V, vararguments...>>::type>>),
	void> radixsortcopynoalloc(size_t count, V *const input[], V *output[], vararguments... varparameters)noexcept{
	using W = helper::tounifunsigned<std::remove_pointer_t<T>>;
	static_assert(!std::is_pointer_v<W>, "third level indirection is not supported");
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && std::is_signed_v<W>) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && std::is_signed_v<W>) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && std::is_floating_point_v<W>) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = std::conditional_t<std::is_const_v<V> && std::is_volatile_v<V>, const volatile helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>,
		std::conditional_t<std::is_const_v<V>, const helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>,
		std::conditional_t<std::is_volatile_v<V>, volatile helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>,
		helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>>>>;
	helper::radixsortcopynoallocsingle<&U::object, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, U>(count, reinterpret_cast<U *const *>(input), reinterpret_cast<U **>(output), varparameters...);
}

// Wrapper for the single-part radixsortcopynoalloc() function with type and offset pointer indirection with a dummy buffer argument
template<typename T, ptrdiff_t indirection1 = 0, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<T>),
	void> radixsortcopynoalloc(size_t count, V *const input[], V *output[], V *buffer[], vararguments... varparameters)noexcept{
	static_cast<void>(buffer);// the single-part version never needs an extra buffer
	helper::radixsortcopynoallocsingle<T, indirection1, direction, mode, indirection2, isindexed2, V>(count, input, output, varparameters...);
}

// Wrapper for the single-part radixsortnoalloc() function with type and offset pointer indirection
template<typename T, ptrdiff_t indirection1 = 0, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<// disable the option for with the bool movetobuffer argument here, and do not allow active compile-time template evaluation with it
	!std::is_same_v<bool, std::conditional_t<0 < sizeof...(vararguments),
		std::invoke_result_t<decltype(helper::splitparameter<vararguments...>), vararguments...>, void>> &&
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<std::decay_t<
		typename std::enable_if<!std::is_same_v<bool, std::conditional_t<0 < sizeof...(vararguments),
			std::invoke_result_t<decltype(helper::splitparameter<vararguments...>), vararguments...>, void>>,
			helper::memberpointerdeducebody<indirection1, isindexed2, V, vararguments...>>::type>>),
	void> radixsortnoalloc(size_t count, V *input[], V *buffer[], vararguments... varparameters)noexcept{
	using W = helper::tounifunsigned<std::remove_pointer_t<T>>;
	static_assert(!std::is_pointer_v<W>, "third level indirection is not supported");
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && std::is_signed_v<W>) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && std::is_signed_v<W>) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && std::is_floating_point_v<W>) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = std::conditional_t<std::is_const_v<V> && std::is_volatile_v<V>, const volatile helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>,
		std::conditional_t<std::is_const_v<V>, const helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>,
		std::conditional_t<std::is_volatile_v<V>, volatile helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>,
		helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>>>>;
	helper::radixsortnoallocsingle<&U::object, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, U>(count, reinterpret_cast<U **>(input), reinterpret_cast<U **>(buffer), varparameters...);
}

// Wrapper for the single-part radixsortnoalloc() and radixsortcopynoalloc() functions with with type and offset pointer indirection
// This variant does not set the default "false" for the "movetobuffer" parameter.
template<typename T, ptrdiff_t indirection1 = 0, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
RSBD8_FUNC_INLINE std::enable_if_t<
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<T>),
	void> radixsortnoalloc(size_t count, V *input[], V *buffer[], bool movetobuffer, vararguments... varparameters)noexcept{
	using W = helper::tounifunsigned<std::remove_pointer_t<T>>;
	static_assert(!std::is_pointer_v<W>, "third level indirection is not supported");
	static bool constexpr isdescsort{static_cast<bool>(1 & static_cast<unsigned char>(direction))};
	static bool constexpr isrevorder{static_cast<bool>(1 << 1 & static_cast<unsigned char>(direction))};
	static bool constexpr isabsvalue{
		(sortingmode::nativeabs <= mode && std::is_signed_v<W>) ||
		(sortingmode::native > mode && static_cast<bool>(1 & static_cast<unsigned char>(mode)))};
	static bool constexpr issignmode{
		(sortingmode::native <= mode && sortingmode::nativeabs >= mode && std::is_signed_v<W>) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 1 & static_cast<unsigned char>(mode)))};
	static bool constexpr isfltpmode{
		(sortingmode::native <= mode && std::is_floating_point_v<W>) ||
		(sortingmode::native > mode && static_cast<bool>(1 << 2 & static_cast<unsigned char>(mode)))};
	using U = std::conditional_t<std::is_const_v<V> && std::is_volatile_v<V>, const volatile helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>,
		std::conditional_t<std::is_const_v<V>, const helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>,
		std::conditional_t<std::is_volatile_v<V>, volatile helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>,
		helper::memberobjectgenerator<std::conditional_t<std::is_pointer_v<T>, helper::tounifunsigned<W> const *, helper::tounifunsigned<W>>, indirection1>>>>;
	if(!movetobuffer){
		helper::radixsortnoallocsingle<&U::object, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, U>(count, reinterpret_cast<U **>(input), reinterpret_cast<U **>(buffer), varparameters...);
	}else{
		helper::radixsortcopynoallocsingle<&U::object, isdescsort, isrevorder, isabsvalue, issignmode, isfltpmode, indirection2, isindexed2, U>(count, reinterpret_cast<U **>(input), reinterpret_cast<U **>(buffer), varparameters...);
	}
}

// Wrapper to implement the radixsort() function with with type and offset pointer indirection, which only allocates some memory prior to sorting arrays with indirection
// This requires no specialisation for handling the single-part types.
template<typename T, ptrdiff_t indirection1 = 0, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
#if defined(__has_cpp_attribute) && __has_cpp_attribute(nodiscard)
[[nodiscard]]
#endif
RSBD8_FUNC_INLINE std::enable_if_t<
	128 >= CHAR_BIT * sizeof(std::remove_pointer_t<T>),
	bool> radixsort(size_t count, V *input[]
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, size_t largepagesize = 0
#elif defined(_POSIX_C_SOURCE)
		, int mmapflags = MAP_ANONYMOUS | MAP_PRIVATE
#endif
		, vararguments... varparameters)noexcept{
	auto
#if defined(_POSIX_C_SOURCE)
		[buffer, allocsize]
#else
		buffer
#endif
		{allocatearray<V *>(count
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, largepagesize
#elif defined(_POSIX_C_SOURCE)
		, mmapflags
#endif
		)};
	if(buffer){
		radixsortnoalloc<T, indirection1, direction, mode, indirection2, isindexed2, V>(count, input, buffer, false, varparameters...);
		deallocatearray(buffer
#if defined(_POSIX_C_SOURCE)
			, allocsize
#endif
			);
		return{true};
	}
	return{false};
}

// Wrapper to implement the multi-part radixsortcopy() function with with type and offset pointer indirection, which only allocates some memory prior to sorting arrays with indirection
template<typename T, ptrdiff_t indirection1 = 0, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
#if defined(__has_cpp_attribute) && __has_cpp_attribute(nodiscard)
[[nodiscard]]
#endif
RSBD8_FUNC_INLINE std::enable_if_t<
	128 >= CHAR_BIT * sizeof(std::remove_pointer_t<T>) &&
	8 < CHAR_BIT * sizeof(std::remove_pointer_t<T>),
	bool> radixsortcopy(size_t count, V *const input[], V output[]
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, size_t largepagesize = 0
#elif defined(_POSIX_C_SOURCE)
		, int mmapflags = MAP_ANONYMOUS | MAP_PRIVATE
#endif
		, vararguments... varparameters)noexcept{
	auto
#if defined(_POSIX_C_SOURCE)
		[buffer, allocsize]
#else
		buffer
#endif
		{allocatearray<V *>(count
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, largepagesize
#elif defined(_POSIX_C_SOURCE)
		, mmapflags
#endif
		)};
	if(buffer){
		radixsortcopynoalloc<T, indirection1, direction, mode, indirection2, isindexed2, V>(count, input, output, buffer, varparameters...);
		deallocatearray(buffer
#if defined(_POSIX_C_SOURCE)
			, allocsize
#endif
			);
		return{true};
	}
	return{false};
}

// Wrapper to implement the single-part radixsortcopy() function with with type and offset pointer indirection, which only allocates some memory prior to sorting arrays with indirection
template<typename T, ptrdiff_t indirection1 = 0, sortingdirection direction = sortingdirection::ascfwdorder, sortingmode mode = sortingmode::native, ptrdiff_t indirection2 = 0, bool isindexed2 = false, typename V, typename... vararguments>
#if defined(__has_cpp_attribute) && __has_cpp_attribute(nodiscard)
[[nodiscard]]
#endif
RSBD8_FUNC_INLINE std::enable_if_t<
	8 >= CHAR_BIT * sizeof(std::remove_pointer_t<T>),
	bool> radixsortcopy(size_t count, V *const input[], V *output[]
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
		, size_t largepagesize = 0
#elif defined(_POSIX_C_SOURCE)
		, int mmapflags = MAP_ANONYMOUS | MAP_PRIVATE
#endif
		, vararguments... varparameters)noexcept{
#ifdef _WIN32// _WIN32 will remain defined for Windows versions past the legacy 32-bit original.
	assert(!(largepagesize - 1 & largepagesize));// a maximum of one bit should be set in the value of largepagesize
	static_cast<void>(largepagesize);
#elif defined(_POSIX_C_SOURCE)
	static_cast<void>(mmapflags);
#endif
	// the single-part version never needs an extra buffer
	radixsortcopynoalloc<T, indirection1, direction, mode, indirection2, isindexed2, V>(count, input, output, varparameters...);
	return{true};
}

// Library finalisation
// It's a good practice to not propagate private macro definitions when compiling the next files.
// There are no more than these two items from #define statements in this library.
#undef RSBD8_FUNC_INLINE
#undef RSBD8_FUNC_NORMAL
}// namespace rsbd8